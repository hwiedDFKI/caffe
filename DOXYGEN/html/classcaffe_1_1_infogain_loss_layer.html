<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.15"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>caffe: caffe::InfogainLossLayer&lt; Dtype &gt; Class Template Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(initResizable);
/* @license-end */</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">caffe
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.15 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('classcaffe_1_1_infogain_loss_layer.html','');});
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pro-methods">Protected Member Functions</a> &#124;
<a href="#pro-attribs">Protected Attributes</a> &#124;
<a href="classcaffe_1_1_infogain_loss_layer-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">caffe::InfogainLossLayer&lt; Dtype &gt; Class Template Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p>A generalization of <a class="el" href="classcaffe_1_1_softmax_with_loss_layer.html" title="Computes the multinomial logistic loss for a one-of-many classification task, passing real-valued pre...">SoftmaxWithLossLayer</a> that takes an "information gain" (infogain) matrix specifying the "value" of all label pairs.  
 <a href="classcaffe_1_1_infogain_loss_layer.html#details">More...</a></p>

<p><code>#include &lt;<a class="el" href="build_2install_2include_2caffe_2layers_2infogain__loss__layer_8hpp_source.html">infogain_loss_layer.hpp</a>&gt;</code></p>
<div class="dynheader">
Inheritance diagram for caffe::InfogainLossLayer&lt; Dtype &gt;:</div>
<div class="dyncontent">
<div class="center"><img src="classcaffe_1_1_infogain_loss_layer__inherit__graph.png" border="0" usemap="#caffe_1_1_infogain_loss_layer_3_01_dtype_01_4_inherit__map" alt="Inheritance graph"/></div>
<map name="caffe_1_1_infogain_loss_layer_3_01_dtype_01_4_inherit__map" id="caffe_1_1_infogain_loss_layer_3_01_dtype_01_4_inherit__map">
<area shape="rect" id="node2" href="classcaffe_1_1_loss_layer.html" title="An interface for Layers that take two Blobs as input â€“ usually (1) predictions and (2) ground&#45;truth ..." alt="" coords="5,80,184,107"/>
<area shape="rect" id="node3" href="classcaffe_1_1_layer.html" title="An interface for the units of computation which can be composed into a Net. " alt="" coords="19,5,170,32"/>
</map>
<center><span class="legend">[<a target="top" href="graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:ac2269ba8dc7d18fa8fab90ea9f295784"><td class="memItemLeft" align="right" valign="top"><a id="ac2269ba8dc7d18fa8fab90ea9f295784"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>InfogainLossLayer</b> (const <a class="el" href="classcaffe_1_1_layer_parameter.html">LayerParameter</a> &amp;param)</td></tr>
<tr class="separator:ac2269ba8dc7d18fa8fab90ea9f295784"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a772be3f4074c72b3cf9214bda3422402"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#a772be3f4074c72b3cf9214bda3422402">LayerSetUp</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="memdesc:a772be3f4074c72b3cf9214bda3422402"><td class="mdescLeft">&#160;</td><td class="mdescRight">Does layer-specific setup: your layer should implement this function as well as Reshape.  <a href="#a772be3f4074c72b3cf9214bda3422402">More...</a><br /></td></tr>
<tr class="separator:a772be3f4074c72b3cf9214bda3422402"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a83ed478450bc7f629499fed37f654c5c"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#a83ed478450bc7f629499fed37f654c5c">Reshape</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="memdesc:a83ed478450bc7f629499fed37f654c5c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Adjust the shapes of top blobs and internal buffers to accommodate the shapes of the bottom blobs.  <a href="#a83ed478450bc7f629499fed37f654c5c">More...</a><br /></td></tr>
<tr class="separator:a83ed478450bc7f629499fed37f654c5c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa03732f381764180748479c83b289869"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#aa03732f381764180748479c83b289869">ExactNumBottomBlobs</a> () const</td></tr>
<tr class="memdesc:aa03732f381764180748479c83b289869"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the exact number of bottom blobs required by the layer, or -1 if no exact number is required.  <a href="#aa03732f381764180748479c83b289869">More...</a><br /></td></tr>
<tr class="separator:aa03732f381764180748479c83b289869"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad8a1ef702a695e379e5d0450369b4a0c"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#ad8a1ef702a695e379e5d0450369b4a0c">MinBottomBlobs</a> () const</td></tr>
<tr class="memdesc:ad8a1ef702a695e379e5d0450369b4a0c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the minimum number of bottom blobs required by the layer, or -1 if no minimum number is required.  <a href="#ad8a1ef702a695e379e5d0450369b4a0c">More...</a><br /></td></tr>
<tr class="separator:ad8a1ef702a695e379e5d0450369b4a0c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9b2372959a16da1e80ae7a98b7689a4c"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#a9b2372959a16da1e80ae7a98b7689a4c">MaxBottomBlobs</a> () const</td></tr>
<tr class="memdesc:a9b2372959a16da1e80ae7a98b7689a4c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the maximum number of bottom blobs required by the layer, or -1 if no maximum number is required.  <a href="#a9b2372959a16da1e80ae7a98b7689a4c">More...</a><br /></td></tr>
<tr class="separator:a9b2372959a16da1e80ae7a98b7689a4c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aaf55e75f2296586b1fee0175e2d72fbb"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#aaf55e75f2296586b1fee0175e2d72fbb">ExactNumTopBlobs</a> () const</td></tr>
<tr class="memdesc:aaf55e75f2296586b1fee0175e2d72fbb"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the exact number of top blobs required by the layer, or -1 if no exact number is required.  <a href="#aaf55e75f2296586b1fee0175e2d72fbb">More...</a><br /></td></tr>
<tr class="separator:aaf55e75f2296586b1fee0175e2d72fbb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a15c4916e5de27151eb745491d8d14d41"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#a15c4916e5de27151eb745491d8d14d41">MinTopBlobs</a> () const</td></tr>
<tr class="memdesc:a15c4916e5de27151eb745491d8d14d41"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the minimum number of top blobs required by the layer, or -1 if no minimum number is required.  <a href="#a15c4916e5de27151eb745491d8d14d41">More...</a><br /></td></tr>
<tr class="separator:a15c4916e5de27151eb745491d8d14d41"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a93019601c6256354fd4758da91d9311f"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#a93019601c6256354fd4758da91d9311f">MaxTopBlobs</a> () const</td></tr>
<tr class="memdesc:a93019601c6256354fd4758da91d9311f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the maximum number of top blobs required by the layer, or -1 if no maximum number is required.  <a href="#a93019601c6256354fd4758da91d9311f">More...</a><br /></td></tr>
<tr class="separator:a93019601c6256354fd4758da91d9311f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aada26ffd60207582fe2af602004e271b"><td class="memItemLeft" align="right" valign="top"><a id="aada26ffd60207582fe2af602004e271b"></a>
virtual const char *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#aada26ffd60207582fe2af602004e271b">type</a> () const</td></tr>
<tr class="memdesc:aada26ffd60207582fe2af602004e271b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the layer type. <br /></td></tr>
<tr class="separator:aada26ffd60207582fe2af602004e271b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac2269ba8dc7d18fa8fab90ea9f295784"><td class="memItemLeft" align="right" valign="top"><a id="ac2269ba8dc7d18fa8fab90ea9f295784"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>InfogainLossLayer</b> (const <a class="el" href="classcaffe_1_1_layer_parameter.html">LayerParameter</a> &amp;param)</td></tr>
<tr class="separator:ac2269ba8dc7d18fa8fab90ea9f295784"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae59c01de80f22c87c1dd2ef87c6e6a2f"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#ae59c01de80f22c87c1dd2ef87c6e6a2f">LayerSetUp</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="memdesc:ae59c01de80f22c87c1dd2ef87c6e6a2f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Does layer-specific setup: your layer should implement this function as well as Reshape.  <a href="#ae59c01de80f22c87c1dd2ef87c6e6a2f">More...</a><br /></td></tr>
<tr class="separator:ae59c01de80f22c87c1dd2ef87c6e6a2f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa2903026b3886816270deb038a463759"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#aa2903026b3886816270deb038a463759">Reshape</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="memdesc:aa2903026b3886816270deb038a463759"><td class="mdescLeft">&#160;</td><td class="mdescRight">Adjust the shapes of top blobs and internal buffers to accommodate the shapes of the bottom blobs.  <a href="#aa2903026b3886816270deb038a463759">More...</a><br /></td></tr>
<tr class="separator:aa2903026b3886816270deb038a463759"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa03732f381764180748479c83b289869"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#aa03732f381764180748479c83b289869">ExactNumBottomBlobs</a> () const</td></tr>
<tr class="memdesc:aa03732f381764180748479c83b289869"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the exact number of bottom blobs required by the layer, or -1 if no exact number is required.  <a href="#aa03732f381764180748479c83b289869">More...</a><br /></td></tr>
<tr class="separator:aa03732f381764180748479c83b289869"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad8a1ef702a695e379e5d0450369b4a0c"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#ad8a1ef702a695e379e5d0450369b4a0c">MinBottomBlobs</a> () const</td></tr>
<tr class="memdesc:ad8a1ef702a695e379e5d0450369b4a0c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the minimum number of bottom blobs required by the layer, or -1 if no minimum number is required.  <a href="#ad8a1ef702a695e379e5d0450369b4a0c">More...</a><br /></td></tr>
<tr class="separator:ad8a1ef702a695e379e5d0450369b4a0c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9b2372959a16da1e80ae7a98b7689a4c"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#a9b2372959a16da1e80ae7a98b7689a4c">MaxBottomBlobs</a> () const</td></tr>
<tr class="memdesc:a9b2372959a16da1e80ae7a98b7689a4c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the maximum number of bottom blobs required by the layer, or -1 if no maximum number is required.  <a href="#a9b2372959a16da1e80ae7a98b7689a4c">More...</a><br /></td></tr>
<tr class="separator:a9b2372959a16da1e80ae7a98b7689a4c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aaf55e75f2296586b1fee0175e2d72fbb"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#aaf55e75f2296586b1fee0175e2d72fbb">ExactNumTopBlobs</a> () const</td></tr>
<tr class="memdesc:aaf55e75f2296586b1fee0175e2d72fbb"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the exact number of top blobs required by the layer, or -1 if no exact number is required.  <a href="#aaf55e75f2296586b1fee0175e2d72fbb">More...</a><br /></td></tr>
<tr class="separator:aaf55e75f2296586b1fee0175e2d72fbb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a15c4916e5de27151eb745491d8d14d41"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#a15c4916e5de27151eb745491d8d14d41">MinTopBlobs</a> () const</td></tr>
<tr class="memdesc:a15c4916e5de27151eb745491d8d14d41"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the minimum number of top blobs required by the layer, or -1 if no minimum number is required.  <a href="#a15c4916e5de27151eb745491d8d14d41">More...</a><br /></td></tr>
<tr class="separator:a15c4916e5de27151eb745491d8d14d41"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a93019601c6256354fd4758da91d9311f"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#a93019601c6256354fd4758da91d9311f">MaxTopBlobs</a> () const</td></tr>
<tr class="memdesc:a93019601c6256354fd4758da91d9311f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the maximum number of top blobs required by the layer, or -1 if no maximum number is required.  <a href="#a93019601c6256354fd4758da91d9311f">More...</a><br /></td></tr>
<tr class="separator:a93019601c6256354fd4758da91d9311f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aada26ffd60207582fe2af602004e271b"><td class="memItemLeft" align="right" valign="top"><a id="aada26ffd60207582fe2af602004e271b"></a>
virtual const char *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#aada26ffd60207582fe2af602004e271b">type</a> () const</td></tr>
<tr class="memdesc:aada26ffd60207582fe2af602004e271b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the layer type. <br /></td></tr>
<tr class="separator:aada26ffd60207582fe2af602004e271b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_classcaffe_1_1_loss_layer"><td colspan="2" onclick="javascript:toggleInherit('pub_methods_classcaffe_1_1_loss_layer')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="classcaffe_1_1_loss_layer.html">caffe::LossLayer&lt; Dtype &gt;</a></td></tr>
<tr class="memitem:a16e133050e2d97c6f024ea74e3ba4ead inherit pub_methods_classcaffe_1_1_loss_layer"><td class="memItemLeft" align="right" valign="top"><a id="a16e133050e2d97c6f024ea74e3ba4ead"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>LossLayer</b> (const <a class="el" href="classcaffe_1_1_layer_parameter.html">LayerParameter</a> &amp;param)</td></tr>
<tr class="separator:a16e133050e2d97c6f024ea74e3ba4ead inherit pub_methods_classcaffe_1_1_loss_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae98a9942cdb1c67e09d45cc2d876618e inherit pub_methods_classcaffe_1_1_loss_layer"><td class="memItemLeft" align="right" valign="top"><a id="ae98a9942cdb1c67e09d45cc2d876618e"></a>
virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_loss_layer.html#ae98a9942cdb1c67e09d45cc2d876618e">AutoTopBlobs</a> () const</td></tr>
<tr class="memdesc:ae98a9942cdb1c67e09d45cc2d876618e inherit pub_methods_classcaffe_1_1_loss_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">For convenience and backwards compatibility, instruct the <a class="el" href="classcaffe_1_1_net.html" title="Connects Layers together into a directed acyclic graph (DAG) specified by a NetParameter. ">Net</a> to automatically allocate a single top <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> for LossLayers, into which they output their singleton loss, (even if the user didn't specify one in the prototxt, etc.). <br /></td></tr>
<tr class="separator:ae98a9942cdb1c67e09d45cc2d876618e inherit pub_methods_classcaffe_1_1_loss_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a36d35155bfe0de53a79c517f33759612 inherit pub_methods_classcaffe_1_1_loss_layer"><td class="memItemLeft" align="right" valign="top">virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_loss_layer.html#a36d35155bfe0de53a79c517f33759612">AllowForceBackward</a> (const int bottom_index) const</td></tr>
<tr class="separator:a36d35155bfe0de53a79c517f33759612 inherit pub_methods_classcaffe_1_1_loss_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a16e133050e2d97c6f024ea74e3ba4ead inherit pub_methods_classcaffe_1_1_loss_layer"><td class="memItemLeft" align="right" valign="top"><a id="a16e133050e2d97c6f024ea74e3ba4ead"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>LossLayer</b> (const <a class="el" href="classcaffe_1_1_layer_parameter.html">LayerParameter</a> &amp;param)</td></tr>
<tr class="separator:a16e133050e2d97c6f024ea74e3ba4ead inherit pub_methods_classcaffe_1_1_loss_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae98a9942cdb1c67e09d45cc2d876618e inherit pub_methods_classcaffe_1_1_loss_layer"><td class="memItemLeft" align="right" valign="top"><a id="ae98a9942cdb1c67e09d45cc2d876618e"></a>
virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_loss_layer.html#ae98a9942cdb1c67e09d45cc2d876618e">AutoTopBlobs</a> () const</td></tr>
<tr class="memdesc:ae98a9942cdb1c67e09d45cc2d876618e inherit pub_methods_classcaffe_1_1_loss_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">For convenience and backwards compatibility, instruct the <a class="el" href="classcaffe_1_1_net.html" title="Connects Layers together into a directed acyclic graph (DAG) specified by a NetParameter. ">Net</a> to automatically allocate a single top <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> for LossLayers, into which they output their singleton loss, (even if the user didn't specify one in the prototxt, etc.). <br /></td></tr>
<tr class="separator:ae98a9942cdb1c67e09d45cc2d876618e inherit pub_methods_classcaffe_1_1_loss_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a36d35155bfe0de53a79c517f33759612 inherit pub_methods_classcaffe_1_1_loss_layer"><td class="memItemLeft" align="right" valign="top">virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_loss_layer.html#a36d35155bfe0de53a79c517f33759612">AllowForceBackward</a> (const int bottom_index) const</td></tr>
<tr class="separator:a36d35155bfe0de53a79c517f33759612 inherit pub_methods_classcaffe_1_1_loss_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_classcaffe_1_1_layer"><td colspan="2" onclick="javascript:toggleInherit('pub_methods_classcaffe_1_1_layer')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="classcaffe_1_1_layer.html">caffe::Layer&lt; Dtype &gt;</a></td></tr>
<tr class="memitem:a7b4e4ccea08c7b8b15acc6829d5735f6 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a7b4e4ccea08c7b8b15acc6829d5735f6">Layer</a> (const <a class="el" href="classcaffe_1_1_layer_parameter.html">LayerParameter</a> &amp;param)</td></tr>
<tr class="separator:a7b4e4ccea08c7b8b15acc6829d5735f6 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a18d6bfdb535ab8e96a971dec4ae39a84 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a18d6bfdb535ab8e96a971dec4ae39a84">SetUp</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="memdesc:a18d6bfdb535ab8e96a971dec4ae39a84 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implements common layer setup functionality.  <a href="classcaffe_1_1_layer.html#a18d6bfdb535ab8e96a971dec4ae39a84">More...</a><br /></td></tr>
<tr class="separator:a18d6bfdb535ab8e96a971dec4ae39a84 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab57d272dabe8c709d2a785eebe72ca57 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">Dtype&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#ab57d272dabe8c709d2a785eebe72ca57">Forward</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="memdesc:ab57d272dabe8c709d2a785eebe72ca57 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Given the bottom blobs, compute the top blobs and the loss.  <a href="classcaffe_1_1_layer.html#ab57d272dabe8c709d2a785eebe72ca57">More...</a><br /></td></tr>
<tr class="separator:ab57d272dabe8c709d2a785eebe72ca57 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a183d343f5183a4762307f2c5e6ed1e12 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a183d343f5183a4762307f2c5e6ed1e12">Backward</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top, const vector&lt; bool &gt; &amp;propagate_down, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom)</td></tr>
<tr class="memdesc:a183d343f5183a4762307f2c5e6ed1e12 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Given the top blob error gradients, compute the bottom blob error gradients.  <a href="classcaffe_1_1_layer.html#a183d343f5183a4762307f2c5e6ed1e12">More...</a><br /></td></tr>
<tr class="separator:a183d343f5183a4762307f2c5e6ed1e12 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aaf4524ce8641a30a8a4784aee1b2b4c8 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a id="aaf4524ce8641a30a8a4784aee1b2b4c8"></a>
vector&lt; shared_ptr&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; &gt; &gt; &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#aaf4524ce8641a30a8a4784aee1b2b4c8">blobs</a> ()</td></tr>
<tr class="memdesc:aaf4524ce8641a30a8a4784aee1b2b4c8 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the vector of learnable parameter blobs. <br /></td></tr>
<tr class="separator:aaf4524ce8641a30a8a4784aee1b2b4c8 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adff82274f146e2b6922d0ebac2aaf215 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a id="adff82274f146e2b6922d0ebac2aaf215"></a>
const <a class="el" href="classcaffe_1_1_layer_parameter.html">LayerParameter</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#adff82274f146e2b6922d0ebac2aaf215">layer_param</a> () const</td></tr>
<tr class="memdesc:adff82274f146e2b6922d0ebac2aaf215 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the layer parameter. <br /></td></tr>
<tr class="separator:adff82274f146e2b6922d0ebac2aaf215 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4a1754828dda22cc8daa2f63377f3579 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a id="a4a1754828dda22cc8daa2f63377f3579"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a4a1754828dda22cc8daa2f63377f3579">ToProto</a> (<a class="el" href="classcaffe_1_1_layer_parameter.html">LayerParameter</a> *param, bool write_diff=false)</td></tr>
<tr class="memdesc:a4a1754828dda22cc8daa2f63377f3579 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Writes the layer parameter to a protocol buffer. <br /></td></tr>
<tr class="separator:a4a1754828dda22cc8daa2f63377f3579 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a899410336f30821644c8bd6c69a070c9 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a id="a899410336f30821644c8bd6c69a070c9"></a>
Dtype&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a899410336f30821644c8bd6c69a070c9">loss</a> (const int top_index) const</td></tr>
<tr class="memdesc:a899410336f30821644c8bd6c69a070c9 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the scalar loss associated with a top blob at a given index. <br /></td></tr>
<tr class="separator:a899410336f30821644c8bd6c69a070c9 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a899b09f4b91ada8545b3a43ee91e0d69 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a id="a899b09f4b91ada8545b3a43ee91e0d69"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a899b09f4b91ada8545b3a43ee91e0d69">set_loss</a> (const int top_index, const Dtype value)</td></tr>
<tr class="memdesc:a899b09f4b91ada8545b3a43ee91e0d69 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets the loss associated with a top blob at a given index. <br /></td></tr>
<tr class="separator:a899b09f4b91ada8545b3a43ee91e0d69 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af452a938bc7596f9b5e9900c8dc4ab3d inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#af452a938bc7596f9b5e9900c8dc4ab3d">EqualNumBottomTopBlobs</a> () const</td></tr>
<tr class="memdesc:af452a938bc7596f9b5e9900c8dc4ab3d inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns true if the layer requires an equal number of bottom and top blobs.  <a href="classcaffe_1_1_layer.html#af452a938bc7596f9b5e9900c8dc4ab3d">More...</a><br /></td></tr>
<tr class="separator:af452a938bc7596f9b5e9900c8dc4ab3d inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1a3708013b0231e71d725252e10ce6e3 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a1a3708013b0231e71d725252e10ce6e3">param_propagate_down</a> (const int param_id)</td></tr>
<tr class="memdesc:a1a3708013b0231e71d725252e10ce6e3 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Specifies whether the layer should compute gradients w.r.t. a parameter at a particular index given by param_id.  <a href="classcaffe_1_1_layer.html#a1a3708013b0231e71d725252e10ce6e3">More...</a><br /></td></tr>
<tr class="separator:a1a3708013b0231e71d725252e10ce6e3 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9a6fcb843803ed556f0a69cc2864379b inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a id="a9a6fcb843803ed556f0a69cc2864379b"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a9a6fcb843803ed556f0a69cc2864379b">set_param_propagate_down</a> (const int param_id, const bool value)</td></tr>
<tr class="memdesc:a9a6fcb843803ed556f0a69cc2864379b inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets whether the layer should compute gradients w.r.t. a parameter at a particular index given by param_id. <br /></td></tr>
<tr class="separator:a9a6fcb843803ed556f0a69cc2864379b inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7b4e4ccea08c7b8b15acc6829d5735f6 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a7b4e4ccea08c7b8b15acc6829d5735f6">Layer</a> (const <a class="el" href="classcaffe_1_1_layer_parameter.html">LayerParameter</a> &amp;param)</td></tr>
<tr class="separator:a7b4e4ccea08c7b8b15acc6829d5735f6 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a18d6bfdb535ab8e96a971dec4ae39a84 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a18d6bfdb535ab8e96a971dec4ae39a84">SetUp</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="memdesc:a18d6bfdb535ab8e96a971dec4ae39a84 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implements common layer setup functionality.  <a href="classcaffe_1_1_layer.html#a18d6bfdb535ab8e96a971dec4ae39a84">More...</a><br /></td></tr>
<tr class="separator:a18d6bfdb535ab8e96a971dec4ae39a84 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab57d272dabe8c709d2a785eebe72ca57 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">Dtype&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#ab57d272dabe8c709d2a785eebe72ca57">Forward</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="memdesc:ab57d272dabe8c709d2a785eebe72ca57 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Given the bottom blobs, compute the top blobs and the loss.  <a href="classcaffe_1_1_layer.html#ab57d272dabe8c709d2a785eebe72ca57">More...</a><br /></td></tr>
<tr class="separator:ab57d272dabe8c709d2a785eebe72ca57 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a183d343f5183a4762307f2c5e6ed1e12 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a183d343f5183a4762307f2c5e6ed1e12">Backward</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top, const vector&lt; bool &gt; &amp;propagate_down, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom)</td></tr>
<tr class="memdesc:a183d343f5183a4762307f2c5e6ed1e12 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Given the top blob error gradients, compute the bottom blob error gradients.  <a href="classcaffe_1_1_layer.html#a183d343f5183a4762307f2c5e6ed1e12">More...</a><br /></td></tr>
<tr class="separator:a183d343f5183a4762307f2c5e6ed1e12 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aaf4524ce8641a30a8a4784aee1b2b4c8 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a id="aaf4524ce8641a30a8a4784aee1b2b4c8"></a>
vector&lt; shared_ptr&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; &gt; &gt; &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#aaf4524ce8641a30a8a4784aee1b2b4c8">blobs</a> ()</td></tr>
<tr class="memdesc:aaf4524ce8641a30a8a4784aee1b2b4c8 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the vector of learnable parameter blobs. <br /></td></tr>
<tr class="separator:aaf4524ce8641a30a8a4784aee1b2b4c8 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adff82274f146e2b6922d0ebac2aaf215 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a id="adff82274f146e2b6922d0ebac2aaf215"></a>
const <a class="el" href="classcaffe_1_1_layer_parameter.html">LayerParameter</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#adff82274f146e2b6922d0ebac2aaf215">layer_param</a> () const</td></tr>
<tr class="memdesc:adff82274f146e2b6922d0ebac2aaf215 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the layer parameter. <br /></td></tr>
<tr class="separator:adff82274f146e2b6922d0ebac2aaf215 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab2badafa974783cee8ecc8f666769a0e inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a id="ab2badafa974783cee8ecc8f666769a0e"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#ab2badafa974783cee8ecc8f666769a0e">ToProto</a> (<a class="el" href="classcaffe_1_1_layer_parameter.html">LayerParameter</a> *param, bool write_diff=false)</td></tr>
<tr class="memdesc:ab2badafa974783cee8ecc8f666769a0e inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Writes the layer parameter to a protocol buffer. <br /></td></tr>
<tr class="separator:ab2badafa974783cee8ecc8f666769a0e inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a899410336f30821644c8bd6c69a070c9 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a id="a899410336f30821644c8bd6c69a070c9"></a>
Dtype&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a899410336f30821644c8bd6c69a070c9">loss</a> (const int top_index) const</td></tr>
<tr class="memdesc:a899410336f30821644c8bd6c69a070c9 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the scalar loss associated with a top blob at a given index. <br /></td></tr>
<tr class="separator:a899410336f30821644c8bd6c69a070c9 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a899b09f4b91ada8545b3a43ee91e0d69 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a id="a899b09f4b91ada8545b3a43ee91e0d69"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a899b09f4b91ada8545b3a43ee91e0d69">set_loss</a> (const int top_index, const Dtype value)</td></tr>
<tr class="memdesc:a899b09f4b91ada8545b3a43ee91e0d69 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets the loss associated with a top blob at a given index. <br /></td></tr>
<tr class="separator:a899b09f4b91ada8545b3a43ee91e0d69 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af452a938bc7596f9b5e9900c8dc4ab3d inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#af452a938bc7596f9b5e9900c8dc4ab3d">EqualNumBottomTopBlobs</a> () const</td></tr>
<tr class="memdesc:af452a938bc7596f9b5e9900c8dc4ab3d inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns true if the layer requires an equal number of bottom and top blobs.  <a href="classcaffe_1_1_layer.html#af452a938bc7596f9b5e9900c8dc4ab3d">More...</a><br /></td></tr>
<tr class="separator:af452a938bc7596f9b5e9900c8dc4ab3d inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1a3708013b0231e71d725252e10ce6e3 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a1a3708013b0231e71d725252e10ce6e3">param_propagate_down</a> (const int param_id)</td></tr>
<tr class="memdesc:a1a3708013b0231e71d725252e10ce6e3 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Specifies whether the layer should compute gradients w.r.t. a parameter at a particular index given by param_id.  <a href="classcaffe_1_1_layer.html#a1a3708013b0231e71d725252e10ce6e3">More...</a><br /></td></tr>
<tr class="separator:a1a3708013b0231e71d725252e10ce6e3 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9a6fcb843803ed556f0a69cc2864379b inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a id="a9a6fcb843803ed556f0a69cc2864379b"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a9a6fcb843803ed556f0a69cc2864379b">set_param_propagate_down</a> (const int param_id, const bool value)</td></tr>
<tr class="memdesc:a9a6fcb843803ed556f0a69cc2864379b inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets whether the layer should compute gradients w.r.t. a parameter at a particular index given by param_id. <br /></td></tr>
<tr class="separator:a9a6fcb843803ed556f0a69cc2864379b inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pro-methods"></a>
Protected Member Functions</h2></td></tr>
<tr class="memitem:a134b51c126eb4b62fac804965f8d8327"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#a134b51c126eb4b62fac804965f8d8327">Forward_cpu</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="memdesc:a134b51c126eb4b62fac804965f8d8327"><td class="mdescLeft">&#160;</td><td class="mdescRight">A generalization of <a class="el" href="classcaffe_1_1_softmax_with_loss_layer.html" title="Computes the multinomial logistic loss for a one-of-many classification task, passing real-valued pre...">SoftmaxWithLossLayer</a> that takes an "information gain" (infogain) matrix specifying the "value" of all label pairs.  <a href="#a134b51c126eb4b62fac804965f8d8327">More...</a><br /></td></tr>
<tr class="separator:a134b51c126eb4b62fac804965f8d8327"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6d8fc17daa7233fb96629b641fbc46ac"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#a6d8fc17daa7233fb96629b641fbc46ac">Backward_cpu</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top, const vector&lt; bool &gt; &amp;propagate_down, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom)</td></tr>
<tr class="memdesc:a6d8fc17daa7233fb96629b641fbc46ac"><td class="mdescLeft">&#160;</td><td class="mdescRight">Computes the infogain loss error gradient w.r.t. the predictions.  <a href="#a6d8fc17daa7233fb96629b641fbc46ac">More...</a><br /></td></tr>
<tr class="separator:a6d8fc17daa7233fb96629b641fbc46ac"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0e5e9667b19fb88ece7298e3e83d2fdb"><td class="memItemLeft" align="right" valign="top">virtual Dtype&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#a0e5e9667b19fb88ece7298e3e83d2fdb">get_normalizer</a> (LossParameter_NormalizationMode normalization_mode, int valid_count)</td></tr>
<tr class="separator:a0e5e9667b19fb88ece7298e3e83d2fdb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a030296e6af30acd17a3cfe4463456147"><td class="memItemLeft" align="right" valign="top"><a id="a030296e6af30acd17a3cfe4463456147"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#a030296e6af30acd17a3cfe4463456147">sum_rows_of_H</a> (const <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *H)</td></tr>
<tr class="memdesc:a030296e6af30acd17a3cfe4463456147"><td class="mdescLeft">&#160;</td><td class="mdescRight">fill sum_rows_H_ according to matrix H <br /></td></tr>
<tr class="separator:a030296e6af30acd17a3cfe4463456147"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a12c67529a9dbc6732db60708a4d9a9f6"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#a12c67529a9dbc6732db60708a4d9a9f6">Forward_cpu</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="memdesc:a12c67529a9dbc6732db60708a4d9a9f6"><td class="mdescLeft">&#160;</td><td class="mdescRight">A generalization of <a class="el" href="classcaffe_1_1_softmax_with_loss_layer.html" title="Computes the multinomial logistic loss for a one-of-many classification task, passing real-valued pre...">SoftmaxWithLossLayer</a> that takes an "information gain" (infogain) matrix specifying the "value" of all label pairs.  <a href="#a12c67529a9dbc6732db60708a4d9a9f6">More...</a><br /></td></tr>
<tr class="separator:a12c67529a9dbc6732db60708a4d9a9f6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2d3651cf83b24ee0508adeeed32d2fc2"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#a2d3651cf83b24ee0508adeeed32d2fc2">Backward_cpu</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top, const vector&lt; bool &gt; &amp;propagate_down, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom)</td></tr>
<tr class="memdesc:a2d3651cf83b24ee0508adeeed32d2fc2"><td class="mdescLeft">&#160;</td><td class="mdescRight">Computes the infogain loss error gradient w.r.t. the predictions.  <a href="#a2d3651cf83b24ee0508adeeed32d2fc2">More...</a><br /></td></tr>
<tr class="separator:a2d3651cf83b24ee0508adeeed32d2fc2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9c223a4b6dc5a48fb56bf653111abea1"><td class="memItemLeft" align="right" valign="top">virtual Dtype&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#a9c223a4b6dc5a48fb56bf653111abea1">get_normalizer</a> (LossParameter_NormalizationMode normalization_mode, int valid_count)</td></tr>
<tr class="separator:a9c223a4b6dc5a48fb56bf653111abea1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a30c3135fc7c5d44b159e59861cd4b232"><td class="memItemLeft" align="right" valign="top"><a id="a30c3135fc7c5d44b159e59861cd4b232"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#a30c3135fc7c5d44b159e59861cd4b232">sum_rows_of_H</a> (const <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *H)</td></tr>
<tr class="memdesc:a30c3135fc7c5d44b159e59861cd4b232"><td class="mdescLeft">&#160;</td><td class="mdescRight">fill sum_rows_H_ according to matrix H <br /></td></tr>
<tr class="separator:a30c3135fc7c5d44b159e59861cd4b232"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pro_methods_classcaffe_1_1_layer"><td colspan="2" onclick="javascript:toggleInherit('pro_methods_classcaffe_1_1_layer')"><img src="closed.png" alt="-"/>&#160;Protected Member Functions inherited from <a class="el" href="classcaffe_1_1_layer.html">caffe::Layer&lt; Dtype &gt;</a></td></tr>
<tr class="memitem:af3a88d8fb290877b4c7eb37daa3499de inherit pro_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a id="af3a88d8fb290877b4c7eb37daa3499de"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#af3a88d8fb290877b4c7eb37daa3499de">Forward_gpu</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="memdesc:af3a88d8fb290877b4c7eb37daa3499de inherit pro_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Using the GPU device, compute the layer output. Fall back to <a class="el" href="classcaffe_1_1_layer.html#a576ac6a60b1e99fe383831f52a6cea77" title="Using the CPU device, compute the layer output. ">Forward_cpu()</a> if unavailable. <br /></td></tr>
<tr class="separator:af3a88d8fb290877b4c7eb37daa3499de inherit pro_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6faee52af6250a38d1b879008257f5a7 inherit pro_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a id="a6faee52af6250a38d1b879008257f5a7"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a6faee52af6250a38d1b879008257f5a7">Backward_gpu</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top, const vector&lt; bool &gt; &amp;propagate_down, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom)</td></tr>
<tr class="memdesc:a6faee52af6250a38d1b879008257f5a7 inherit pro_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Using the GPU device, compute the gradients for any parameters and for the bottom blobs if propagate_down is true. Fall back to <a class="el" href="classcaffe_1_1_layer.html#a75c9b2a321dc713e0eaef530d02dc37f" title="Using the CPU device, compute the gradients for any parameters and for the bottom blobs if propagate_...">Backward_cpu()</a> if unavailable. <br /></td></tr>
<tr class="separator:a6faee52af6250a38d1b879008257f5a7 inherit pro_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a55c8036130225fbc874a986bdf4b27e2 inherit pro_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a55c8036130225fbc874a986bdf4b27e2">CheckBlobCounts</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="separator:a55c8036130225fbc874a986bdf4b27e2 inherit pro_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a04eb2a3d1d59c64cd64c233217d5d6fc inherit pro_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a04eb2a3d1d59c64cd64c233217d5d6fc">SetLossWeights</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="separator:a04eb2a3d1d59c64cd64c233217d5d6fc inherit pro_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af3a88d8fb290877b4c7eb37daa3499de inherit pro_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a id="af3a88d8fb290877b4c7eb37daa3499de"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#af3a88d8fb290877b4c7eb37daa3499de">Forward_gpu</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="memdesc:af3a88d8fb290877b4c7eb37daa3499de inherit pro_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Using the GPU device, compute the layer output. Fall back to <a class="el" href="classcaffe_1_1_layer.html#a576ac6a60b1e99fe383831f52a6cea77" title="Using the CPU device, compute the layer output. ">Forward_cpu()</a> if unavailable. <br /></td></tr>
<tr class="separator:af3a88d8fb290877b4c7eb37daa3499de inherit pro_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6faee52af6250a38d1b879008257f5a7 inherit pro_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a id="a6faee52af6250a38d1b879008257f5a7"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a6faee52af6250a38d1b879008257f5a7">Backward_gpu</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top, const vector&lt; bool &gt; &amp;propagate_down, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom)</td></tr>
<tr class="memdesc:a6faee52af6250a38d1b879008257f5a7 inherit pro_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Using the GPU device, compute the gradients for any parameters and for the bottom blobs if propagate_down is true. Fall back to <a class="el" href="classcaffe_1_1_layer.html#a75c9b2a321dc713e0eaef530d02dc37f" title="Using the CPU device, compute the gradients for any parameters and for the bottom blobs if propagate_...">Backward_cpu()</a> if unavailable. <br /></td></tr>
<tr class="separator:a6faee52af6250a38d1b879008257f5a7 inherit pro_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a55c8036130225fbc874a986bdf4b27e2 inherit pro_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a55c8036130225fbc874a986bdf4b27e2">CheckBlobCounts</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="separator:a55c8036130225fbc874a986bdf4b27e2 inherit pro_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a04eb2a3d1d59c64cd64c233217d5d6fc inherit pro_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a04eb2a3d1d59c64cd64c233217d5d6fc">SetLossWeights</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="separator:a04eb2a3d1d59c64cd64c233217d5d6fc inherit pro_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pro-attribs"></a>
Protected Attributes</h2></td></tr>
<tr class="memitem:a4d15d74b6f3e109ad1fef2fdc8b2746d"><td class="memItemLeft" align="right" valign="top"><a id="a4d15d74b6f3e109ad1fef2fdc8b2746d"></a>
shared_ptr&lt; <a class="el" href="classcaffe_1_1_layer.html">Layer</a>&lt; Dtype &gt; &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#a4d15d74b6f3e109ad1fef2fdc8b2746d">softmax_layer_</a></td></tr>
<tr class="memdesc:a4d15d74b6f3e109ad1fef2fdc8b2746d"><td class="mdescLeft">&#160;</td><td class="mdescRight">The internal <a class="el" href="classcaffe_1_1_softmax_layer.html" title="Computes the softmax function. ">SoftmaxLayer</a> used to map predictions to a distribution. <br /></td></tr>
<tr class="separator:a4d15d74b6f3e109ad1fef2fdc8b2746d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aca1a18ac9d1bfa669d366d7ded927503"><td class="memItemLeft" align="right" valign="top"><a id="aca1a18ac9d1bfa669d366d7ded927503"></a>
<a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#aca1a18ac9d1bfa669d366d7ded927503">prob_</a></td></tr>
<tr class="memdesc:aca1a18ac9d1bfa669d366d7ded927503"><td class="mdescLeft">&#160;</td><td class="mdescRight">prob stores the output probability predictions from the <a class="el" href="classcaffe_1_1_softmax_layer.html" title="Computes the softmax function. ">SoftmaxLayer</a>. <br /></td></tr>
<tr class="separator:aca1a18ac9d1bfa669d366d7ded927503"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7122d05d7ea094afece0bbbf0addec33"><td class="memItemLeft" align="right" valign="top"><a id="a7122d05d7ea094afece0bbbf0addec33"></a>
vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; * &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#a7122d05d7ea094afece0bbbf0addec33">softmax_bottom_vec_</a></td></tr>
<tr class="memdesc:a7122d05d7ea094afece0bbbf0addec33"><td class="mdescLeft">&#160;</td><td class="mdescRight">bottom vector holder used in call to the underlying <a class="el" href="classcaffe_1_1_layer.html#ab57d272dabe8c709d2a785eebe72ca57" title="Given the bottom blobs, compute the top blobs and the loss. ">SoftmaxLayer::Forward</a> <br /></td></tr>
<tr class="separator:a7122d05d7ea094afece0bbbf0addec33"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aaad8260753f21cdd54631870128297ad"><td class="memItemLeft" align="right" valign="top"><a id="aaad8260753f21cdd54631870128297ad"></a>
vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; * &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#aaad8260753f21cdd54631870128297ad">softmax_top_vec_</a></td></tr>
<tr class="memdesc:aaad8260753f21cdd54631870128297ad"><td class="mdescLeft">&#160;</td><td class="mdescRight">top vector holder used in call to the underlying <a class="el" href="classcaffe_1_1_layer.html#ab57d272dabe8c709d2a785eebe72ca57" title="Given the bottom blobs, compute the top blobs and the loss. ">SoftmaxLayer::Forward</a> <br /></td></tr>
<tr class="separator:aaad8260753f21cdd54631870128297ad"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7ad59c638cc23fbf53c2014375ae2b88"><td class="memItemLeft" align="right" valign="top"><a id="a7ad59c638cc23fbf53c2014375ae2b88"></a>
<a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt;&#160;</td><td class="memItemRight" valign="bottom"><b>infogain_</b></td></tr>
<tr class="separator:a7ad59c638cc23fbf53c2014375ae2b88"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a27ea069c3bfd42e4b476696314285964"><td class="memItemLeft" align="right" valign="top"><a id="a27ea069c3bfd42e4b476696314285964"></a>
<a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt;&#160;</td><td class="memItemRight" valign="bottom"><b>sum_rows_H_</b></td></tr>
<tr class="separator:a27ea069c3bfd42e4b476696314285964"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a421720fc0f85daf8b6b7808719b1f9e8"><td class="memItemLeft" align="right" valign="top"><a id="a421720fc0f85daf8b6b7808719b1f9e8"></a>
bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#a421720fc0f85daf8b6b7808719b1f9e8">has_ignore_label_</a></td></tr>
<tr class="memdesc:a421720fc0f85daf8b6b7808719b1f9e8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Whether to ignore instances with a certain label. <br /></td></tr>
<tr class="separator:a421720fc0f85daf8b6b7808719b1f9e8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad3f7c2efdf32f99510186495ba7c5cff"><td class="memItemLeft" align="right" valign="top"><a id="ad3f7c2efdf32f99510186495ba7c5cff"></a>
int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#ad3f7c2efdf32f99510186495ba7c5cff">ignore_label_</a></td></tr>
<tr class="memdesc:ad3f7c2efdf32f99510186495ba7c5cff"><td class="mdescLeft">&#160;</td><td class="mdescRight">The label indicating that an instance should be ignored. <br /></td></tr>
<tr class="separator:ad3f7c2efdf32f99510186495ba7c5cff"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab7fe88c996d31d67f5e13fc0ffc803c2"><td class="memItemLeft" align="right" valign="top"><a id="ab7fe88c996d31d67f5e13fc0ffc803c2"></a>
LossParameter_NormalizationMode&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html#ab7fe88c996d31d67f5e13fc0ffc803c2">normalization_</a></td></tr>
<tr class="memdesc:ab7fe88c996d31d67f5e13fc0ffc803c2"><td class="mdescLeft">&#160;</td><td class="mdescRight">How to normalize the output loss. <br /></td></tr>
<tr class="separator:ab7fe88c996d31d67f5e13fc0ffc803c2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0f94b595bd8be01b31994236af8cecbd"><td class="memItemLeft" align="right" valign="top"><a id="a0f94b595bd8be01b31994236af8cecbd"></a>
int&#160;</td><td class="memItemRight" valign="bottom"><b>infogain_axis_</b></td></tr>
<tr class="separator:a0f94b595bd8be01b31994236af8cecbd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aaf878e99cf00ac309ed533adf43bcbc3"><td class="memItemLeft" align="right" valign="top"><a id="aaf878e99cf00ac309ed533adf43bcbc3"></a>
int&#160;</td><td class="memItemRight" valign="bottom"><b>outer_num_</b></td></tr>
<tr class="separator:aaf878e99cf00ac309ed533adf43bcbc3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6707b103411c0acc3fddd065e91fe6e0"><td class="memItemLeft" align="right" valign="top"><a id="a6707b103411c0acc3fddd065e91fe6e0"></a>
int&#160;</td><td class="memItemRight" valign="bottom"><b>inner_num_</b></td></tr>
<tr class="separator:a6707b103411c0acc3fddd065e91fe6e0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8b58f39c561263cfa44c3831e6a15355"><td class="memItemLeft" align="right" valign="top"><a id="a8b58f39c561263cfa44c3831e6a15355"></a>
int&#160;</td><td class="memItemRight" valign="bottom"><b>num_labels_</b></td></tr>
<tr class="separator:a8b58f39c561263cfa44c3831e6a15355"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pro_attribs_classcaffe_1_1_layer"><td colspan="2" onclick="javascript:toggleInherit('pro_attribs_classcaffe_1_1_layer')"><img src="closed.png" alt="-"/>&#160;Protected Attributes inherited from <a class="el" href="classcaffe_1_1_layer.html">caffe::Layer&lt; Dtype &gt;</a></td></tr>
<tr class="memitem:a7ed12bb2df25c887e41d7ea9557fc701 inherit pro_attribs_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classcaffe_1_1_layer_parameter.html">LayerParameter</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a7ed12bb2df25c887e41d7ea9557fc701">layer_param_</a></td></tr>
<tr class="separator:a7ed12bb2df25c887e41d7ea9557fc701 inherit pro_attribs_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1d04ad7f595a82a1c811f102d68b8a19 inherit pro_attribs_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">Phase&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a1d04ad7f595a82a1c811f102d68b8a19">phase_</a></td></tr>
<tr class="separator:a1d04ad7f595a82a1c811f102d68b8a19 inherit pro_attribs_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8d6998a5f8ca95990976021de743dd21 inherit pro_attribs_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">vector&lt; shared_ptr&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; &gt; &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a8d6998a5f8ca95990976021de743dd21">blobs_</a></td></tr>
<tr class="separator:a8d6998a5f8ca95990976021de743dd21 inherit pro_attribs_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab1db6c32fa71343dac868b07288eb45e inherit pro_attribs_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">vector&lt; bool &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#ab1db6c32fa71343dac868b07288eb45e">param_propagate_down_</a></td></tr>
<tr class="separator:ab1db6c32fa71343dac868b07288eb45e inherit pro_attribs_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5fbf5ce7385b2da3d8edc7eec3822ac7 inherit pro_attribs_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">vector&lt; Dtype &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a5fbf5ce7385b2da3d8edc7eec3822ac7">loss_</a></td></tr>
<tr class="separator:a5fbf5ce7385b2da3d8edc7eec3822ac7 inherit pro_attribs_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><h3>template&lt;typename Dtype&gt;<br />
class caffe::InfogainLossLayer&lt; Dtype &gt;</h3>

<p class="">A generalization of <a class="el" href="classcaffe_1_1_softmax_with_loss_layer.html" title="Computes the multinomial logistic loss for a one-of-many classification task, passing real-valued pre...">SoftmaxWithLossLayer</a> that takes an "information gain" (infogain) matrix specifying the "value" of all label pairs. </p>
<p class="">Equivalent to the <a class="el" href="classcaffe_1_1_softmax_with_loss_layer.html" title="Computes the multinomial logistic loss for a one-of-many classification task, passing real-valued pre...">SoftmaxWithLossLayer</a> if the infogain matrix is the identity.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">bottom</td><td>input <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> vector (length 2-3)<ol type="1">
<li><img class="formulaInl" alt="$ (N \times C \times H \times W) $" src="form_10.png"/> the predictions <img class="formulaInl" alt="$ x $" src="form_11.png"/>, a <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> with values in <img class="formulaInl" alt="$ [-\infty, +\infty] $" src="form_17.png"/> indicating the predicted score for each of the <img class="formulaInl" alt="$ K = CHW $" src="form_18.png"/> classes. This layer maps these scores to a probability distribution over classes using the softmax function <img class="formulaInl" alt="$ \hat{p}_{nk} = \exp(x_{nk}) / \left[\sum_{k'} \exp(x_{nk'})\right] $" src="form_101.png"/> (see <a class="el" href="classcaffe_1_1_softmax_layer.html" title="Computes the softmax function. ">SoftmaxLayer</a>).</li>
<li><img class="formulaInl" alt="$ (N \times 1 \times 1 \times 1) $" src="form_22.png"/> the labels <img class="formulaInl" alt="$ l $" src="form_23.png"/>, an integer-valued <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> with values <img class="formulaInl" alt="$ l_n \in [0, 1, 2, ..., K - 1] $" src="form_24.png"/> indicating the correct class label among the <img class="formulaInl" alt="$ K $" src="form_25.png"/> classes</li>
<li><img class="formulaInl" alt="$ (1 \times 1 \times K \times K) $" src="form_102.png"/> (<b>optional</b>) the infogain matrix <img class="formulaInl" alt="$ H $" src="form_103.png"/>. This must be provided as the third bottom blob input if not provided as the infogain_mat in the <a class="el" href="classcaffe_1_1_infogain_loss_parameter.html">InfogainLossParameter</a>. If <img class="formulaInl" alt="$ H = I $" src="form_104.png"/>, this layer is equivalent to the <a class="el" href="classcaffe_1_1_softmax_with_loss_layer.html" title="Computes the multinomial logistic loss for a one-of-many classification task, passing real-valued pre...">SoftmaxWithLossLayer</a>. </li>
</ol>
</td></tr>
    <tr><td class="paramname">top</td><td>output <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> vector (length 1)<ol type="1">
<li><img class="formulaInl" alt="$ (1 \times 1 \times 1 \times 1) $" src="form_26.png"/> the computed infogain multinomial logistic loss: <img class="formulaInl" alt="$ E = \frac{-1}{N} \sum\limits_{n=1}^N H_{l_n} \log(\hat{p}_n) = \frac{-1}{N} \sum\limits_{n=1}^N \sum\limits_{k=1}^{K} H_{l_n,k} \log(\hat{p}_{n,k}) $" src="form_105.png"/>, where <img class="formulaInl" alt="$ H_{l_n} $" src="form_106.png"/> denotes row <img class="formulaInl" alt="$l_n$" src="form_107.png"/> of <img class="formulaInl" alt="$H$" src="form_108.png"/>. </li>
</ol>
</td></tr>
  </table>
  </dd>
</dl>
</div><h2 class="groupheader">Member Function Documentation</h2>
<a id="a6d8fc17daa7233fb96629b641fbc46ac"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a6d8fc17daa7233fb96629b641fbc46ac">&#9670;&nbsp;</a></span>Backward_cpu() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classcaffe_1_1_infogain_loss_layer.html">caffe::InfogainLossLayer</a>&lt; Dtype &gt;::Backward_cpu </td>
          <td>(</td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;&#160;</td>
          <td class="paramname"><em>top</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const vector&lt; bool &gt; &amp;&#160;</td>
          <td class="paramname"><em>propagate_down</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;&#160;</td>
          <td class="paramname"><em>bottom</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Computes the infogain loss error gradient w.r.t. the predictions. </p>
<p class="">Gradients cannot be computed with respect to the label inputs (bottom[1]), so this method ignores bottom[1] and requires !propagate_down[1], crashing if propagate_down[1] is set. (The same applies to the infogain matrix, if provided as bottom[2] rather than in the layer_param.)</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">top</td><td>output <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> vector (length 1), providing the error gradient with respect to the outputs<ol type="1">
<li><img class="formulaInl" alt="$ (1 \times 1 \times 1 \times 1) $" src="form_26.png"/> This <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a>'s diff will simply contain the loss_weight* <img class="formulaInl" alt="$ \lambda $" src="form_57.png"/>, as <img class="formulaInl" alt="$ \lambda $" src="form_57.png"/> is the coefficient of this layer's output <img class="formulaInl" alt="$\ell_i$" src="form_58.png"/> in the overall <a class="el" href="classcaffe_1_1_net.html" title="Connects Layers together into a directed acyclic graph (DAG) specified by a NetParameter. ">Net</a> loss <img class="formulaInl" alt="$ E = \lambda_i \ell_i + \mbox{other loss terms}$" src="form_59.png"/>; hence <img class="formulaInl" alt="$ \frac{\partial E}{\partial \ell_i} = \lambda_i $" src="form_60.png"/>. (*Assuming that this top <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> is not used as a bottom (input) by any other layer of the <a class="el" href="classcaffe_1_1_net.html" title="Connects Layers together into a directed acyclic graph (DAG) specified by a NetParameter. ">Net</a>.) </li>
</ol>
</td></tr>
    <tr><td class="paramname">propagate_down</td><td>see <a class="el" href="classcaffe_1_1_layer.html#a183d343f5183a4762307f2c5e6ed1e12" title="Given the top blob error gradients, compute the bottom blob error gradients. ">Layer::Backward</a>. propagate_down[1] must be false as we can't compute gradients with respect to the labels (similarly for propagate_down[2] and the infogain matrix, if provided as bottom[2]) </td></tr>
    <tr><td class="paramname">bottom</td><td>input <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> vector (length 2-3)<ol type="1">
<li><img class="formulaInl" alt="$ (N \times C \times H \times W) $" src="form_10.png"/> the predictions <img class="formulaInl" alt="$ x $" src="form_11.png"/>; Backward computes diff <img class="formulaInl" alt="$ \frac{\partial E}{\partial x} $" src="form_45.png"/></li>
<li><img class="formulaInl" alt="$ (N \times 1 \times 1 \times 1) $" src="form_22.png"/> the labels &ndash; ignored as we can't compute their error gradients</li>
<li><img class="formulaInl" alt="$ (1 \times 1 \times K \times K) $" src="form_102.png"/> (<b>optional</b>) the information gain matrix &ndash; ignored as its error gradient computation is not implemented. </li>
</ol>
</td></tr>
  </table>
  </dd>
</dl>

<p>Implements <a class="el" href="classcaffe_1_1_layer.html#a75c9b2a321dc713e0eaef530d02dc37f">caffe::Layer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a id="a2d3651cf83b24ee0508adeeed32d2fc2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2d3651cf83b24ee0508adeeed32d2fc2">&#9670;&nbsp;</a></span>Backward_cpu() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual void <a class="el" href="classcaffe_1_1_infogain_loss_layer.html">caffe::InfogainLossLayer</a>&lt; Dtype &gt;::Backward_cpu </td>
          <td>(</td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;&#160;</td>
          <td class="paramname"><em>top</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const vector&lt; bool &gt; &amp;&#160;</td>
          <td class="paramname"><em>propagate_down</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;&#160;</td>
          <td class="paramname"><em>bottom</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Computes the infogain loss error gradient w.r.t. the predictions. </p>
<p class="">Gradients cannot be computed with respect to the label inputs (bottom[1]), so this method ignores bottom[1] and requires !propagate_down[1], crashing if propagate_down[1] is set. (The same applies to the infogain matrix, if provided as bottom[2] rather than in the layer_param.)</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">top</td><td>output <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> vector (length 1), providing the error gradient with respect to the outputs<ol type="1">
<li><img class="formulaInl" alt="$ (1 \times 1 \times 1 \times 1) $" src="form_26.png"/> This <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a>'s diff will simply contain the loss_weight* <img class="formulaInl" alt="$ \lambda $" src="form_57.png"/>, as <img class="formulaInl" alt="$ \lambda $" src="form_57.png"/> is the coefficient of this layer's output <img class="formulaInl" alt="$\ell_i$" src="form_58.png"/> in the overall <a class="el" href="classcaffe_1_1_net.html" title="Connects Layers together into a directed acyclic graph (DAG) specified by a NetParameter. ">Net</a> loss <img class="formulaInl" alt="$ E = \lambda_i \ell_i + \mbox{other loss terms}$" src="form_59.png"/>; hence <img class="formulaInl" alt="$ \frac{\partial E}{\partial \ell_i} = \lambda_i $" src="form_60.png"/>. (*Assuming that this top <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> is not used as a bottom (input) by any other layer of the <a class="el" href="classcaffe_1_1_net.html" title="Connects Layers together into a directed acyclic graph (DAG) specified by a NetParameter. ">Net</a>.) </li>
</ol>
</td></tr>
    <tr><td class="paramname">propagate_down</td><td>see <a class="el" href="classcaffe_1_1_layer.html#a183d343f5183a4762307f2c5e6ed1e12" title="Given the top blob error gradients, compute the bottom blob error gradients. ">Layer::Backward</a>. propagate_down[1] must be false as we can't compute gradients with respect to the labels (similarly for propagate_down[2] and the infogain matrix, if provided as bottom[2]) </td></tr>
    <tr><td class="paramname">bottom</td><td>input <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> vector (length 2-3)<ol type="1">
<li><img class="formulaInl" alt="$ (N \times C \times H \times W) $" src="form_10.png"/> the predictions <img class="formulaInl" alt="$ x $" src="form_11.png"/>; Backward computes diff <img class="formulaInl" alt="$ \frac{\partial E}{\partial x} $" src="form_45.png"/></li>
<li><img class="formulaInl" alt="$ (N \times 1 \times 1 \times 1) $" src="form_22.png"/> the labels &ndash; ignored as we can't compute their error gradients</li>
<li><img class="formulaInl" alt="$ (1 \times 1 \times K \times K) $" src="form_102.png"/> (<b>optional</b>) the information gain matrix &ndash; ignored as its error gradient computation is not implemented. </li>
</ol>
</td></tr>
  </table>
  </dd>
</dl>

<p>Implements <a class="el" href="classcaffe_1_1_layer.html#a75c9b2a321dc713e0eaef530d02dc37f">caffe::Layer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a id="aa03732f381764180748479c83b289869"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa03732f381764180748479c83b289869">&#9670;&nbsp;</a></span>ExactNumBottomBlobs() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual int <a class="el" href="classcaffe_1_1_infogain_loss_layer.html">caffe::InfogainLossLayer</a>&lt; Dtype &gt;::ExactNumBottomBlobs </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns the exact number of bottom blobs required by the layer, or -1 if no exact number is required. </p>
<p class="">This method should be overridden to return a non-negative value if your layer expects some exact number of bottom blobs. </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1_loss_layer.html#af1620064baefb711e2c767bdc92b6fb1">caffe::LossLayer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a id="aa03732f381764180748479c83b289869"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa03732f381764180748479c83b289869">&#9670;&nbsp;</a></span>ExactNumBottomBlobs() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual int <a class="el" href="classcaffe_1_1_infogain_loss_layer.html">caffe::InfogainLossLayer</a>&lt; Dtype &gt;::ExactNumBottomBlobs </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns the exact number of bottom blobs required by the layer, or -1 if no exact number is required. </p>
<p class="">This method should be overridden to return a non-negative value if your layer expects some exact number of bottom blobs. </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1_loss_layer.html#af1620064baefb711e2c767bdc92b6fb1">caffe::LossLayer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a id="aaf55e75f2296586b1fee0175e2d72fbb"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aaf55e75f2296586b1fee0175e2d72fbb">&#9670;&nbsp;</a></span>ExactNumTopBlobs() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual int <a class="el" href="classcaffe_1_1_infogain_loss_layer.html">caffe::InfogainLossLayer</a>&lt; Dtype &gt;::ExactNumTopBlobs </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns the exact number of top blobs required by the layer, or -1 if no exact number is required. </p>
<p class="">This method should be overridden to return a non-negative value if your layer expects some exact number of top blobs. </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1_loss_layer.html#aa5d5ab714a14082f5343dc9c49025b23">caffe::LossLayer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a id="aaf55e75f2296586b1fee0175e2d72fbb"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aaf55e75f2296586b1fee0175e2d72fbb">&#9670;&nbsp;</a></span>ExactNumTopBlobs() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual int <a class="el" href="classcaffe_1_1_infogain_loss_layer.html">caffe::InfogainLossLayer</a>&lt; Dtype &gt;::ExactNumTopBlobs </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns the exact number of top blobs required by the layer, or -1 if no exact number is required. </p>
<p class="">This method should be overridden to return a non-negative value if your layer expects some exact number of top blobs. </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1_loss_layer.html#aa5d5ab714a14082f5343dc9c49025b23">caffe::LossLayer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a id="a134b51c126eb4b62fac804965f8d8327"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a134b51c126eb4b62fac804965f8d8327">&#9670;&nbsp;</a></span>Forward_cpu() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classcaffe_1_1_infogain_loss_layer.html">caffe::InfogainLossLayer</a>&lt; Dtype &gt;::Forward_cpu </td>
          <td>(</td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;&#160;</td>
          <td class="paramname"><em>bottom</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;&#160;</td>
          <td class="paramname"><em>top</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>A generalization of <a class="el" href="classcaffe_1_1_softmax_with_loss_layer.html" title="Computes the multinomial logistic loss for a one-of-many classification task, passing real-valued pre...">SoftmaxWithLossLayer</a> that takes an "information gain" (infogain) matrix specifying the "value" of all label pairs. </p>
<p class="">Equivalent to the <a class="el" href="classcaffe_1_1_softmax_with_loss_layer.html" title="Computes the multinomial logistic loss for a one-of-many classification task, passing real-valued pre...">SoftmaxWithLossLayer</a> if the infogain matrix is the identity.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">bottom</td><td>input <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> vector (length 2-3)<ol type="1">
<li><img class="formulaInl" alt="$ (N \times C \times H \times W) $" src="form_10.png"/> the predictions <img class="formulaInl" alt="$ x $" src="form_11.png"/>, a <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> with values in <img class="formulaInl" alt="$ [-\infty, +\infty] $" src="form_17.png"/> indicating the predicted score for each of the <img class="formulaInl" alt="$ K = CHW $" src="form_18.png"/> classes. This layer maps these scores to a probability distribution over classes using the softmax function <img class="formulaInl" alt="$ \hat{p}_{nk} = \exp(x_{nk}) / \left[\sum_{k'} \exp(x_{nk'})\right] $" src="form_101.png"/> (see <a class="el" href="classcaffe_1_1_softmax_layer.html" title="Computes the softmax function. ">SoftmaxLayer</a>).</li>
<li><img class="formulaInl" alt="$ (N \times 1 \times 1 \times 1) $" src="form_22.png"/> the labels <img class="formulaInl" alt="$ l $" src="form_23.png"/>, an integer-valued <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> with values <img class="formulaInl" alt="$ l_n \in [0, 1, 2, ..., K - 1] $" src="form_24.png"/> indicating the correct class label among the <img class="formulaInl" alt="$ K $" src="form_25.png"/> classes</li>
<li><img class="formulaInl" alt="$ (1 \times 1 \times K \times K) $" src="form_102.png"/> (<b>optional</b>) the infogain matrix <img class="formulaInl" alt="$ H $" src="form_103.png"/>. This must be provided as the third bottom blob input if not provided as the infogain_mat in the <a class="el" href="classcaffe_1_1_infogain_loss_parameter.html">InfogainLossParameter</a>. If <img class="formulaInl" alt="$ H = I $" src="form_104.png"/>, this layer is equivalent to the <a class="el" href="classcaffe_1_1_softmax_with_loss_layer.html" title="Computes the multinomial logistic loss for a one-of-many classification task, passing real-valued pre...">SoftmaxWithLossLayer</a>. </li>
</ol>
</td></tr>
    <tr><td class="paramname">top</td><td>output <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> vector (length 1)<ol type="1">
<li><img class="formulaInl" alt="$ (1 \times 1 \times 1 \times 1) $" src="form_26.png"/> the computed infogain multinomial logistic loss: <img class="formulaInl" alt="$ E = \frac{-1}{N} \sum\limits_{n=1}^N H_{l_n} \log(\hat{p}_n) = \frac{-1}{N} \sum\limits_{n=1}^N \sum\limits_{k=1}^{K} H_{l_n,k} \log(\hat{p}_{n,k}) $" src="form_105.png"/>, where <img class="formulaInl" alt="$ H_{l_n} $" src="form_106.png"/> denotes row <img class="formulaInl" alt="$l_n$" src="form_107.png"/> of <img class="formulaInl" alt="$H$" src="form_108.png"/>. </li>
</ol>
</td></tr>
  </table>
  </dd>
</dl>

<p>Implements <a class="el" href="classcaffe_1_1_layer.html#a576ac6a60b1e99fe383831f52a6cea77">caffe::Layer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a id="a12c67529a9dbc6732db60708a4d9a9f6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a12c67529a9dbc6732db60708a4d9a9f6">&#9670;&nbsp;</a></span>Forward_cpu() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual void <a class="el" href="classcaffe_1_1_infogain_loss_layer.html">caffe::InfogainLossLayer</a>&lt; Dtype &gt;::Forward_cpu </td>
          <td>(</td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;&#160;</td>
          <td class="paramname"><em>bottom</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;&#160;</td>
          <td class="paramname"><em>top</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>A generalization of <a class="el" href="classcaffe_1_1_softmax_with_loss_layer.html" title="Computes the multinomial logistic loss for a one-of-many classification task, passing real-valued pre...">SoftmaxWithLossLayer</a> that takes an "information gain" (infogain) matrix specifying the "value" of all label pairs. </p>
<p class="">Equivalent to the <a class="el" href="classcaffe_1_1_softmax_with_loss_layer.html" title="Computes the multinomial logistic loss for a one-of-many classification task, passing real-valued pre...">SoftmaxWithLossLayer</a> if the infogain matrix is the identity.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">bottom</td><td>input <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> vector (length 2-3)<ol type="1">
<li><img class="formulaInl" alt="$ (N \times C \times H \times W) $" src="form_10.png"/> the predictions <img class="formulaInl" alt="$ x $" src="form_11.png"/>, a <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> with values in <img class="formulaInl" alt="$ [-\infty, +\infty] $" src="form_17.png"/> indicating the predicted score for each of the <img class="formulaInl" alt="$ K = CHW $" src="form_18.png"/> classes. This layer maps these scores to a probability distribution over classes using the softmax function <img class="formulaInl" alt="$ \hat{p}_{nk} = \exp(x_{nk}) / \left[\sum_{k'} \exp(x_{nk'})\right] $" src="form_101.png"/> (see <a class="el" href="classcaffe_1_1_softmax_layer.html" title="Computes the softmax function. ">SoftmaxLayer</a>).</li>
<li><img class="formulaInl" alt="$ (N \times 1 \times 1 \times 1) $" src="form_22.png"/> the labels <img class="formulaInl" alt="$ l $" src="form_23.png"/>, an integer-valued <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> with values <img class="formulaInl" alt="$ l_n \in [0, 1, 2, ..., K - 1] $" src="form_24.png"/> indicating the correct class label among the <img class="formulaInl" alt="$ K $" src="form_25.png"/> classes</li>
<li><img class="formulaInl" alt="$ (1 \times 1 \times K \times K) $" src="form_102.png"/> (<b>optional</b>) the infogain matrix <img class="formulaInl" alt="$ H $" src="form_103.png"/>. This must be provided as the third bottom blob input if not provided as the infogain_mat in the <a class="el" href="classcaffe_1_1_infogain_loss_parameter.html">InfogainLossParameter</a>. If <img class="formulaInl" alt="$ H = I $" src="form_104.png"/>, this layer is equivalent to the <a class="el" href="classcaffe_1_1_softmax_with_loss_layer.html" title="Computes the multinomial logistic loss for a one-of-many classification task, passing real-valued pre...">SoftmaxWithLossLayer</a>. </li>
</ol>
</td></tr>
    <tr><td class="paramname">top</td><td>output <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> vector (length 1)<ol type="1">
<li><img class="formulaInl" alt="$ (1 \times 1 \times 1 \times 1) $" src="form_26.png"/> the computed infogain multinomial logistic loss: <img class="formulaInl" alt="$ E = \frac{-1}{N} \sum\limits_{n=1}^N H_{l_n} \log(\hat{p}_n) = \frac{-1}{N} \sum\limits_{n=1}^N \sum\limits_{k=1}^{K} H_{l_n,k} \log(\hat{p}_{n,k}) $" src="form_105.png"/>, where <img class="formulaInl" alt="$ H_{l_n} $" src="form_106.png"/> denotes row <img class="formulaInl" alt="$l_n$" src="form_107.png"/> of <img class="formulaInl" alt="$H$" src="form_108.png"/>. </li>
</ol>
</td></tr>
  </table>
  </dd>
</dl>

<p>Implements <a class="el" href="classcaffe_1_1_layer.html#a576ac6a60b1e99fe383831f52a6cea77">caffe::Layer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a id="a0e5e9667b19fb88ece7298e3e83d2fdb"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0e5e9667b19fb88ece7298e3e83d2fdb">&#9670;&nbsp;</a></span>get_normalizer() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">Dtype <a class="el" href="classcaffe_1_1_infogain_loss_layer.html">caffe::InfogainLossLayer</a>&lt; Dtype &gt;::get_normalizer </td>
          <td>(</td>
          <td class="paramtype">LossParameter_NormalizationMode&#160;</td>
          <td class="paramname"><em>normalization_mode</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>valid_count</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<p class="">Read the normalization mode parameter and compute the normalizer based on the blob size. If normalization_mode is VALID, the count of valid outputs will be read from valid_count, unless it is -1 in which case all outputs are assumed to be valid. </p>

</div>
</div>
<a id="a9c223a4b6dc5a48fb56bf653111abea1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9c223a4b6dc5a48fb56bf653111abea1">&#9670;&nbsp;</a></span>get_normalizer() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual Dtype <a class="el" href="classcaffe_1_1_infogain_loss_layer.html">caffe::InfogainLossLayer</a>&lt; Dtype &gt;::get_normalizer </td>
          <td>(</td>
          <td class="paramtype">LossParameter_NormalizationMode&#160;</td>
          <td class="paramname"><em>normalization_mode</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>valid_count</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<p class="">Read the normalization mode parameter and compute the normalizer based on the blob size. If normalization_mode is VALID, the count of valid outputs will be read from valid_count, unless it is -1 in which case all outputs are assumed to be valid. </p>

</div>
</div>
<a id="a772be3f4074c72b3cf9214bda3422402"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a772be3f4074c72b3cf9214bda3422402">&#9670;&nbsp;</a></span>LayerSetUp() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classcaffe_1_1_infogain_loss_layer.html">caffe::InfogainLossLayer</a>&lt; Dtype &gt;::LayerSetUp </td>
          <td>(</td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;&#160;</td>
          <td class="paramname"><em>bottom</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;&#160;</td>
          <td class="paramname"><em>top</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Does layer-specific setup: your layer should implement this function as well as Reshape. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">bottom</td><td>the preshaped input blobs, whose data fields store the input data for this layer </td></tr>
    <tr><td class="paramname">top</td><td>the allocated but unshaped output blobs</td></tr>
  </table>
  </dd>
</dl>
<p>This method should do one-time layer specific setup. This includes reading and processing relevent parameters from the <code>layer_param_</code>. Setting up the shapes of top blobs and internal buffers should be done in <code>Reshape</code>, which will be called before the forward pass to adjust the top blob sizes. </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1_loss_layer.html#aa6fc7c2e90be66f1c1f0683637c949da">caffe::LossLayer&lt; Dtype &gt;</a>.</p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="classcaffe_1_1_infogain_loss_layer_a772be3f4074c72b3cf9214bda3422402_cgraph.png" border="0" usemap="#classcaffe_1_1_infogain_loss_layer_a772be3f4074c72b3cf9214bda3422402_cgraph" alt=""/></div>
<map name="classcaffe_1_1_infogain_loss_layer_a772be3f4074c72b3cf9214bda3422402_cgraph" id="classcaffe_1_1_infogain_loss_layer_a772be3f4074c72b3cf9214bda3422402_cgraph">
<area shape="rect" id="node2" href="classcaffe_1_1_loss_layer.html#aa6fc7c2e90be66f1c1f0683637c949da" title="Does layer&#45;specific setup: your layer should implement this function as well as Reshape. " alt="" coords="219,13,415,39"/>
</map>
</div>

</div>
</div>
<a id="ae59c01de80f22c87c1dd2ef87c6e6a2f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae59c01de80f22c87c1dd2ef87c6e6a2f">&#9670;&nbsp;</a></span>LayerSetUp() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual void <a class="el" href="classcaffe_1_1_infogain_loss_layer.html">caffe::InfogainLossLayer</a>&lt; Dtype &gt;::LayerSetUp </td>
          <td>(</td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;&#160;</td>
          <td class="paramname"><em>bottom</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;&#160;</td>
          <td class="paramname"><em>top</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Does layer-specific setup: your layer should implement this function as well as Reshape. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">bottom</td><td>the preshaped input blobs, whose data fields store the input data for this layer </td></tr>
    <tr><td class="paramname">top</td><td>the allocated but unshaped output blobs</td></tr>
  </table>
  </dd>
</dl>
<p>This method should do one-time layer specific setup. This includes reading and processing relevent parameters from the <code>layer_param_</code>. Setting up the shapes of top blobs and internal buffers should be done in <code>Reshape</code>, which will be called before the forward pass to adjust the top blob sizes. </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1_loss_layer.html#aa6fc7c2e90be66f1c1f0683637c949da">caffe::LossLayer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a id="a9b2372959a16da1e80ae7a98b7689a4c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9b2372959a16da1e80ae7a98b7689a4c">&#9670;&nbsp;</a></span>MaxBottomBlobs() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual int <a class="el" href="classcaffe_1_1_infogain_loss_layer.html">caffe::InfogainLossLayer</a>&lt; Dtype &gt;::MaxBottomBlobs </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns the maximum number of bottom blobs required by the layer, or -1 if no maximum number is required. </p>
<p class="">This method should be overridden to return a non-negative value if your layer expects some maximum number of bottom blobs. </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1_layer.html#af8bdc989053e0363ab032026b46de7c3">caffe::Layer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a id="a9b2372959a16da1e80ae7a98b7689a4c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9b2372959a16da1e80ae7a98b7689a4c">&#9670;&nbsp;</a></span>MaxBottomBlobs() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual int <a class="el" href="classcaffe_1_1_infogain_loss_layer.html">caffe::InfogainLossLayer</a>&lt; Dtype &gt;::MaxBottomBlobs </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns the maximum number of bottom blobs required by the layer, or -1 if no maximum number is required. </p>
<p class="">This method should be overridden to return a non-negative value if your layer expects some maximum number of bottom blobs. </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1_layer.html#af8bdc989053e0363ab032026b46de7c3">caffe::Layer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a id="a93019601c6256354fd4758da91d9311f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a93019601c6256354fd4758da91d9311f">&#9670;&nbsp;</a></span>MaxTopBlobs() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual int <a class="el" href="classcaffe_1_1_infogain_loss_layer.html">caffe::InfogainLossLayer</a>&lt; Dtype &gt;::MaxTopBlobs </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns the maximum number of top blobs required by the layer, or -1 if no maximum number is required. </p>
<p class="">This method should be overridden to return a non-negative value if your layer expects some maximum number of top blobs. </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1_layer.html#ac6c03df0b6e40e776c94001e19994a2e">caffe::Layer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a id="a93019601c6256354fd4758da91d9311f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a93019601c6256354fd4758da91d9311f">&#9670;&nbsp;</a></span>MaxTopBlobs() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual int <a class="el" href="classcaffe_1_1_infogain_loss_layer.html">caffe::InfogainLossLayer</a>&lt; Dtype &gt;::MaxTopBlobs </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns the maximum number of top blobs required by the layer, or -1 if no maximum number is required. </p>
<p class="">This method should be overridden to return a non-negative value if your layer expects some maximum number of top blobs. </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1_layer.html#ac6c03df0b6e40e776c94001e19994a2e">caffe::Layer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a id="ad8a1ef702a695e379e5d0450369b4a0c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad8a1ef702a695e379e5d0450369b4a0c">&#9670;&nbsp;</a></span>MinBottomBlobs() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual int <a class="el" href="classcaffe_1_1_infogain_loss_layer.html">caffe::InfogainLossLayer</a>&lt; Dtype &gt;::MinBottomBlobs </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns the minimum number of bottom blobs required by the layer, or -1 if no minimum number is required. </p>
<p class="">This method should be overridden to return a non-negative value if your layer expects some minimum number of bottom blobs. </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1_layer.html#aca3cb2bafaefda5d4760aaebd0b72def">caffe::Layer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a id="ad8a1ef702a695e379e5d0450369b4a0c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad8a1ef702a695e379e5d0450369b4a0c">&#9670;&nbsp;</a></span>MinBottomBlobs() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual int <a class="el" href="classcaffe_1_1_infogain_loss_layer.html">caffe::InfogainLossLayer</a>&lt; Dtype &gt;::MinBottomBlobs </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns the minimum number of bottom blobs required by the layer, or -1 if no minimum number is required. </p>
<p class="">This method should be overridden to return a non-negative value if your layer expects some minimum number of bottom blobs. </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1_layer.html#aca3cb2bafaefda5d4760aaebd0b72def">caffe::Layer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a id="a15c4916e5de27151eb745491d8d14d41"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a15c4916e5de27151eb745491d8d14d41">&#9670;&nbsp;</a></span>MinTopBlobs() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual int <a class="el" href="classcaffe_1_1_infogain_loss_layer.html">caffe::InfogainLossLayer</a>&lt; Dtype &gt;::MinTopBlobs </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns the minimum number of top blobs required by the layer, or -1 if no minimum number is required. </p>
<p class="">This method should be overridden to return a non-negative value if your layer expects some minimum number of top blobs. </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1_layer.html#ab9e4c8d642e413948b131d851a8462a4">caffe::Layer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a id="a15c4916e5de27151eb745491d8d14d41"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a15c4916e5de27151eb745491d8d14d41">&#9670;&nbsp;</a></span>MinTopBlobs() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual int <a class="el" href="classcaffe_1_1_infogain_loss_layer.html">caffe::InfogainLossLayer</a>&lt; Dtype &gt;::MinTopBlobs </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns the minimum number of top blobs required by the layer, or -1 if no minimum number is required. </p>
<p class="">This method should be overridden to return a non-negative value if your layer expects some minimum number of top blobs. </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1_layer.html#ab9e4c8d642e413948b131d851a8462a4">caffe::Layer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a id="aa2903026b3886816270deb038a463759"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa2903026b3886816270deb038a463759">&#9670;&nbsp;</a></span>Reshape() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual void <a class="el" href="classcaffe_1_1_infogain_loss_layer.html">caffe::InfogainLossLayer</a>&lt; Dtype &gt;::Reshape </td>
          <td>(</td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;&#160;</td>
          <td class="paramname"><em>bottom</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;&#160;</td>
          <td class="paramname"><em>top</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Adjust the shapes of top blobs and internal buffers to accommodate the shapes of the bottom blobs. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">bottom</td><td>the input blobs, with the requested input shapes </td></tr>
    <tr><td class="paramname">top</td><td>the top blobs, which should be reshaped as needed</td></tr>
  </table>
  </dd>
</dl>
<p>This method should reshape top blobs as needed according to the shapes of the bottom (input) blobs, as well as reshaping any internal buffers and making any other necessary adjustments so that the layer can accommodate the bottom blobs. </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1_loss_layer.html#abf00412194f5413ea9468ee44b0d986f">caffe::LossLayer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a id="a83ed478450bc7f629499fed37f654c5c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a83ed478450bc7f629499fed37f654c5c">&#9670;&nbsp;</a></span>Reshape() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classcaffe_1_1_infogain_loss_layer.html">caffe::InfogainLossLayer</a>&lt; Dtype &gt;::Reshape </td>
          <td>(</td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;&#160;</td>
          <td class="paramname"><em>bottom</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;&#160;</td>
          <td class="paramname"><em>top</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Adjust the shapes of top blobs and internal buffers to accommodate the shapes of the bottom blobs. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">bottom</td><td>the input blobs, with the requested input shapes </td></tr>
    <tr><td class="paramname">top</td><td>the top blobs, which should be reshaped as needed</td></tr>
  </table>
  </dd>
</dl>
<p>This method should reshape top blobs as needed according to the shapes of the bottom (input) blobs, as well as reshaping any internal buffers and making any other necessary adjustments so that the layer can accommodate the bottom blobs. </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1_loss_layer.html#abf00412194f5413ea9468ee44b0d986f">caffe::LossLayer&lt; Dtype &gt;</a>.</p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="classcaffe_1_1_infogain_loss_layer_a83ed478450bc7f629499fed37f654c5c_cgraph.png" border="0" usemap="#classcaffe_1_1_infogain_loss_layer_a83ed478450bc7f629499fed37f654c5c_cgraph" alt=""/></div>
<map name="classcaffe_1_1_infogain_loss_layer_a83ed478450bc7f629499fed37f654c5c_cgraph" id="classcaffe_1_1_infogain_loss_layer_a83ed478450bc7f629499fed37f654c5c_cgraph">
<area shape="rect" id="node2" href="classcaffe_1_1_loss_layer.html#abf00412194f5413ea9468ee44b0d986f" title="Adjust the shapes of top blobs and internal buffers to accommodate the shapes of the bottom blobs..." alt="" coords="219,5,397,32"/>
<area shape="rect" id="node3" href="classcaffe_1_1_blob.html#ad0e0a9a4f49478e89161c6afe4e341a0" title="Deprecated; use Reshape(const vector&lt;int&gt;&amp; shape). " alt="" coords="235,56,381,83"/>
</map>
</div>

</div>
</div>
<hr/>The documentation for this class was generated from the following files:<ul>
<li>build/install/include/caffe/layers/<a class="el" href="build_2install_2include_2caffe_2layers_2infogain__loss__layer_8hpp_source.html">infogain_loss_layer.hpp</a></li>
<li>src/caffe/layers/infogain_loss_layer.cpp</li>
</ul>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="namespacecaffe.html">caffe</a></li><li class="navelem"><a class="el" href="classcaffe_1_1_infogain_loss_layer.html">InfogainLossLayer</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.15 </li>
  </ul>
</div>
</body>
</html>
