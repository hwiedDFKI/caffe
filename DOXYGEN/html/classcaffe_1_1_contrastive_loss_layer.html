<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.15"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>caffe: caffe::ContrastiveLossLayer&lt; Dtype &gt; Class Template Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(initResizable);
/* @license-end */</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">caffe
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.15 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('classcaffe_1_1_contrastive_loss_layer.html','');});
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pro-methods">Protected Member Functions</a> &#124;
<a href="#pro-attribs">Protected Attributes</a> &#124;
<a href="classcaffe_1_1_contrastive_loss_layer-members.html">List of all members</a>  </div>
  <div class="headertitle">
<div class="title">caffe::ContrastiveLossLayer&lt; Dtype &gt; Class Template Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p>Computes the contrastive loss <img class="formulaInl" alt="$ E = \frac{1}{2N} \sum\limits_{n=1}^N \left(y\right) d^2 + \left(1-y\right) \max \left(margin-d, 0\right)^2 $" src="form_51.png"/> where <img class="formulaInl" alt="$ d = \left| \left| a_n - b_n \right| \right|_2 $" src="form_52.png"/>. This can be used to train siamese networks.  
 <a href="classcaffe_1_1_contrastive_loss_layer.html#details">More...</a></p>

<p><code>#include &lt;<a class="el" href="build_2install_2include_2caffe_2layers_2contrastive__loss__layer_8hpp_source.html">contrastive_loss_layer.hpp</a>&gt;</code></p>
<div class="dynheader">
Inheritance diagram for caffe::ContrastiveLossLayer&lt; Dtype &gt;:</div>
<div class="dyncontent">
<div class="center"><img src="classcaffe_1_1_contrastive_loss_layer__inherit__graph.png" border="0" usemap="#caffe_1_1_contrastive_loss_layer_3_01_dtype_01_4_inherit__map" alt="Inheritance graph"/></div>
<map name="caffe_1_1_contrastive_loss_layer_3_01_dtype_01_4_inherit__map" id="caffe_1_1_contrastive_loss_layer_3_01_dtype_01_4_inherit__map">
<area shape="rect" id="node2" href="classcaffe_1_1_loss_layer.html" title="An interface for Layers that take two Blobs as input â€“ usually (1) predictions and (2) ground&#45;truth ..." alt="" coords="9,80,188,107"/>
<area shape="rect" id="node3" href="classcaffe_1_1_layer.html" title="An interface for the units of computation which can be composed into a Net. " alt="" coords="23,5,174,32"/>
</map>
<center><span class="legend">[<a target="top" href="graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:aab41120fe462451196d14321264aef60"><td class="memItemLeft" align="right" valign="top"><a id="aab41120fe462451196d14321264aef60"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>ContrastiveLossLayer</b> (const <a class="el" href="classcaffe_1_1_layer_parameter.html">LayerParameter</a> &amp;param)</td></tr>
<tr class="separator:aab41120fe462451196d14321264aef60"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a943e67e7bb9c2362ec20ce44c777beac"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_contrastive_loss_layer.html#a943e67e7bb9c2362ec20ce44c777beac">LayerSetUp</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="memdesc:a943e67e7bb9c2362ec20ce44c777beac"><td class="mdescLeft">&#160;</td><td class="mdescRight">Does layer-specific setup: your layer should implement this function as well as Reshape.  <a href="#a943e67e7bb9c2362ec20ce44c777beac">More...</a><br /></td></tr>
<tr class="separator:a943e67e7bb9c2362ec20ce44c777beac"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa6f3ad6918e64ffa1828e821accf25e9"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_contrastive_loss_layer.html#aa6f3ad6918e64ffa1828e821accf25e9">ExactNumBottomBlobs</a> () const</td></tr>
<tr class="memdesc:aa6f3ad6918e64ffa1828e821accf25e9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the exact number of bottom blobs required by the layer, or -1 if no exact number is required.  <a href="#aa6f3ad6918e64ffa1828e821accf25e9">More...</a><br /></td></tr>
<tr class="separator:aa6f3ad6918e64ffa1828e821accf25e9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab88839b44729c1bd11de97a44011aaa9"><td class="memItemLeft" align="right" valign="top"><a id="ab88839b44729c1bd11de97a44011aaa9"></a>
virtual const char *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_contrastive_loss_layer.html#ab88839b44729c1bd11de97a44011aaa9">type</a> () const</td></tr>
<tr class="memdesc:ab88839b44729c1bd11de97a44011aaa9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the layer type. <br /></td></tr>
<tr class="separator:ab88839b44729c1bd11de97a44011aaa9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af0f16d5119ac6118b670c1966c38fd7d"><td class="memItemLeft" align="right" valign="top">virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_contrastive_loss_layer.html#af0f16d5119ac6118b670c1966c38fd7d">AllowForceBackward</a> (const int bottom_index) const</td></tr>
<tr class="separator:af0f16d5119ac6118b670c1966c38fd7d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aab41120fe462451196d14321264aef60"><td class="memItemLeft" align="right" valign="top"><a id="aab41120fe462451196d14321264aef60"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>ContrastiveLossLayer</b> (const <a class="el" href="classcaffe_1_1_layer_parameter.html">LayerParameter</a> &amp;param)</td></tr>
<tr class="separator:aab41120fe462451196d14321264aef60"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a957623c05cb2289cd2ae9e9e93b48969"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_contrastive_loss_layer.html#a957623c05cb2289cd2ae9e9e93b48969">LayerSetUp</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="memdesc:a957623c05cb2289cd2ae9e9e93b48969"><td class="mdescLeft">&#160;</td><td class="mdescRight">Does layer-specific setup: your layer should implement this function as well as Reshape.  <a href="#a957623c05cb2289cd2ae9e9e93b48969">More...</a><br /></td></tr>
<tr class="separator:a957623c05cb2289cd2ae9e9e93b48969"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa6f3ad6918e64ffa1828e821accf25e9"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_contrastive_loss_layer.html#aa6f3ad6918e64ffa1828e821accf25e9">ExactNumBottomBlobs</a> () const</td></tr>
<tr class="memdesc:aa6f3ad6918e64ffa1828e821accf25e9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the exact number of bottom blobs required by the layer, or -1 if no exact number is required.  <a href="#aa6f3ad6918e64ffa1828e821accf25e9">More...</a><br /></td></tr>
<tr class="separator:aa6f3ad6918e64ffa1828e821accf25e9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab88839b44729c1bd11de97a44011aaa9"><td class="memItemLeft" align="right" valign="top"><a id="ab88839b44729c1bd11de97a44011aaa9"></a>
virtual const char *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_contrastive_loss_layer.html#ab88839b44729c1bd11de97a44011aaa9">type</a> () const</td></tr>
<tr class="memdesc:ab88839b44729c1bd11de97a44011aaa9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the layer type. <br /></td></tr>
<tr class="separator:ab88839b44729c1bd11de97a44011aaa9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af0f16d5119ac6118b670c1966c38fd7d"><td class="memItemLeft" align="right" valign="top">virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_contrastive_loss_layer.html#af0f16d5119ac6118b670c1966c38fd7d">AllowForceBackward</a> (const int bottom_index) const</td></tr>
<tr class="separator:af0f16d5119ac6118b670c1966c38fd7d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_classcaffe_1_1_loss_layer"><td colspan="2" onclick="javascript:toggleInherit('pub_methods_classcaffe_1_1_loss_layer')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="classcaffe_1_1_loss_layer.html">caffe::LossLayer&lt; Dtype &gt;</a></td></tr>
<tr class="memitem:a16e133050e2d97c6f024ea74e3ba4ead inherit pub_methods_classcaffe_1_1_loss_layer"><td class="memItemLeft" align="right" valign="top"><a id="a16e133050e2d97c6f024ea74e3ba4ead"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>LossLayer</b> (const <a class="el" href="classcaffe_1_1_layer_parameter.html">LayerParameter</a> &amp;param)</td></tr>
<tr class="separator:a16e133050e2d97c6f024ea74e3ba4ead inherit pub_methods_classcaffe_1_1_loss_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abf00412194f5413ea9468ee44b0d986f inherit pub_methods_classcaffe_1_1_loss_layer"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_loss_layer.html#abf00412194f5413ea9468ee44b0d986f">Reshape</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="memdesc:abf00412194f5413ea9468ee44b0d986f inherit pub_methods_classcaffe_1_1_loss_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Adjust the shapes of top blobs and internal buffers to accommodate the shapes of the bottom blobs.  <a href="classcaffe_1_1_loss_layer.html#abf00412194f5413ea9468ee44b0d986f">More...</a><br /></td></tr>
<tr class="separator:abf00412194f5413ea9468ee44b0d986f inherit pub_methods_classcaffe_1_1_loss_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae98a9942cdb1c67e09d45cc2d876618e inherit pub_methods_classcaffe_1_1_loss_layer"><td class="memItemLeft" align="right" valign="top"><a id="ae98a9942cdb1c67e09d45cc2d876618e"></a>
virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_loss_layer.html#ae98a9942cdb1c67e09d45cc2d876618e">AutoTopBlobs</a> () const</td></tr>
<tr class="memdesc:ae98a9942cdb1c67e09d45cc2d876618e inherit pub_methods_classcaffe_1_1_loss_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">For convenience and backwards compatibility, instruct the <a class="el" href="classcaffe_1_1_net.html" title="Connects Layers together into a directed acyclic graph (DAG) specified by a NetParameter. ">Net</a> to automatically allocate a single top <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> for LossLayers, into which they output their singleton loss, (even if the user didn't specify one in the prototxt, etc.). <br /></td></tr>
<tr class="separator:ae98a9942cdb1c67e09d45cc2d876618e inherit pub_methods_classcaffe_1_1_loss_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa5d5ab714a14082f5343dc9c49025b23 inherit pub_methods_classcaffe_1_1_loss_layer"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_loss_layer.html#aa5d5ab714a14082f5343dc9c49025b23">ExactNumTopBlobs</a> () const</td></tr>
<tr class="memdesc:aa5d5ab714a14082f5343dc9c49025b23 inherit pub_methods_classcaffe_1_1_loss_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the exact number of top blobs required by the layer, or -1 if no exact number is required.  <a href="classcaffe_1_1_loss_layer.html#aa5d5ab714a14082f5343dc9c49025b23">More...</a><br /></td></tr>
<tr class="separator:aa5d5ab714a14082f5343dc9c49025b23 inherit pub_methods_classcaffe_1_1_loss_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a16e133050e2d97c6f024ea74e3ba4ead inherit pub_methods_classcaffe_1_1_loss_layer"><td class="memItemLeft" align="right" valign="top"><a id="a16e133050e2d97c6f024ea74e3ba4ead"></a>
&#160;</td><td class="memItemRight" valign="bottom"><b>LossLayer</b> (const <a class="el" href="classcaffe_1_1_layer_parameter.html">LayerParameter</a> &amp;param)</td></tr>
<tr class="separator:a16e133050e2d97c6f024ea74e3ba4ead inherit pub_methods_classcaffe_1_1_loss_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1324e1bd42d84a6eb7e2c559d2da9ecc inherit pub_methods_classcaffe_1_1_loss_layer"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_loss_layer.html#a1324e1bd42d84a6eb7e2c559d2da9ecc">Reshape</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="memdesc:a1324e1bd42d84a6eb7e2c559d2da9ecc inherit pub_methods_classcaffe_1_1_loss_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Adjust the shapes of top blobs and internal buffers to accommodate the shapes of the bottom blobs.  <a href="classcaffe_1_1_loss_layer.html#a1324e1bd42d84a6eb7e2c559d2da9ecc">More...</a><br /></td></tr>
<tr class="separator:a1324e1bd42d84a6eb7e2c559d2da9ecc inherit pub_methods_classcaffe_1_1_loss_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae98a9942cdb1c67e09d45cc2d876618e inherit pub_methods_classcaffe_1_1_loss_layer"><td class="memItemLeft" align="right" valign="top"><a id="ae98a9942cdb1c67e09d45cc2d876618e"></a>
virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_loss_layer.html#ae98a9942cdb1c67e09d45cc2d876618e">AutoTopBlobs</a> () const</td></tr>
<tr class="memdesc:ae98a9942cdb1c67e09d45cc2d876618e inherit pub_methods_classcaffe_1_1_loss_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">For convenience and backwards compatibility, instruct the <a class="el" href="classcaffe_1_1_net.html" title="Connects Layers together into a directed acyclic graph (DAG) specified by a NetParameter. ">Net</a> to automatically allocate a single top <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> for LossLayers, into which they output their singleton loss, (even if the user didn't specify one in the prototxt, etc.). <br /></td></tr>
<tr class="separator:ae98a9942cdb1c67e09d45cc2d876618e inherit pub_methods_classcaffe_1_1_loss_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa5d5ab714a14082f5343dc9c49025b23 inherit pub_methods_classcaffe_1_1_loss_layer"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_loss_layer.html#aa5d5ab714a14082f5343dc9c49025b23">ExactNumTopBlobs</a> () const</td></tr>
<tr class="memdesc:aa5d5ab714a14082f5343dc9c49025b23 inherit pub_methods_classcaffe_1_1_loss_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the exact number of top blobs required by the layer, or -1 if no exact number is required.  <a href="classcaffe_1_1_loss_layer.html#aa5d5ab714a14082f5343dc9c49025b23">More...</a><br /></td></tr>
<tr class="separator:aa5d5ab714a14082f5343dc9c49025b23 inherit pub_methods_classcaffe_1_1_loss_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_classcaffe_1_1_layer"><td colspan="2" onclick="javascript:toggleInherit('pub_methods_classcaffe_1_1_layer')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="classcaffe_1_1_layer.html">caffe::Layer&lt; Dtype &gt;</a></td></tr>
<tr class="memitem:a7b4e4ccea08c7b8b15acc6829d5735f6 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a7b4e4ccea08c7b8b15acc6829d5735f6">Layer</a> (const <a class="el" href="classcaffe_1_1_layer_parameter.html">LayerParameter</a> &amp;param)</td></tr>
<tr class="separator:a7b4e4ccea08c7b8b15acc6829d5735f6 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a18d6bfdb535ab8e96a971dec4ae39a84 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a18d6bfdb535ab8e96a971dec4ae39a84">SetUp</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="memdesc:a18d6bfdb535ab8e96a971dec4ae39a84 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implements common layer setup functionality.  <a href="classcaffe_1_1_layer.html#a18d6bfdb535ab8e96a971dec4ae39a84">More...</a><br /></td></tr>
<tr class="separator:a18d6bfdb535ab8e96a971dec4ae39a84 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab57d272dabe8c709d2a785eebe72ca57 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">Dtype&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#ab57d272dabe8c709d2a785eebe72ca57">Forward</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="memdesc:ab57d272dabe8c709d2a785eebe72ca57 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Given the bottom blobs, compute the top blobs and the loss.  <a href="classcaffe_1_1_layer.html#ab57d272dabe8c709d2a785eebe72ca57">More...</a><br /></td></tr>
<tr class="separator:ab57d272dabe8c709d2a785eebe72ca57 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a183d343f5183a4762307f2c5e6ed1e12 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a183d343f5183a4762307f2c5e6ed1e12">Backward</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top, const vector&lt; bool &gt; &amp;propagate_down, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom)</td></tr>
<tr class="memdesc:a183d343f5183a4762307f2c5e6ed1e12 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Given the top blob error gradients, compute the bottom blob error gradients.  <a href="classcaffe_1_1_layer.html#a183d343f5183a4762307f2c5e6ed1e12">More...</a><br /></td></tr>
<tr class="separator:a183d343f5183a4762307f2c5e6ed1e12 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aaf4524ce8641a30a8a4784aee1b2b4c8 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a id="aaf4524ce8641a30a8a4784aee1b2b4c8"></a>
vector&lt; shared_ptr&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; &gt; &gt; &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#aaf4524ce8641a30a8a4784aee1b2b4c8">blobs</a> ()</td></tr>
<tr class="memdesc:aaf4524ce8641a30a8a4784aee1b2b4c8 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the vector of learnable parameter blobs. <br /></td></tr>
<tr class="separator:aaf4524ce8641a30a8a4784aee1b2b4c8 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adff82274f146e2b6922d0ebac2aaf215 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a id="adff82274f146e2b6922d0ebac2aaf215"></a>
const <a class="el" href="classcaffe_1_1_layer_parameter.html">LayerParameter</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#adff82274f146e2b6922d0ebac2aaf215">layer_param</a> () const</td></tr>
<tr class="memdesc:adff82274f146e2b6922d0ebac2aaf215 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the layer parameter. <br /></td></tr>
<tr class="separator:adff82274f146e2b6922d0ebac2aaf215 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4a1754828dda22cc8daa2f63377f3579 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a id="a4a1754828dda22cc8daa2f63377f3579"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a4a1754828dda22cc8daa2f63377f3579">ToProto</a> (<a class="el" href="classcaffe_1_1_layer_parameter.html">LayerParameter</a> *param, bool write_diff=false)</td></tr>
<tr class="memdesc:a4a1754828dda22cc8daa2f63377f3579 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Writes the layer parameter to a protocol buffer. <br /></td></tr>
<tr class="separator:a4a1754828dda22cc8daa2f63377f3579 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a899410336f30821644c8bd6c69a070c9 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a id="a899410336f30821644c8bd6c69a070c9"></a>
Dtype&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a899410336f30821644c8bd6c69a070c9">loss</a> (const int top_index) const</td></tr>
<tr class="memdesc:a899410336f30821644c8bd6c69a070c9 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the scalar loss associated with a top blob at a given index. <br /></td></tr>
<tr class="separator:a899410336f30821644c8bd6c69a070c9 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a899b09f4b91ada8545b3a43ee91e0d69 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a id="a899b09f4b91ada8545b3a43ee91e0d69"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a899b09f4b91ada8545b3a43ee91e0d69">set_loss</a> (const int top_index, const Dtype value)</td></tr>
<tr class="memdesc:a899b09f4b91ada8545b3a43ee91e0d69 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets the loss associated with a top blob at a given index. <br /></td></tr>
<tr class="separator:a899b09f4b91ada8545b3a43ee91e0d69 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aca3cb2bafaefda5d4760aaebd0b72def inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#aca3cb2bafaefda5d4760aaebd0b72def">MinBottomBlobs</a> () const</td></tr>
<tr class="memdesc:aca3cb2bafaefda5d4760aaebd0b72def inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the minimum number of bottom blobs required by the layer, or -1 if no minimum number is required.  <a href="classcaffe_1_1_layer.html#aca3cb2bafaefda5d4760aaebd0b72def">More...</a><br /></td></tr>
<tr class="separator:aca3cb2bafaefda5d4760aaebd0b72def inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af8bdc989053e0363ab032026b46de7c3 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#af8bdc989053e0363ab032026b46de7c3">MaxBottomBlobs</a> () const</td></tr>
<tr class="memdesc:af8bdc989053e0363ab032026b46de7c3 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the maximum number of bottom blobs required by the layer, or -1 if no maximum number is required.  <a href="classcaffe_1_1_layer.html#af8bdc989053e0363ab032026b46de7c3">More...</a><br /></td></tr>
<tr class="separator:af8bdc989053e0363ab032026b46de7c3 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab9e4c8d642e413948b131d851a8462a4 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#ab9e4c8d642e413948b131d851a8462a4">MinTopBlobs</a> () const</td></tr>
<tr class="memdesc:ab9e4c8d642e413948b131d851a8462a4 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the minimum number of top blobs required by the layer, or -1 if no minimum number is required.  <a href="classcaffe_1_1_layer.html#ab9e4c8d642e413948b131d851a8462a4">More...</a><br /></td></tr>
<tr class="separator:ab9e4c8d642e413948b131d851a8462a4 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac6c03df0b6e40e776c94001e19994a2e inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#ac6c03df0b6e40e776c94001e19994a2e">MaxTopBlobs</a> () const</td></tr>
<tr class="memdesc:ac6c03df0b6e40e776c94001e19994a2e inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the maximum number of top blobs required by the layer, or -1 if no maximum number is required.  <a href="classcaffe_1_1_layer.html#ac6c03df0b6e40e776c94001e19994a2e">More...</a><br /></td></tr>
<tr class="separator:ac6c03df0b6e40e776c94001e19994a2e inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af452a938bc7596f9b5e9900c8dc4ab3d inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#af452a938bc7596f9b5e9900c8dc4ab3d">EqualNumBottomTopBlobs</a> () const</td></tr>
<tr class="memdesc:af452a938bc7596f9b5e9900c8dc4ab3d inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns true if the layer requires an equal number of bottom and top blobs.  <a href="classcaffe_1_1_layer.html#af452a938bc7596f9b5e9900c8dc4ab3d">More...</a><br /></td></tr>
<tr class="separator:af452a938bc7596f9b5e9900c8dc4ab3d inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1a3708013b0231e71d725252e10ce6e3 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a1a3708013b0231e71d725252e10ce6e3">param_propagate_down</a> (const int param_id)</td></tr>
<tr class="memdesc:a1a3708013b0231e71d725252e10ce6e3 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Specifies whether the layer should compute gradients w.r.t. a parameter at a particular index given by param_id.  <a href="classcaffe_1_1_layer.html#a1a3708013b0231e71d725252e10ce6e3">More...</a><br /></td></tr>
<tr class="separator:a1a3708013b0231e71d725252e10ce6e3 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9a6fcb843803ed556f0a69cc2864379b inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a id="a9a6fcb843803ed556f0a69cc2864379b"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a9a6fcb843803ed556f0a69cc2864379b">set_param_propagate_down</a> (const int param_id, const bool value)</td></tr>
<tr class="memdesc:a9a6fcb843803ed556f0a69cc2864379b inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets whether the layer should compute gradients w.r.t. a parameter at a particular index given by param_id. <br /></td></tr>
<tr class="separator:a9a6fcb843803ed556f0a69cc2864379b inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7b4e4ccea08c7b8b15acc6829d5735f6 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a7b4e4ccea08c7b8b15acc6829d5735f6">Layer</a> (const <a class="el" href="classcaffe_1_1_layer_parameter.html">LayerParameter</a> &amp;param)</td></tr>
<tr class="separator:a7b4e4ccea08c7b8b15acc6829d5735f6 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a18d6bfdb535ab8e96a971dec4ae39a84 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a18d6bfdb535ab8e96a971dec4ae39a84">SetUp</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="memdesc:a18d6bfdb535ab8e96a971dec4ae39a84 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implements common layer setup functionality.  <a href="classcaffe_1_1_layer.html#a18d6bfdb535ab8e96a971dec4ae39a84">More...</a><br /></td></tr>
<tr class="separator:a18d6bfdb535ab8e96a971dec4ae39a84 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab57d272dabe8c709d2a785eebe72ca57 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">Dtype&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#ab57d272dabe8c709d2a785eebe72ca57">Forward</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="memdesc:ab57d272dabe8c709d2a785eebe72ca57 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Given the bottom blobs, compute the top blobs and the loss.  <a href="classcaffe_1_1_layer.html#ab57d272dabe8c709d2a785eebe72ca57">More...</a><br /></td></tr>
<tr class="separator:ab57d272dabe8c709d2a785eebe72ca57 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a183d343f5183a4762307f2c5e6ed1e12 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a183d343f5183a4762307f2c5e6ed1e12">Backward</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top, const vector&lt; bool &gt; &amp;propagate_down, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom)</td></tr>
<tr class="memdesc:a183d343f5183a4762307f2c5e6ed1e12 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Given the top blob error gradients, compute the bottom blob error gradients.  <a href="classcaffe_1_1_layer.html#a183d343f5183a4762307f2c5e6ed1e12">More...</a><br /></td></tr>
<tr class="separator:a183d343f5183a4762307f2c5e6ed1e12 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aaf4524ce8641a30a8a4784aee1b2b4c8 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a id="aaf4524ce8641a30a8a4784aee1b2b4c8"></a>
vector&lt; shared_ptr&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; &gt; &gt; &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#aaf4524ce8641a30a8a4784aee1b2b4c8">blobs</a> ()</td></tr>
<tr class="memdesc:aaf4524ce8641a30a8a4784aee1b2b4c8 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the vector of learnable parameter blobs. <br /></td></tr>
<tr class="separator:aaf4524ce8641a30a8a4784aee1b2b4c8 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adff82274f146e2b6922d0ebac2aaf215 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a id="adff82274f146e2b6922d0ebac2aaf215"></a>
const <a class="el" href="classcaffe_1_1_layer_parameter.html">LayerParameter</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#adff82274f146e2b6922d0ebac2aaf215">layer_param</a> () const</td></tr>
<tr class="memdesc:adff82274f146e2b6922d0ebac2aaf215 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the layer parameter. <br /></td></tr>
<tr class="separator:adff82274f146e2b6922d0ebac2aaf215 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab2badafa974783cee8ecc8f666769a0e inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a id="ab2badafa974783cee8ecc8f666769a0e"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#ab2badafa974783cee8ecc8f666769a0e">ToProto</a> (<a class="el" href="classcaffe_1_1_layer_parameter.html">LayerParameter</a> *param, bool write_diff=false)</td></tr>
<tr class="memdesc:ab2badafa974783cee8ecc8f666769a0e inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Writes the layer parameter to a protocol buffer. <br /></td></tr>
<tr class="separator:ab2badafa974783cee8ecc8f666769a0e inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a899410336f30821644c8bd6c69a070c9 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a id="a899410336f30821644c8bd6c69a070c9"></a>
Dtype&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a899410336f30821644c8bd6c69a070c9">loss</a> (const int top_index) const</td></tr>
<tr class="memdesc:a899410336f30821644c8bd6c69a070c9 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the scalar loss associated with a top blob at a given index. <br /></td></tr>
<tr class="separator:a899410336f30821644c8bd6c69a070c9 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a899b09f4b91ada8545b3a43ee91e0d69 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a id="a899b09f4b91ada8545b3a43ee91e0d69"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a899b09f4b91ada8545b3a43ee91e0d69">set_loss</a> (const int top_index, const Dtype value)</td></tr>
<tr class="memdesc:a899b09f4b91ada8545b3a43ee91e0d69 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets the loss associated with a top blob at a given index. <br /></td></tr>
<tr class="separator:a899b09f4b91ada8545b3a43ee91e0d69 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aca3cb2bafaefda5d4760aaebd0b72def inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#aca3cb2bafaefda5d4760aaebd0b72def">MinBottomBlobs</a> () const</td></tr>
<tr class="memdesc:aca3cb2bafaefda5d4760aaebd0b72def inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the minimum number of bottom blobs required by the layer, or -1 if no minimum number is required.  <a href="classcaffe_1_1_layer.html#aca3cb2bafaefda5d4760aaebd0b72def">More...</a><br /></td></tr>
<tr class="separator:aca3cb2bafaefda5d4760aaebd0b72def inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af8bdc989053e0363ab032026b46de7c3 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#af8bdc989053e0363ab032026b46de7c3">MaxBottomBlobs</a> () const</td></tr>
<tr class="memdesc:af8bdc989053e0363ab032026b46de7c3 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the maximum number of bottom blobs required by the layer, or -1 if no maximum number is required.  <a href="classcaffe_1_1_layer.html#af8bdc989053e0363ab032026b46de7c3">More...</a><br /></td></tr>
<tr class="separator:af8bdc989053e0363ab032026b46de7c3 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab9e4c8d642e413948b131d851a8462a4 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#ab9e4c8d642e413948b131d851a8462a4">MinTopBlobs</a> () const</td></tr>
<tr class="memdesc:ab9e4c8d642e413948b131d851a8462a4 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the minimum number of top blobs required by the layer, or -1 if no minimum number is required.  <a href="classcaffe_1_1_layer.html#ab9e4c8d642e413948b131d851a8462a4">More...</a><br /></td></tr>
<tr class="separator:ab9e4c8d642e413948b131d851a8462a4 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac6c03df0b6e40e776c94001e19994a2e inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">virtual int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#ac6c03df0b6e40e776c94001e19994a2e">MaxTopBlobs</a> () const</td></tr>
<tr class="memdesc:ac6c03df0b6e40e776c94001e19994a2e inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns the maximum number of top blobs required by the layer, or -1 if no maximum number is required.  <a href="classcaffe_1_1_layer.html#ac6c03df0b6e40e776c94001e19994a2e">More...</a><br /></td></tr>
<tr class="separator:ac6c03df0b6e40e776c94001e19994a2e inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af452a938bc7596f9b5e9900c8dc4ab3d inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">virtual bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#af452a938bc7596f9b5e9900c8dc4ab3d">EqualNumBottomTopBlobs</a> () const</td></tr>
<tr class="memdesc:af452a938bc7596f9b5e9900c8dc4ab3d inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns true if the layer requires an equal number of bottom and top blobs.  <a href="classcaffe_1_1_layer.html#af452a938bc7596f9b5e9900c8dc4ab3d">More...</a><br /></td></tr>
<tr class="separator:af452a938bc7596f9b5e9900c8dc4ab3d inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1a3708013b0231e71d725252e10ce6e3 inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a1a3708013b0231e71d725252e10ce6e3">param_propagate_down</a> (const int param_id)</td></tr>
<tr class="memdesc:a1a3708013b0231e71d725252e10ce6e3 inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Specifies whether the layer should compute gradients w.r.t. a parameter at a particular index given by param_id.  <a href="classcaffe_1_1_layer.html#a1a3708013b0231e71d725252e10ce6e3">More...</a><br /></td></tr>
<tr class="separator:a1a3708013b0231e71d725252e10ce6e3 inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9a6fcb843803ed556f0a69cc2864379b inherit pub_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a id="a9a6fcb843803ed556f0a69cc2864379b"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a9a6fcb843803ed556f0a69cc2864379b">set_param_propagate_down</a> (const int param_id, const bool value)</td></tr>
<tr class="memdesc:a9a6fcb843803ed556f0a69cc2864379b inherit pub_methods_classcaffe_1_1_layer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets whether the layer should compute gradients w.r.t. a parameter at a particular index given by param_id. <br /></td></tr>
<tr class="separator:a9a6fcb843803ed556f0a69cc2864379b inherit pub_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pro-methods"></a>
Protected Member Functions</h2></td></tr>
<tr class="memitem:ae55966330621c1a2bd5a0012f2b09fe4"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_contrastive_loss_layer.html#ae55966330621c1a2bd5a0012f2b09fe4">Forward_cpu</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="memdesc:ae55966330621c1a2bd5a0012f2b09fe4"><td class="mdescLeft">&#160;</td><td class="mdescRight">Computes the contrastive loss <img class="formulaInl" alt="$ E = \frac{1}{2N} \sum\limits_{n=1}^N \left(y\right) d^2 + \left(1-y\right) \max \left(margin-d, 0\right)^2 $" src="form_51.png"/> where <img class="formulaInl" alt="$ d = \left| \left| a_n - b_n \right| \right|_2 $" src="form_52.png"/>. This can be used to train siamese networks.  <a href="#ae55966330621c1a2bd5a0012f2b09fe4">More...</a><br /></td></tr>
<tr class="separator:ae55966330621c1a2bd5a0012f2b09fe4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abf66587f81d75255e4619c09a95da566"><td class="memItemLeft" align="right" valign="top"><a id="abf66587f81d75255e4619c09a95da566"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_contrastive_loss_layer.html#abf66587f81d75255e4619c09a95da566">Forward_gpu</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="memdesc:abf66587f81d75255e4619c09a95da566"><td class="mdescLeft">&#160;</td><td class="mdescRight">Using the GPU device, compute the layer output. Fall back to <a class="el" href="classcaffe_1_1_contrastive_loss_layer.html#ae55966330621c1a2bd5a0012f2b09fe4" title="Computes the contrastive loss  where . This can be used to train siamese networks. ">Forward_cpu()</a> if unavailable. <br /></td></tr>
<tr class="separator:abf66587f81d75255e4619c09a95da566"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a60af9729fe340be3ae0f87737215d9d0"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_contrastive_loss_layer.html#a60af9729fe340be3ae0f87737215d9d0">Backward_cpu</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top, const vector&lt; bool &gt; &amp;propagate_down, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom)</td></tr>
<tr class="memdesc:a60af9729fe340be3ae0f87737215d9d0"><td class="mdescLeft">&#160;</td><td class="mdescRight">Computes the Contrastive error gradient w.r.t. the inputs.  <a href="#a60af9729fe340be3ae0f87737215d9d0">More...</a><br /></td></tr>
<tr class="separator:a60af9729fe340be3ae0f87737215d9d0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1501e5437e3f0929da03a1046559dd06"><td class="memItemLeft" align="right" valign="top"><a id="a1501e5437e3f0929da03a1046559dd06"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_contrastive_loss_layer.html#a1501e5437e3f0929da03a1046559dd06">Backward_gpu</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top, const vector&lt; bool &gt; &amp;propagate_down, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom)</td></tr>
<tr class="memdesc:a1501e5437e3f0929da03a1046559dd06"><td class="mdescLeft">&#160;</td><td class="mdescRight">Using the GPU device, compute the gradients for any parameters and for the bottom blobs if propagate_down is true. Fall back to <a class="el" href="classcaffe_1_1_contrastive_loss_layer.html#a60af9729fe340be3ae0f87737215d9d0" title="Computes the Contrastive error gradient w.r.t. the inputs. ">Backward_cpu()</a> if unavailable. <br /></td></tr>
<tr class="separator:a1501e5437e3f0929da03a1046559dd06"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aad90e509c2f7ebd3a36054101d1d15fb"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_contrastive_loss_layer.html#aad90e509c2f7ebd3a36054101d1d15fb">Forward_cpu</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="memdesc:aad90e509c2f7ebd3a36054101d1d15fb"><td class="mdescLeft">&#160;</td><td class="mdescRight">Computes the contrastive loss <img class="formulaInl" alt="$ E = \frac{1}{2N} \sum\limits_{n=1}^N \left(y\right) d^2 + \left(1-y\right) \max \left(margin-d, 0\right)^2 $" src="form_51.png"/> where <img class="formulaInl" alt="$ d = \left| \left| a_n - b_n \right| \right|_2 $" src="form_52.png"/>. This can be used to train siamese networks.  <a href="#aad90e509c2f7ebd3a36054101d1d15fb">More...</a><br /></td></tr>
<tr class="separator:aad90e509c2f7ebd3a36054101d1d15fb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abf66587f81d75255e4619c09a95da566"><td class="memItemLeft" align="right" valign="top"><a id="abf66587f81d75255e4619c09a95da566"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_contrastive_loss_layer.html#abf66587f81d75255e4619c09a95da566">Forward_gpu</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="memdesc:abf66587f81d75255e4619c09a95da566"><td class="mdescLeft">&#160;</td><td class="mdescRight">Using the GPU device, compute the layer output. Fall back to <a class="el" href="classcaffe_1_1_contrastive_loss_layer.html#ae55966330621c1a2bd5a0012f2b09fe4" title="Computes the contrastive loss  where . This can be used to train siamese networks. ">Forward_cpu()</a> if unavailable. <br /></td></tr>
<tr class="separator:abf66587f81d75255e4619c09a95da566"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2c8e0737bba7568b172468be5c33d2a7"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_contrastive_loss_layer.html#a2c8e0737bba7568b172468be5c33d2a7">Backward_cpu</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top, const vector&lt; bool &gt; &amp;propagate_down, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom)</td></tr>
<tr class="memdesc:a2c8e0737bba7568b172468be5c33d2a7"><td class="mdescLeft">&#160;</td><td class="mdescRight">Computes the Contrastive error gradient w.r.t. the inputs.  <a href="#a2c8e0737bba7568b172468be5c33d2a7">More...</a><br /></td></tr>
<tr class="separator:a2c8e0737bba7568b172468be5c33d2a7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1501e5437e3f0929da03a1046559dd06"><td class="memItemLeft" align="right" valign="top"><a id="a1501e5437e3f0929da03a1046559dd06"></a>
virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_contrastive_loss_layer.html#a1501e5437e3f0929da03a1046559dd06">Backward_gpu</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top, const vector&lt; bool &gt; &amp;propagate_down, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom)</td></tr>
<tr class="memdesc:a1501e5437e3f0929da03a1046559dd06"><td class="mdescLeft">&#160;</td><td class="mdescRight">Using the GPU device, compute the gradients for any parameters and for the bottom blobs if propagate_down is true. Fall back to <a class="el" href="classcaffe_1_1_contrastive_loss_layer.html#a60af9729fe340be3ae0f87737215d9d0" title="Computes the Contrastive error gradient w.r.t. the inputs. ">Backward_cpu()</a> if unavailable. <br /></td></tr>
<tr class="separator:a1501e5437e3f0929da03a1046559dd06"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pro_methods_classcaffe_1_1_layer"><td colspan="2" onclick="javascript:toggleInherit('pro_methods_classcaffe_1_1_layer')"><img src="closed.png" alt="-"/>&#160;Protected Member Functions inherited from <a class="el" href="classcaffe_1_1_layer.html">caffe::Layer&lt; Dtype &gt;</a></td></tr>
<tr class="memitem:a55c8036130225fbc874a986bdf4b27e2 inherit pro_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a55c8036130225fbc874a986bdf4b27e2">CheckBlobCounts</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="separator:a55c8036130225fbc874a986bdf4b27e2 inherit pro_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a04eb2a3d1d59c64cd64c233217d5d6fc inherit pro_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a04eb2a3d1d59c64cd64c233217d5d6fc">SetLossWeights</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="separator:a04eb2a3d1d59c64cd64c233217d5d6fc inherit pro_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a55c8036130225fbc874a986bdf4b27e2 inherit pro_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a55c8036130225fbc874a986bdf4b27e2">CheckBlobCounts</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;bottom, const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="separator:a55c8036130225fbc874a986bdf4b27e2 inherit pro_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a04eb2a3d1d59c64cd64c233217d5d6fc inherit pro_methods_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a04eb2a3d1d59c64cd64c233217d5d6fc">SetLossWeights</a> (const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;top)</td></tr>
<tr class="separator:a04eb2a3d1d59c64cd64c233217d5d6fc inherit pro_methods_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="pro-attribs"></a>
Protected Attributes</h2></td></tr>
<tr class="memitem:a2eb9543a04f7a3beb66b3a41bfd0c2b5"><td class="memItemLeft" align="right" valign="top"><a id="a2eb9543a04f7a3beb66b3a41bfd0c2b5"></a>
<a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt;&#160;</td><td class="memItemRight" valign="bottom"><b>diff_</b></td></tr>
<tr class="separator:a2eb9543a04f7a3beb66b3a41bfd0c2b5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae77329ab259285b66bcae6dd4bd9b277"><td class="memItemLeft" align="right" valign="top"><a id="ae77329ab259285b66bcae6dd4bd9b277"></a>
<a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt;&#160;</td><td class="memItemRight" valign="bottom"><b>dist_sq_</b></td></tr>
<tr class="separator:ae77329ab259285b66bcae6dd4bd9b277"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9166263402e5f7bbf2f992dbdc879cb3"><td class="memItemLeft" align="right" valign="top"><a id="a9166263402e5f7bbf2f992dbdc879cb3"></a>
<a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt;&#160;</td><td class="memItemRight" valign="bottom"><b>diff_sq_</b></td></tr>
<tr class="separator:a9166263402e5f7bbf2f992dbdc879cb3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4ccba3740b8c081d7e8ac9302984544d"><td class="memItemLeft" align="right" valign="top"><a id="a4ccba3740b8c081d7e8ac9302984544d"></a>
<a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt;&#160;</td><td class="memItemRight" valign="bottom"><b>summer_vec_</b></td></tr>
<tr class="separator:a4ccba3740b8c081d7e8ac9302984544d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pro_attribs_classcaffe_1_1_layer"><td colspan="2" onclick="javascript:toggleInherit('pro_attribs_classcaffe_1_1_layer')"><img src="closed.png" alt="-"/>&#160;Protected Attributes inherited from <a class="el" href="classcaffe_1_1_layer.html">caffe::Layer&lt; Dtype &gt;</a></td></tr>
<tr class="memitem:a7ed12bb2df25c887e41d7ea9557fc701 inherit pro_attribs_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classcaffe_1_1_layer_parameter.html">LayerParameter</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a7ed12bb2df25c887e41d7ea9557fc701">layer_param_</a></td></tr>
<tr class="separator:a7ed12bb2df25c887e41d7ea9557fc701 inherit pro_attribs_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1d04ad7f595a82a1c811f102d68b8a19 inherit pro_attribs_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">Phase&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a1d04ad7f595a82a1c811f102d68b8a19">phase_</a></td></tr>
<tr class="separator:a1d04ad7f595a82a1c811f102d68b8a19 inherit pro_attribs_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8d6998a5f8ca95990976021de743dd21 inherit pro_attribs_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">vector&lt; shared_ptr&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; &gt; &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a8d6998a5f8ca95990976021de743dd21">blobs_</a></td></tr>
<tr class="separator:a8d6998a5f8ca95990976021de743dd21 inherit pro_attribs_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab1db6c32fa71343dac868b07288eb45e inherit pro_attribs_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">vector&lt; bool &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#ab1db6c32fa71343dac868b07288eb45e">param_propagate_down_</a></td></tr>
<tr class="separator:ab1db6c32fa71343dac868b07288eb45e inherit pro_attribs_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5fbf5ce7385b2da3d8edc7eec3822ac7 inherit pro_attribs_classcaffe_1_1_layer"><td class="memItemLeft" align="right" valign="top">vector&lt; Dtype &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcaffe_1_1_layer.html#a5fbf5ce7385b2da3d8edc7eec3822ac7">loss_</a></td></tr>
<tr class="separator:a5fbf5ce7385b2da3d8edc7eec3822ac7 inherit pro_attribs_classcaffe_1_1_layer"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><h3>template&lt;typename Dtype&gt;<br />
class caffe::ContrastiveLossLayer&lt; Dtype &gt;</h3>

<p class="">Computes the contrastive loss <img class="formulaInl" alt="$ E = \frac{1}{2N} \sum\limits_{n=1}^N \left(y\right) d^2 + \left(1-y\right) \max \left(margin-d, 0\right)^2 $" src="form_51.png"/> where <img class="formulaInl" alt="$ d = \left| \left| a_n - b_n \right| \right|_2 $" src="form_52.png"/>. This can be used to train siamese networks. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">bottom</td><td>input <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> vector (length 3)<ol type="1">
<li><img class="formulaInl" alt="$ (N \times C \times 1 \times 1) $" src="form_53.png"/> the features <img class="formulaInl" alt="$ a \in [-\infty, +\infty]$" src="form_54.png"/></li>
<li><img class="formulaInl" alt="$ (N \times C \times 1 \times 1) $" src="form_53.png"/> the features <img class="formulaInl" alt="$ b \in [-\infty, +\infty]$" src="form_55.png"/></li>
<li><img class="formulaInl" alt="$ (N \times 1 \times 1 \times 1) $" src="form_22.png"/> the binary similarity <img class="formulaInl" alt="$ s \in [0, 1]$" src="form_56.png"/> </li>
</ol>
</td></tr>
    <tr><td class="paramname">top</td><td>output <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> vector (length 1)<ol type="1">
<li><img class="formulaInl" alt="$ (1 \times 1 \times 1 \times 1) $" src="form_26.png"/> the computed contrastive loss: <img class="formulaInl" alt="$ E = \frac{1}{2N} \sum\limits_{n=1}^N \left(y\right) d^2 + \left(1-y\right) \max \left(margin-d, 0\right)^2 $" src="form_51.png"/> where <img class="formulaInl" alt="$ d = \left| \left| a_n - b_n \right| \right|_2 $" src="form_52.png"/>. This can be used to train siamese networks. </li>
</ol>
</td></tr>
  </table>
  </dd>
</dl>
</div><h2 class="groupheader">Member Function Documentation</h2>
<a id="af0f16d5119ac6118b670c1966c38fd7d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af0f16d5119ac6118b670c1966c38fd7d">&#9670;&nbsp;</a></span>AllowForceBackward() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual bool <a class="el" href="classcaffe_1_1_contrastive_loss_layer.html">caffe::ContrastiveLossLayer</a>&lt; Dtype &gt;::AllowForceBackward </td>
          <td>(</td>
          <td class="paramtype">const int&#160;</td>
          <td class="paramname"><em>bottom_index</em></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<p class="">Unlike most loss layers, in the <a class="el" href="classcaffe_1_1_contrastive_loss_layer.html" title="Computes the contrastive loss  where . This can be used to train siamese networks. ">ContrastiveLossLayer</a> we can backpropagate to the first two inputs. </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1_loss_layer.html#a36d35155bfe0de53a79c517f33759612">caffe::LossLayer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a id="af0f16d5119ac6118b670c1966c38fd7d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af0f16d5119ac6118b670c1966c38fd7d">&#9670;&nbsp;</a></span>AllowForceBackward() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual bool <a class="el" href="classcaffe_1_1_contrastive_loss_layer.html">caffe::ContrastiveLossLayer</a>&lt; Dtype &gt;::AllowForceBackward </td>
          <td>(</td>
          <td class="paramtype">const int&#160;</td>
          <td class="paramname"><em>bottom_index</em></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<p class="">Unlike most loss layers, in the <a class="el" href="classcaffe_1_1_contrastive_loss_layer.html" title="Computes the contrastive loss  where . This can be used to train siamese networks. ">ContrastiveLossLayer</a> we can backpropagate to the first two inputs. </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1_loss_layer.html#a36d35155bfe0de53a79c517f33759612">caffe::LossLayer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a id="a60af9729fe340be3ae0f87737215d9d0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a60af9729fe340be3ae0f87737215d9d0">&#9670;&nbsp;</a></span>Backward_cpu() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classcaffe_1_1_contrastive_loss_layer.html">caffe::ContrastiveLossLayer</a>&lt; Dtype &gt;::Backward_cpu </td>
          <td>(</td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;&#160;</td>
          <td class="paramname"><em>top</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const vector&lt; bool &gt; &amp;&#160;</td>
          <td class="paramname"><em>propagate_down</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;&#160;</td>
          <td class="paramname"><em>bottom</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Computes the Contrastive error gradient w.r.t. the inputs. </p>
<p class="">Computes the gradients with respect to the two input vectors (bottom[0] and bottom[1]), but not the similarity label (bottom[2]).</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">top</td><td>output <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> vector (length 1), providing the error gradient with respect to the outputs<ol type="1">
<li><img class="formulaInl" alt="$ (1 \times 1 \times 1 \times 1) $" src="form_26.png"/> This <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a>'s diff will simply contain the loss_weight* <img class="formulaInl" alt="$ \lambda $" src="form_57.png"/>, as <img class="formulaInl" alt="$ \lambda $" src="form_57.png"/> is the coefficient of this layer's output <img class="formulaInl" alt="$\ell_i$" src="form_58.png"/> in the overall <a class="el" href="classcaffe_1_1_net.html" title="Connects Layers together into a directed acyclic graph (DAG) specified by a NetParameter. ">Net</a> loss <img class="formulaInl" alt="$ E = \lambda_i \ell_i + \mbox{other loss terms}$" src="form_59.png"/>; hence <img class="formulaInl" alt="$ \frac{\partial E}{\partial \ell_i} = \lambda_i $" src="form_60.png"/>. (*Assuming that this top <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> is not used as a bottom (input) by any other layer of the <a class="el" href="classcaffe_1_1_net.html" title="Connects Layers together into a directed acyclic graph (DAG) specified by a NetParameter. ">Net</a>.) </li>
</ol>
</td></tr>
    <tr><td class="paramname">propagate_down</td><td>see <a class="el" href="classcaffe_1_1_layer.html#a183d343f5183a4762307f2c5e6ed1e12" title="Given the top blob error gradients, compute the bottom blob error gradients. ">Layer::Backward</a>. </td></tr>
    <tr><td class="paramname">bottom</td><td>input <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> vector (length 2)<ol type="1">
<li><img class="formulaInl" alt="$ (N \times C \times 1 \times 1) $" src="form_53.png"/> the features <img class="formulaInl" alt="$a$" src="form_61.png"/>; Backward fills their diff with gradients if propagate_down[0]</li>
<li><img class="formulaInl" alt="$ (N \times C \times 1 \times 1) $" src="form_53.png"/> the features <img class="formulaInl" alt="$b$" src="form_62.png"/>; Backward fills their diff with gradients if propagate_down[1] </li>
</ol>
</td></tr>
  </table>
  </dd>
</dl>

<p>Implements <a class="el" href="classcaffe_1_1_layer.html#a75c9b2a321dc713e0eaef530d02dc37f">caffe::Layer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a id="a2c8e0737bba7568b172468be5c33d2a7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2c8e0737bba7568b172468be5c33d2a7">&#9670;&nbsp;</a></span>Backward_cpu() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual void <a class="el" href="classcaffe_1_1_contrastive_loss_layer.html">caffe::ContrastiveLossLayer</a>&lt; Dtype &gt;::Backward_cpu </td>
          <td>(</td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;&#160;</td>
          <td class="paramname"><em>top</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const vector&lt; bool &gt; &amp;&#160;</td>
          <td class="paramname"><em>propagate_down</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;&#160;</td>
          <td class="paramname"><em>bottom</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Computes the Contrastive error gradient w.r.t. the inputs. </p>
<p class="">Computes the gradients with respect to the two input vectors (bottom[0] and bottom[1]), but not the similarity label (bottom[2]).</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">top</td><td>output <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> vector (length 1), providing the error gradient with respect to the outputs<ol type="1">
<li><img class="formulaInl" alt="$ (1 \times 1 \times 1 \times 1) $" src="form_26.png"/> This <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a>'s diff will simply contain the loss_weight* <img class="formulaInl" alt="$ \lambda $" src="form_57.png"/>, as <img class="formulaInl" alt="$ \lambda $" src="form_57.png"/> is the coefficient of this layer's output <img class="formulaInl" alt="$\ell_i$" src="form_58.png"/> in the overall <a class="el" href="classcaffe_1_1_net.html" title="Connects Layers together into a directed acyclic graph (DAG) specified by a NetParameter. ">Net</a> loss <img class="formulaInl" alt="$ E = \lambda_i \ell_i + \mbox{other loss terms}$" src="form_59.png"/>; hence <img class="formulaInl" alt="$ \frac{\partial E}{\partial \ell_i} = \lambda_i $" src="form_60.png"/>. (*Assuming that this top <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> is not used as a bottom (input) by any other layer of the <a class="el" href="classcaffe_1_1_net.html" title="Connects Layers together into a directed acyclic graph (DAG) specified by a NetParameter. ">Net</a>.) </li>
</ol>
</td></tr>
    <tr><td class="paramname">propagate_down</td><td>see <a class="el" href="classcaffe_1_1_layer.html#a183d343f5183a4762307f2c5e6ed1e12" title="Given the top blob error gradients, compute the bottom blob error gradients. ">Layer::Backward</a>. </td></tr>
    <tr><td class="paramname">bottom</td><td>input <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> vector (length 2)<ol type="1">
<li><img class="formulaInl" alt="$ (N \times C \times 1 \times 1) $" src="form_53.png"/> the features <img class="formulaInl" alt="$a$" src="form_61.png"/>; Backward fills their diff with gradients if propagate_down[0]</li>
<li><img class="formulaInl" alt="$ (N \times C \times 1 \times 1) $" src="form_53.png"/> the features <img class="formulaInl" alt="$b$" src="form_62.png"/>; Backward fills their diff with gradients if propagate_down[1] </li>
</ol>
</td></tr>
  </table>
  </dd>
</dl>

<p>Implements <a class="el" href="classcaffe_1_1_layer.html#a75c9b2a321dc713e0eaef530d02dc37f">caffe::Layer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a id="aa6f3ad6918e64ffa1828e821accf25e9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa6f3ad6918e64ffa1828e821accf25e9">&#9670;&nbsp;</a></span>ExactNumBottomBlobs() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual int <a class="el" href="classcaffe_1_1_contrastive_loss_layer.html">caffe::ContrastiveLossLayer</a>&lt; Dtype &gt;::ExactNumBottomBlobs </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns the exact number of bottom blobs required by the layer, or -1 if no exact number is required. </p>
<p class="">This method should be overridden to return a non-negative value if your layer expects some exact number of bottom blobs. </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1_loss_layer.html#af1620064baefb711e2c767bdc92b6fb1">caffe::LossLayer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a id="aa6f3ad6918e64ffa1828e821accf25e9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa6f3ad6918e64ffa1828e821accf25e9">&#9670;&nbsp;</a></span>ExactNumBottomBlobs() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual int <a class="el" href="classcaffe_1_1_contrastive_loss_layer.html">caffe::ContrastiveLossLayer</a>&lt; Dtype &gt;::ExactNumBottomBlobs </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Returns the exact number of bottom blobs required by the layer, or -1 if no exact number is required. </p>
<p class="">This method should be overridden to return a non-negative value if your layer expects some exact number of bottom blobs. </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1_loss_layer.html#af1620064baefb711e2c767bdc92b6fb1">caffe::LossLayer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a id="ae55966330621c1a2bd5a0012f2b09fe4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae55966330621c1a2bd5a0012f2b09fe4">&#9670;&nbsp;</a></span>Forward_cpu() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classcaffe_1_1_contrastive_loss_layer.html">caffe::ContrastiveLossLayer</a>&lt; Dtype &gt;::Forward_cpu </td>
          <td>(</td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;&#160;</td>
          <td class="paramname"><em>bottom</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;&#160;</td>
          <td class="paramname"><em>top</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Computes the contrastive loss <img class="formulaInl" alt="$ E = \frac{1}{2N} \sum\limits_{n=1}^N \left(y\right) d^2 + \left(1-y\right) \max \left(margin-d, 0\right)^2 $" src="form_51.png"/> where <img class="formulaInl" alt="$ d = \left| \left| a_n - b_n \right| \right|_2 $" src="form_52.png"/>. This can be used to train siamese networks. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">bottom</td><td>input <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> vector (length 3)<ol type="1">
<li><img class="formulaInl" alt="$ (N \times C \times 1 \times 1) $" src="form_53.png"/> the features <img class="formulaInl" alt="$ a \in [-\infty, +\infty]$" src="form_54.png"/></li>
<li><img class="formulaInl" alt="$ (N \times C \times 1 \times 1) $" src="form_53.png"/> the features <img class="formulaInl" alt="$ b \in [-\infty, +\infty]$" src="form_55.png"/></li>
<li><img class="formulaInl" alt="$ (N \times 1 \times 1 \times 1) $" src="form_22.png"/> the binary similarity <img class="formulaInl" alt="$ s \in [0, 1]$" src="form_56.png"/> </li>
</ol>
</td></tr>
    <tr><td class="paramname">top</td><td>output <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> vector (length 1)<ol type="1">
<li><img class="formulaInl" alt="$ (1 \times 1 \times 1 \times 1) $" src="form_26.png"/> the computed contrastive loss: <img class="formulaInl" alt="$ E = \frac{1}{2N} \sum\limits_{n=1}^N \left(y\right) d^2 + \left(1-y\right) \max \left(margin-d, 0\right)^2 $" src="form_51.png"/> where <img class="formulaInl" alt="$ d = \left| \left| a_n - b_n \right| \right|_2 $" src="form_52.png"/>. This can be used to train siamese networks. </li>
</ol>
</td></tr>
  </table>
  </dd>
</dl>

<p>Implements <a class="el" href="classcaffe_1_1_layer.html#a576ac6a60b1e99fe383831f52a6cea77">caffe::Layer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a id="aad90e509c2f7ebd3a36054101d1d15fb"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aad90e509c2f7ebd3a36054101d1d15fb">&#9670;&nbsp;</a></span>Forward_cpu() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual void <a class="el" href="classcaffe_1_1_contrastive_loss_layer.html">caffe::ContrastiveLossLayer</a>&lt; Dtype &gt;::Forward_cpu </td>
          <td>(</td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;&#160;</td>
          <td class="paramname"><em>bottom</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;&#160;</td>
          <td class="paramname"><em>top</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">protected</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Computes the contrastive loss <img class="formulaInl" alt="$ E = \frac{1}{2N} \sum\limits_{n=1}^N \left(y\right) d^2 + \left(1-y\right) \max \left(margin-d, 0\right)^2 $" src="form_51.png"/> where <img class="formulaInl" alt="$ d = \left| \left| a_n - b_n \right| \right|_2 $" src="form_52.png"/>. This can be used to train siamese networks. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">bottom</td><td>input <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> vector (length 3)<ol type="1">
<li><img class="formulaInl" alt="$ (N \times C \times 1 \times 1) $" src="form_53.png"/> the features <img class="formulaInl" alt="$ a \in [-\infty, +\infty]$" src="form_54.png"/></li>
<li><img class="formulaInl" alt="$ (N \times C \times 1 \times 1) $" src="form_53.png"/> the features <img class="formulaInl" alt="$ b \in [-\infty, +\infty]$" src="form_55.png"/></li>
<li><img class="formulaInl" alt="$ (N \times 1 \times 1 \times 1) $" src="form_22.png"/> the binary similarity <img class="formulaInl" alt="$ s \in [0, 1]$" src="form_56.png"/> </li>
</ol>
</td></tr>
    <tr><td class="paramname">top</td><td>output <a class="el" href="classcaffe_1_1_blob.html" title="A wrapper around SyncedMemory holders serving as the basic computational unit through which Layers...">Blob</a> vector (length 1)<ol type="1">
<li><img class="formulaInl" alt="$ (1 \times 1 \times 1 \times 1) $" src="form_26.png"/> the computed contrastive loss: <img class="formulaInl" alt="$ E = \frac{1}{2N} \sum\limits_{n=1}^N \left(y\right) d^2 + \left(1-y\right) \max \left(margin-d, 0\right)^2 $" src="form_51.png"/> where <img class="formulaInl" alt="$ d = \left| \left| a_n - b_n \right| \right|_2 $" src="form_52.png"/>. This can be used to train siamese networks. </li>
</ol>
</td></tr>
  </table>
  </dd>
</dl>

<p>Implements <a class="el" href="classcaffe_1_1_layer.html#a576ac6a60b1e99fe383831f52a6cea77">caffe::Layer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a id="a957623c05cb2289cd2ae9e9e93b48969"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a957623c05cb2289cd2ae9e9e93b48969">&#9670;&nbsp;</a></span>LayerSetUp() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype&gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual void <a class="el" href="classcaffe_1_1_contrastive_loss_layer.html">caffe::ContrastiveLossLayer</a>&lt; Dtype &gt;::LayerSetUp </td>
          <td>(</td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;&#160;</td>
          <td class="paramname"><em>bottom</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;&#160;</td>
          <td class="paramname"><em>top</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Does layer-specific setup: your layer should implement this function as well as Reshape. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">bottom</td><td>the preshaped input blobs, whose data fields store the input data for this layer </td></tr>
    <tr><td class="paramname">top</td><td>the allocated but unshaped output blobs</td></tr>
  </table>
  </dd>
</dl>
<p>This method should do one-time layer specific setup. This includes reading and processing relevent parameters from the <code>layer_param_</code>. Setting up the shapes of top blobs and internal buffers should be done in <code>Reshape</code>, which will be called before the forward pass to adjust the top blob sizes. </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1_loss_layer.html#aa6fc7c2e90be66f1c1f0683637c949da">caffe::LossLayer&lt; Dtype &gt;</a>.</p>

</div>
</div>
<a id="a943e67e7bb9c2362ec20ce44c777beac"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a943e67e7bb9c2362ec20ce44c777beac">&#9670;&nbsp;</a></span>LayerSetUp() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Dtype &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void <a class="el" href="classcaffe_1_1_contrastive_loss_layer.html">caffe::ContrastiveLossLayer</a>&lt; Dtype &gt;::LayerSetUp </td>
          <td>(</td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;&#160;</td>
          <td class="paramname"><em>bottom</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const vector&lt; <a class="el" href="classcaffe_1_1_blob.html">Blob</a>&lt; Dtype &gt; *&gt; &amp;&#160;</td>
          <td class="paramname"><em>top</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Does layer-specific setup: your layer should implement this function as well as Reshape. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">bottom</td><td>the preshaped input blobs, whose data fields store the input data for this layer </td></tr>
    <tr><td class="paramname">top</td><td>the allocated but unshaped output blobs</td></tr>
  </table>
  </dd>
</dl>
<p>This method should do one-time layer specific setup. This includes reading and processing relevent parameters from the <code>layer_param_</code>. Setting up the shapes of top blobs and internal buffers should be done in <code>Reshape</code>, which will be called before the forward pass to adjust the top blob sizes. </p>

<p>Reimplemented from <a class="el" href="classcaffe_1_1_loss_layer.html#aa6fc7c2e90be66f1c1f0683637c949da">caffe::LossLayer&lt; Dtype &gt;</a>.</p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="classcaffe_1_1_contrastive_loss_layer_a943e67e7bb9c2362ec20ce44c777beac_cgraph.png" border="0" usemap="#classcaffe_1_1_contrastive_loss_layer_a943e67e7bb9c2362ec20ce44c777beac_cgraph" alt=""/></div>
<map name="classcaffe_1_1_contrastive_loss_layer_a943e67e7bb9c2362ec20ce44c777beac_cgraph" id="classcaffe_1_1_contrastive_loss_layer_a943e67e7bb9c2362ec20ce44c777beac_cgraph">
<area shape="rect" id="node2" href="classcaffe_1_1_loss_layer.html#aa6fc7c2e90be66f1c1f0683637c949da" title="Does layer&#45;specific setup: your layer should implement this function as well as Reshape. " alt="" coords="240,13,436,39"/>
</map>
</div>

</div>
</div>
<hr/>The documentation for this class was generated from the following files:<ul>
<li>build/install/include/caffe/layers/<a class="el" href="build_2install_2include_2caffe_2layers_2contrastive__loss__layer_8hpp_source.html">contrastive_loss_layer.hpp</a></li>
<li>src/caffe/layers/contrastive_loss_layer.cpp</li>
</ul>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="namespacecaffe.html">caffe</a></li><li class="navelem"><a class="el" href="classcaffe_1_1_contrastive_loss_layer.html">ContrastiveLossLayer</a></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.15 </li>
  </ul>
</div>
</body>
</html>
