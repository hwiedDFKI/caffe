

 \subsection*{title\+: Softmax with Loss Layer }

\section*{Softmax with Loss Layer}


\begin{DoxyItemize}
\item Layer type\+: {\ttfamily Softmax\+With\+Loss}
\item \href{http://caffe.berkeleyvision.org/doxygen/classcaffe_1_1SoftmaxWithLossLayer.html}{\tt Doxygen Documentation}
\item Header\+: \href{https://github.com/BVLC/caffe/blob/master/include/caffe/layers/softmax_loss_layer.hpp}{\tt {\ttfamily ./include/caffe/layers/softmax\+\_\+loss\+\_\+layer.hpp}}
\item C\+PU implementation\+: \href{https://github.com/BVLC/caffe/blob/master/src/caffe/layers/softmax_loss_layer.cpp}{\tt {\ttfamily ./src/caffe/layers/softmax\+\_\+loss\+\_\+layer.cpp}}
\item C\+U\+DA G\+PU implementation\+: \href{https://github.com/BVLC/caffe/blob/master/src/caffe/layers/softmax_loss_layer.cu}{\tt {\ttfamily ./src/caffe/layers/softmax\+\_\+loss\+\_\+layer.cu}}
\end{DoxyItemize}

The softmax loss layer computes the multinomial logistic loss of the softmax of its inputs. It\textquotesingle{}s conceptually identical to a softmax layer followed by a multinomial logistic loss layer, but provides a more numerically stable gradient.

\subsection*{Parameters}


\begin{DoxyItemize}
\item Parameters ({\ttfamily Softmax\+Parameter softmax\+\_\+param})
\item From \href{https://github.com/BVLC/caffe/blob/master/src/caffe/proto/caffe.proto}{\tt {\ttfamily ./src/caffe/proto/caffe.proto}}\+:
\end{DoxyItemize}

\{\% highlight Protobuf \%\} \{\% include proto/\+Softmax\+Parameter.\+txt \%\} \{\% endhighlight \%\}


\begin{DoxyItemize}
\item Parameters ({\ttfamily Loss\+Parameter loss\+\_\+param})
\item From \href{https://github.com/BVLC/caffe/blob/master/src/caffe/proto/caffe.proto}{\tt {\ttfamily ./src/caffe/proto/caffe.proto}}\+:
\end{DoxyItemize}

\{\% highlight Protobuf \%\} \{\% include proto/\+Loss\+Parameter.\+txt \%\} \{\% endhighlight \%\}

\subsection*{See also}


\begin{DoxyItemize}
\item \href{softmax.html}{\tt Softmax layer} 
\end{DoxyItemize}