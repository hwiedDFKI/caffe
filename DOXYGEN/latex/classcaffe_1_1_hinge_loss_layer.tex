\hypertarget{classcaffe_1_1_hinge_loss_layer}{}\section{caffe\+:\+:Hinge\+Loss\+Layer$<$ Dtype $>$ Class Template Reference}
\label{classcaffe_1_1_hinge_loss_layer}\index{caffe\+::\+Hinge\+Loss\+Layer$<$ Dtype $>$@{caffe\+::\+Hinge\+Loss\+Layer$<$ Dtype $>$}}


Computes the hinge loss for a one-\/of-\/many classification task.  




{\ttfamily \#include $<$hinge\+\_\+loss\+\_\+layer.\+hpp$>$}



Inheritance diagram for caffe\+:\+:Hinge\+Loss\+Layer$<$ Dtype $>$\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=214pt]{classcaffe_1_1_hinge_loss_layer__inherit__graph}
\end{center}
\end{figure}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\Hypertarget{classcaffe_1_1_hinge_loss_layer_a358a5bd2625bb7fed61052dd8e1cb588}\label{classcaffe_1_1_hinge_loss_layer_a358a5bd2625bb7fed61052dd8e1cb588}} 
{\bfseries Hinge\+Loss\+Layer} (const \mbox{\hyperlink{classcaffe_1_1_layer_parameter}{Layer\+Parameter}} \&param)
\item 
\mbox{\Hypertarget{classcaffe_1_1_hinge_loss_layer_a9bee53540accf1ad93e68bfcf0302c1f}\label{classcaffe_1_1_hinge_loss_layer_a9bee53540accf1ad93e68bfcf0302c1f}} 
virtual const char $\ast$ \mbox{\hyperlink{classcaffe_1_1_hinge_loss_layer_a9bee53540accf1ad93e68bfcf0302c1f}{type}} () const
\begin{DoxyCompactList}\small\item\em Returns the layer type. \end{DoxyCompactList}\item 
\mbox{\Hypertarget{classcaffe_1_1_hinge_loss_layer_a358a5bd2625bb7fed61052dd8e1cb588}\label{classcaffe_1_1_hinge_loss_layer_a358a5bd2625bb7fed61052dd8e1cb588}} 
{\bfseries Hinge\+Loss\+Layer} (const \mbox{\hyperlink{classcaffe_1_1_layer_parameter}{Layer\+Parameter}} \&param)
\item 
\mbox{\Hypertarget{classcaffe_1_1_hinge_loss_layer_a9bee53540accf1ad93e68bfcf0302c1f}\label{classcaffe_1_1_hinge_loss_layer_a9bee53540accf1ad93e68bfcf0302c1f}} 
virtual const char $\ast$ \mbox{\hyperlink{classcaffe_1_1_hinge_loss_layer_a9bee53540accf1ad93e68bfcf0302c1f}{type}} () const
\begin{DoxyCompactList}\small\item\em Returns the layer type. \end{DoxyCompactList}\end{DoxyCompactItemize}
\subsection*{Protected Member Functions}
\begin{DoxyCompactItemize}
\item 
virtual void \mbox{\hyperlink{classcaffe_1_1_hinge_loss_layer_ab10e623d1ffbef7ba7ceeaf225bca428}{Forward\+\_\+cpu}} (const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&bottom, const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&top)
\begin{DoxyCompactList}\small\item\em Computes the hinge loss for a one-\/of-\/many classification task. \end{DoxyCompactList}\item 
virtual void \mbox{\hyperlink{classcaffe_1_1_hinge_loss_layer_a4eb93cbad1070ce4e6e4de8fa1eb01ab}{Backward\+\_\+cpu}} (const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&top, const vector$<$ bool $>$ \&propagate\+\_\+down, const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&bottom)
\begin{DoxyCompactList}\small\item\em Computes the hinge loss error gradient w.\+r.\+t. the predictions. \end{DoxyCompactList}\item 
virtual void \mbox{\hyperlink{classcaffe_1_1_hinge_loss_layer_a8e8e160c36e0f3d1f1ab60e623506ec1}{Forward\+\_\+cpu}} (const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&bottom, const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&top)
\begin{DoxyCompactList}\small\item\em Computes the hinge loss for a one-\/of-\/many classification task. \end{DoxyCompactList}\item 
virtual void \mbox{\hyperlink{classcaffe_1_1_hinge_loss_layer_a1e16888f22c7ace492343a96b1f48eb8}{Backward\+\_\+cpu}} (const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&top, const vector$<$ bool $>$ \&propagate\+\_\+down, const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&bottom)
\begin{DoxyCompactList}\small\item\em Computes the hinge loss error gradient w.\+r.\+t. the predictions. \end{DoxyCompactList}\end{DoxyCompactItemize}
\subsection*{Additional Inherited Members}


\subsection{Detailed Description}
\subsubsection*{template$<$typename Dtype$>$\newline
class caffe\+::\+Hinge\+Loss\+Layer$<$ Dtype $>$}

Computes the hinge loss for a one-\/of-\/many classification task. 


\begin{DoxyParams}{Parameters}
{\em bottom} & input \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 2)
\begin{DoxyEnumerate}
\item $ (N \times C \times H \times W) $ the predictions $ t $, a \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} with values in $ [-\infty, +\infty] $ indicating the predicted score for each of the $ K = CHW $ classes. In an S\+VM, $ t $ is the result of taking the inner product $ X^T W $ of the D-\/dimensional features $ X \in \mathcal{R}^{D \times N} $ and the learned hyperplane parameters $ W \in \mathcal{R}^{D \times K} $, so a \mbox{\hyperlink{classcaffe_1_1_net}{Net}} with just an \mbox{\hyperlink{classcaffe_1_1_inner_product_layer}{Inner\+Product\+Layer}} (with num\+\_\+output = D) providing predictions to a \mbox{\hyperlink{classcaffe_1_1_hinge_loss_layer}{Hinge\+Loss\+Layer}} and no other learnable parameters or losses is equivalent to an S\+VM.
\item $ (N \times 1 \times 1 \times 1) $ the labels $ l $, an integer-\/valued \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} with values $ l_n \in [0, 1, 2, ..., K - 1] $ indicating the correct class label among the $ K $ classes 
\end{DoxyEnumerate}\\
\hline
{\em top} & output \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 1)
\begin{DoxyEnumerate}
\item $ (1 \times 1 \times 1 \times 1) $ the computed hinge loss\+: $ E = \frac{1}{N} \sum\limits_{n=1}^N \sum\limits_{k=1}^K [\max(0, 1 - \delta\{l_n = k\} t_{nk})] ^ p $, for the $ L^p $ norm (defaults to $ p = 1 $, the L1 norm; L2 norm, as in L2-\/\+S\+VM, is also available), and $ \delta\{\mathrm{condition}\} = \left\{ \begin{array}{lr} 1 & \mbox{if condition} \\ -1 & \mbox{otherwise} \end{array} \right. $
\end{DoxyEnumerate}\\
\hline
\end{DoxyParams}
In an S\+VM, $ t \in \mathcal{R}^{N \times K} $ is the result of taking the inner product $ X^T W $ of the features $ X \in \mathcal{R}^{D \times N} $ and the learned hyperplane parameters $ W \in \mathcal{R}^{D \times K} $. So, a \mbox{\hyperlink{classcaffe_1_1_net}{Net}} with just an \mbox{\hyperlink{classcaffe_1_1_inner_product_layer}{Inner\+Product\+Layer}} (with num\+\_\+output = $k$) providing predictions to a \mbox{\hyperlink{classcaffe_1_1_hinge_loss_layer}{Hinge\+Loss\+Layer}} is equivalent to an S\+VM (assuming it has no other learned outside the \mbox{\hyperlink{classcaffe_1_1_inner_product_layer}{Inner\+Product\+Layer}} and no other losses outside the \mbox{\hyperlink{classcaffe_1_1_hinge_loss_layer}{Hinge\+Loss\+Layer}}). 

\subsection{Member Function Documentation}
\mbox{\Hypertarget{classcaffe_1_1_hinge_loss_layer_a4eb93cbad1070ce4e6e4de8fa1eb01ab}\label{classcaffe_1_1_hinge_loss_layer_a4eb93cbad1070ce4e6e4de8fa1eb01ab}} 
\index{caffe\+::\+Hinge\+Loss\+Layer@{caffe\+::\+Hinge\+Loss\+Layer}!Backward\+\_\+cpu@{Backward\+\_\+cpu}}
\index{Backward\+\_\+cpu@{Backward\+\_\+cpu}!caffe\+::\+Hinge\+Loss\+Layer@{caffe\+::\+Hinge\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Backward\+\_\+cpu()}{Backward\_cpu()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily template$<$typename Dtype $>$ \\
void \mbox{\hyperlink{classcaffe_1_1_hinge_loss_layer}{caffe\+::\+Hinge\+Loss\+Layer}}$<$ Dtype $>$\+::Backward\+\_\+cpu (\begin{DoxyParamCaption}\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{top,  }\item[{const vector$<$ bool $>$ \&}]{propagate\+\_\+down,  }\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{bottom }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}, {\ttfamily [virtual]}}



Computes the hinge loss error gradient w.\+r.\+t. the predictions. 

Gradients cannot be computed with respect to the label inputs (bottom\mbox{[}1\mbox{]}), so this method ignores bottom\mbox{[}1\mbox{]} and requires !propagate\+\_\+down\mbox{[}1\mbox{]}, crashing if propagate\+\_\+down\mbox{[}1\mbox{]} is set.


\begin{DoxyParams}{Parameters}
{\em top} & output \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 1), providing the error gradient with respect to the outputs
\begin{DoxyEnumerate}
\item $ (1 \times 1 \times 1 \times 1) $ This \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}\textquotesingle{}s diff will simply contain the loss\+\_\+weight$\ast$ $ \lambda $, as $ \lambda $ is the coefficient of this layer\textquotesingle{}s output $\ell_i$ in the overall \mbox{\hyperlink{classcaffe_1_1_net}{Net}} loss $ E = \lambda_i \ell_i + \mbox{other loss terms}$; hence $ \frac{\partial E}{\partial \ell_i} = \lambda_i $. ($\ast$\+Assuming that this top \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} is not used as a bottom (input) by any other layer of the \mbox{\hyperlink{classcaffe_1_1_net}{Net}}.) 
\end{DoxyEnumerate}\\
\hline
{\em propagate\+\_\+down} & see \mbox{\hyperlink{classcaffe_1_1_layer_a183d343f5183a4762307f2c5e6ed1e12}{Layer\+::\+Backward}}. propagate\+\_\+down\mbox{[}1\mbox{]} must be false as we can\textquotesingle{}t compute gradients with respect to the labels. \\
\hline
{\em bottom} & input \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 2)
\begin{DoxyEnumerate}
\item $ (N \times C \times H \times W) $ the predictions $t$; Backward computes diff $ \frac{\partial E}{\partial t} $
\item $ (N \times 1 \times 1 \times 1) $ the labels -- ignored as we can\textquotesingle{}t compute their error gradients 
\end{DoxyEnumerate}\\
\hline
\end{DoxyParams}


Implements \mbox{\hyperlink{classcaffe_1_1_layer_a75c9b2a321dc713e0eaef530d02dc37f}{caffe\+::\+Layer$<$ Dtype $>$}}.

\mbox{\Hypertarget{classcaffe_1_1_hinge_loss_layer_a1e16888f22c7ace492343a96b1f48eb8}\label{classcaffe_1_1_hinge_loss_layer_a1e16888f22c7ace492343a96b1f48eb8}} 
\index{caffe\+::\+Hinge\+Loss\+Layer@{caffe\+::\+Hinge\+Loss\+Layer}!Backward\+\_\+cpu@{Backward\+\_\+cpu}}
\index{Backward\+\_\+cpu@{Backward\+\_\+cpu}!caffe\+::\+Hinge\+Loss\+Layer@{caffe\+::\+Hinge\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Backward\+\_\+cpu()}{Backward\_cpu()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily template$<$typename Dtype$>$ \\
virtual void \mbox{\hyperlink{classcaffe_1_1_hinge_loss_layer}{caffe\+::\+Hinge\+Loss\+Layer}}$<$ Dtype $>$\+::Backward\+\_\+cpu (\begin{DoxyParamCaption}\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{top,  }\item[{const vector$<$ bool $>$ \&}]{propagate\+\_\+down,  }\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{bottom }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}, {\ttfamily [virtual]}}



Computes the hinge loss error gradient w.\+r.\+t. the predictions. 

Gradients cannot be computed with respect to the label inputs (bottom\mbox{[}1\mbox{]}), so this method ignores bottom\mbox{[}1\mbox{]} and requires !propagate\+\_\+down\mbox{[}1\mbox{]}, crashing if propagate\+\_\+down\mbox{[}1\mbox{]} is set.


\begin{DoxyParams}{Parameters}
{\em top} & output \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 1), providing the error gradient with respect to the outputs
\begin{DoxyEnumerate}
\item $ (1 \times 1 \times 1 \times 1) $ This \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}\textquotesingle{}s diff will simply contain the loss\+\_\+weight$\ast$ $ \lambda $, as $ \lambda $ is the coefficient of this layer\textquotesingle{}s output $\ell_i$ in the overall \mbox{\hyperlink{classcaffe_1_1_net}{Net}} loss $ E = \lambda_i \ell_i + \mbox{other loss terms}$; hence $ \frac{\partial E}{\partial \ell_i} = \lambda_i $. ($\ast$\+Assuming that this top \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} is not used as a bottom (input) by any other layer of the \mbox{\hyperlink{classcaffe_1_1_net}{Net}}.) 
\end{DoxyEnumerate}\\
\hline
{\em propagate\+\_\+down} & see \mbox{\hyperlink{classcaffe_1_1_layer_a183d343f5183a4762307f2c5e6ed1e12}{Layer\+::\+Backward}}. propagate\+\_\+down\mbox{[}1\mbox{]} must be false as we can\textquotesingle{}t compute gradients with respect to the labels. \\
\hline
{\em bottom} & input \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 2)
\begin{DoxyEnumerate}
\item $ (N \times C \times H \times W) $ the predictions $t$; Backward computes diff $ \frac{\partial E}{\partial t} $
\item $ (N \times 1 \times 1 \times 1) $ the labels -- ignored as we can\textquotesingle{}t compute their error gradients 
\end{DoxyEnumerate}\\
\hline
\end{DoxyParams}


Implements \mbox{\hyperlink{classcaffe_1_1_layer_a75c9b2a321dc713e0eaef530d02dc37f}{caffe\+::\+Layer$<$ Dtype $>$}}.

\mbox{\Hypertarget{classcaffe_1_1_hinge_loss_layer_a8e8e160c36e0f3d1f1ab60e623506ec1}\label{classcaffe_1_1_hinge_loss_layer_a8e8e160c36e0f3d1f1ab60e623506ec1}} 
\index{caffe\+::\+Hinge\+Loss\+Layer@{caffe\+::\+Hinge\+Loss\+Layer}!Forward\+\_\+cpu@{Forward\+\_\+cpu}}
\index{Forward\+\_\+cpu@{Forward\+\_\+cpu}!caffe\+::\+Hinge\+Loss\+Layer@{caffe\+::\+Hinge\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Forward\+\_\+cpu()}{Forward\_cpu()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily template$<$typename Dtype$>$ \\
virtual void \mbox{\hyperlink{classcaffe_1_1_hinge_loss_layer}{caffe\+::\+Hinge\+Loss\+Layer}}$<$ Dtype $>$\+::Forward\+\_\+cpu (\begin{DoxyParamCaption}\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{bottom,  }\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{top }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}, {\ttfamily [virtual]}}



Computes the hinge loss for a one-\/of-\/many classification task. 


\begin{DoxyParams}{Parameters}
{\em bottom} & input \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 2)
\begin{DoxyEnumerate}
\item $ (N \times C \times H \times W) $ the predictions $ t $, a \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} with values in $ [-\infty, +\infty] $ indicating the predicted score for each of the $ K = CHW $ classes. In an S\+VM, $ t $ is the result of taking the inner product $ X^T W $ of the D-\/dimensional features $ X \in \mathcal{R}^{D \times N} $ and the learned hyperplane parameters $ W \in \mathcal{R}^{D \times K} $, so a \mbox{\hyperlink{classcaffe_1_1_net}{Net}} with just an \mbox{\hyperlink{classcaffe_1_1_inner_product_layer}{Inner\+Product\+Layer}} (with num\+\_\+output = D) providing predictions to a \mbox{\hyperlink{classcaffe_1_1_hinge_loss_layer}{Hinge\+Loss\+Layer}} and no other learnable parameters or losses is equivalent to an S\+VM.
\item $ (N \times 1 \times 1 \times 1) $ the labels $ l $, an integer-\/valued \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} with values $ l_n \in [0, 1, 2, ..., K - 1] $ indicating the correct class label among the $ K $ classes 
\end{DoxyEnumerate}\\
\hline
{\em top} & output \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 1)
\begin{DoxyEnumerate}
\item $ (1 \times 1 \times 1 \times 1) $ the computed hinge loss\+: $ E = \frac{1}{N} \sum\limits_{n=1}^N \sum\limits_{k=1}^K [\max(0, 1 - \delta\{l_n = k\} t_{nk})] ^ p $, for the $ L^p $ norm (defaults to $ p = 1 $, the L1 norm; L2 norm, as in L2-\/\+S\+VM, is also available), and $ \delta\{\mathrm{condition}\} = \left\{ \begin{array}{lr} 1 & \mbox{if condition} \\ -1 & \mbox{otherwise} \end{array} \right. $
\end{DoxyEnumerate}\\
\hline
\end{DoxyParams}
In an S\+VM, $ t \in \mathcal{R}^{N \times K} $ is the result of taking the inner product $ X^T W $ of the features $ X \in \mathcal{R}^{D \times N} $ and the learned hyperplane parameters $ W \in \mathcal{R}^{D \times K} $. So, a \mbox{\hyperlink{classcaffe_1_1_net}{Net}} with just an \mbox{\hyperlink{classcaffe_1_1_inner_product_layer}{Inner\+Product\+Layer}} (with num\+\_\+output = $k$) providing predictions to a \mbox{\hyperlink{classcaffe_1_1_hinge_loss_layer}{Hinge\+Loss\+Layer}} is equivalent to an S\+VM (assuming it has no other learned outside the \mbox{\hyperlink{classcaffe_1_1_inner_product_layer}{Inner\+Product\+Layer}} and no other losses outside the \mbox{\hyperlink{classcaffe_1_1_hinge_loss_layer}{Hinge\+Loss\+Layer}}). 

Implements \mbox{\hyperlink{classcaffe_1_1_layer_a576ac6a60b1e99fe383831f52a6cea77}{caffe\+::\+Layer$<$ Dtype $>$}}.

\mbox{\Hypertarget{classcaffe_1_1_hinge_loss_layer_ab10e623d1ffbef7ba7ceeaf225bca428}\label{classcaffe_1_1_hinge_loss_layer_ab10e623d1ffbef7ba7ceeaf225bca428}} 
\index{caffe\+::\+Hinge\+Loss\+Layer@{caffe\+::\+Hinge\+Loss\+Layer}!Forward\+\_\+cpu@{Forward\+\_\+cpu}}
\index{Forward\+\_\+cpu@{Forward\+\_\+cpu}!caffe\+::\+Hinge\+Loss\+Layer@{caffe\+::\+Hinge\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Forward\+\_\+cpu()}{Forward\_cpu()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily template$<$typename Dtype $>$ \\
void \mbox{\hyperlink{classcaffe_1_1_hinge_loss_layer}{caffe\+::\+Hinge\+Loss\+Layer}}$<$ Dtype $>$\+::Forward\+\_\+cpu (\begin{DoxyParamCaption}\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{bottom,  }\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{top }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}, {\ttfamily [virtual]}}



Computes the hinge loss for a one-\/of-\/many classification task. 


\begin{DoxyParams}{Parameters}
{\em bottom} & input \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 2)
\begin{DoxyEnumerate}
\item $ (N \times C \times H \times W) $ the predictions $ t $, a \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} with values in $ [-\infty, +\infty] $ indicating the predicted score for each of the $ K = CHW $ classes. In an S\+VM, $ t $ is the result of taking the inner product $ X^T W $ of the D-\/dimensional features $ X \in \mathcal{R}^{D \times N} $ and the learned hyperplane parameters $ W \in \mathcal{R}^{D \times K} $, so a \mbox{\hyperlink{classcaffe_1_1_net}{Net}} with just an \mbox{\hyperlink{classcaffe_1_1_inner_product_layer}{Inner\+Product\+Layer}} (with num\+\_\+output = D) providing predictions to a \mbox{\hyperlink{classcaffe_1_1_hinge_loss_layer}{Hinge\+Loss\+Layer}} and no other learnable parameters or losses is equivalent to an S\+VM.
\item $ (N \times 1 \times 1 \times 1) $ the labels $ l $, an integer-\/valued \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} with values $ l_n \in [0, 1, 2, ..., K - 1] $ indicating the correct class label among the $ K $ classes 
\end{DoxyEnumerate}\\
\hline
{\em top} & output \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 1)
\begin{DoxyEnumerate}
\item $ (1 \times 1 \times 1 \times 1) $ the computed hinge loss\+: $ E = \frac{1}{N} \sum\limits_{n=1}^N \sum\limits_{k=1}^K [\max(0, 1 - \delta\{l_n = k\} t_{nk})] ^ p $, for the $ L^p $ norm (defaults to $ p = 1 $, the L1 norm; L2 norm, as in L2-\/\+S\+VM, is also available), and $ \delta\{\mathrm{condition}\} = \left\{ \begin{array}{lr} 1 & \mbox{if condition} \\ -1 & \mbox{otherwise} \end{array} \right. $
\end{DoxyEnumerate}\\
\hline
\end{DoxyParams}
In an S\+VM, $ t \in \mathcal{R}^{N \times K} $ is the result of taking the inner product $ X^T W $ of the features $ X \in \mathcal{R}^{D \times N} $ and the learned hyperplane parameters $ W \in \mathcal{R}^{D \times K} $. So, a \mbox{\hyperlink{classcaffe_1_1_net}{Net}} with just an \mbox{\hyperlink{classcaffe_1_1_inner_product_layer}{Inner\+Product\+Layer}} (with num\+\_\+output = $k$) providing predictions to a \mbox{\hyperlink{classcaffe_1_1_hinge_loss_layer}{Hinge\+Loss\+Layer}} is equivalent to an S\+VM (assuming it has no other learned outside the \mbox{\hyperlink{classcaffe_1_1_inner_product_layer}{Inner\+Product\+Layer}} and no other losses outside the \mbox{\hyperlink{classcaffe_1_1_hinge_loss_layer}{Hinge\+Loss\+Layer}}). 

Implements \mbox{\hyperlink{classcaffe_1_1_layer_a576ac6a60b1e99fe383831f52a6cea77}{caffe\+::\+Layer$<$ Dtype $>$}}.



The documentation for this class was generated from the following files\+:\begin{DoxyCompactItemize}
\item 
build/install/include/caffe/layers/hinge\+\_\+loss\+\_\+layer.\+hpp\item 
src/caffe/layers/hinge\+\_\+loss\+\_\+layer.\+cpp\end{DoxyCompactItemize}
