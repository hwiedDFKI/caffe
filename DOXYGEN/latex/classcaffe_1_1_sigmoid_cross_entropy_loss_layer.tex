\hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer}{}\section{caffe\+:\+:Sigmoid\+Cross\+Entropy\+Loss\+Layer$<$ Dtype $>$ Class Template Reference}
\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer}\index{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer$<$ Dtype $>$@{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer$<$ Dtype $>$}}


Computes the cross-\/entropy (logistic) loss $ E = \frac{-1}{n} \sum\limits_{n=1}^N \left[ p_n \log \hat{p}_n + (1 - p_n) \log(1 - \hat{p}_n) \right] $, often used for predicting targets interpreted as probabilities.  




{\ttfamily \#include $<$sigmoid\+\_\+cross\+\_\+entropy\+\_\+loss\+\_\+layer.\+hpp$>$}



Inheritance diagram for caffe\+:\+:Sigmoid\+Cross\+Entropy\+Loss\+Layer$<$ Dtype $>$\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=220pt]{classcaffe_1_1_sigmoid_cross_entropy_loss_layer__inherit__graph}
\end{center}
\end{figure}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\Hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a3b4478b3d5c5130de685b240b274c06c}\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a3b4478b3d5c5130de685b240b274c06c}} 
{\bfseries Sigmoid\+Cross\+Entropy\+Loss\+Layer} (const \mbox{\hyperlink{classcaffe_1_1_layer_parameter}{Layer\+Parameter}} \&param)
\item 
virtual void \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_aa1535140dd4eb94557c3afc89076d56d}{Layer\+Set\+Up}} (const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&bottom, const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&top)
\begin{DoxyCompactList}\small\item\em Does layer-\/specific setup\+: your layer should implement this function as well as Reshape. \end{DoxyCompactList}\item 
virtual void \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a305423abeea4bd1652ff7e696aaba808}{Reshape}} (const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&bottom, const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&top)
\begin{DoxyCompactList}\small\item\em Adjust the shapes of top blobs and internal buffers to accommodate the shapes of the bottom blobs. \end{DoxyCompactList}\item 
\mbox{\Hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a38dd36e04f37f4692446b057a48e96e1}\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a38dd36e04f37f4692446b057a48e96e1}} 
virtual const char $\ast$ \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a38dd36e04f37f4692446b057a48e96e1}{type}} () const
\begin{DoxyCompactList}\small\item\em Returns the layer type. \end{DoxyCompactList}\item 
\mbox{\Hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a3b4478b3d5c5130de685b240b274c06c}\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a3b4478b3d5c5130de685b240b274c06c}} 
{\bfseries Sigmoid\+Cross\+Entropy\+Loss\+Layer} (const \mbox{\hyperlink{classcaffe_1_1_layer_parameter}{Layer\+Parameter}} \&param)
\item 
virtual void \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_ac67af0cc1033db08d47a3f56aff5d600}{Layer\+Set\+Up}} (const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&bottom, const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&top)
\begin{DoxyCompactList}\small\item\em Does layer-\/specific setup\+: your layer should implement this function as well as Reshape. \end{DoxyCompactList}\item 
virtual void \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a4199eb0668451022f8da20ebca129eb3}{Reshape}} (const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&bottom, const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&top)
\begin{DoxyCompactList}\small\item\em Adjust the shapes of top blobs and internal buffers to accommodate the shapes of the bottom blobs. \end{DoxyCompactList}\item 
\mbox{\Hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a38dd36e04f37f4692446b057a48e96e1}\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a38dd36e04f37f4692446b057a48e96e1}} 
virtual const char $\ast$ \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a38dd36e04f37f4692446b057a48e96e1}{type}} () const
\begin{DoxyCompactList}\small\item\em Returns the layer type. \end{DoxyCompactList}\end{DoxyCompactItemize}
\subsection*{Protected Member Functions}
\begin{DoxyCompactItemize}
\item 
virtual void \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_aa3e7f285742d862435d5e49d13e05064}{Forward\+\_\+cpu}} (const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&bottom, const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&top)
\begin{DoxyCompactList}\small\item\em Computes the cross-\/entropy (logistic) loss $ E = \frac{-1}{n} \sum\limits_{n=1}^N \left[ p_n \log \hat{p}_n + (1 - p_n) \log(1 - \hat{p}_n) \right] $, often used for predicting targets interpreted as probabilities. \end{DoxyCompactList}\item 
\mbox{\Hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a3a973821a2a73fd8bf4c2e474b2ad5d8}\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a3a973821a2a73fd8bf4c2e474b2ad5d8}} 
virtual void \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a3a973821a2a73fd8bf4c2e474b2ad5d8}{Forward\+\_\+gpu}} (const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&bottom, const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&top)
\begin{DoxyCompactList}\small\item\em Using the G\+PU device, compute the layer output. Fall back to \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_aa3e7f285742d862435d5e49d13e05064}{Forward\+\_\+cpu()}} if unavailable. \end{DoxyCompactList}\item 
virtual void \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a025360b1de1fefbc4684e43603394a22}{Backward\+\_\+cpu}} (const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&top, const vector$<$ bool $>$ \&propagate\+\_\+down, const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&bottom)
\begin{DoxyCompactList}\small\item\em Computes the sigmoid cross-\/entropy loss error gradient w.\+r.\+t. the predictions. \end{DoxyCompactList}\item 
\mbox{\Hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a3e1aa9138092aad788dc72fef27041f2}\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a3e1aa9138092aad788dc72fef27041f2}} 
virtual void \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a3e1aa9138092aad788dc72fef27041f2}{Backward\+\_\+gpu}} (const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&top, const vector$<$ bool $>$ \&propagate\+\_\+down, const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&bottom)
\begin{DoxyCompactList}\small\item\em Using the G\+PU device, compute the gradients for any parameters and for the bottom blobs if propagate\+\_\+down is true. Fall back to \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a025360b1de1fefbc4684e43603394a22}{Backward\+\_\+cpu()}} if unavailable. \end{DoxyCompactList}\item 
virtual Dtype \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_af8ce9b84227c0be01d4a1cc248a7aa52}{get\+\_\+normalizer}} (Loss\+Parameter\+\_\+\+Normalization\+Mode normalization\+\_\+mode, int valid\+\_\+count)
\item 
virtual void \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_ac4530f8bfb0349021c10cea856d2348b}{Forward\+\_\+cpu}} (const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&bottom, const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&top)
\begin{DoxyCompactList}\small\item\em Computes the cross-\/entropy (logistic) loss $ E = \frac{-1}{n} \sum\limits_{n=1}^N \left[ p_n \log \hat{p}_n + (1 - p_n) \log(1 - \hat{p}_n) \right] $, often used for predicting targets interpreted as probabilities. \end{DoxyCompactList}\item 
\mbox{\Hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a3a973821a2a73fd8bf4c2e474b2ad5d8}\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a3a973821a2a73fd8bf4c2e474b2ad5d8}} 
virtual void \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a3a973821a2a73fd8bf4c2e474b2ad5d8}{Forward\+\_\+gpu}} (const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&bottom, const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&top)
\begin{DoxyCompactList}\small\item\em Using the G\+PU device, compute the layer output. Fall back to \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_aa3e7f285742d862435d5e49d13e05064}{Forward\+\_\+cpu()}} if unavailable. \end{DoxyCompactList}\item 
virtual void \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_adf71e6f3bc5952c721c3bdd965612aed}{Backward\+\_\+cpu}} (const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&top, const vector$<$ bool $>$ \&propagate\+\_\+down, const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&bottom)
\begin{DoxyCompactList}\small\item\em Computes the sigmoid cross-\/entropy loss error gradient w.\+r.\+t. the predictions. \end{DoxyCompactList}\item 
\mbox{\Hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a3e1aa9138092aad788dc72fef27041f2}\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a3e1aa9138092aad788dc72fef27041f2}} 
virtual void \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a3e1aa9138092aad788dc72fef27041f2}{Backward\+\_\+gpu}} (const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&top, const vector$<$ bool $>$ \&propagate\+\_\+down, const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&bottom)
\begin{DoxyCompactList}\small\item\em Using the G\+PU device, compute the gradients for any parameters and for the bottom blobs if propagate\+\_\+down is true. Fall back to \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a025360b1de1fefbc4684e43603394a22}{Backward\+\_\+cpu()}} if unavailable. \end{DoxyCompactList}\item 
virtual Dtype \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_accab5e6e98e83dec65af0ecb49deb36b}{get\+\_\+normalizer}} (Loss\+Parameter\+\_\+\+Normalization\+Mode normalization\+\_\+mode, int valid\+\_\+count)
\end{DoxyCompactItemize}
\subsection*{Protected Attributes}
\begin{DoxyCompactItemize}
\item 
\mbox{\Hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a8dc9c17e12483369f39da936f3062f86}\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a8dc9c17e12483369f39da936f3062f86}} 
shared\+\_\+ptr$<$ \mbox{\hyperlink{classcaffe_1_1_sigmoid_layer}{Sigmoid\+Layer}}$<$ Dtype $>$ $>$ \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a8dc9c17e12483369f39da936f3062f86}{sigmoid\+\_\+layer\+\_\+}}
\begin{DoxyCompactList}\small\item\em The internal \mbox{\hyperlink{classcaffe_1_1_sigmoid_layer}{Sigmoid\+Layer}} used to map predictions to probabilities. \end{DoxyCompactList}\item 
\mbox{\Hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_afb2125344c21f5d9627077b1ba741972}\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_afb2125344c21f5d9627077b1ba741972}} 
shared\+\_\+ptr$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $>$ \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_afb2125344c21f5d9627077b1ba741972}{sigmoid\+\_\+output\+\_\+}}
\begin{DoxyCompactList}\small\item\em sigmoid\+\_\+output stores the output of the \mbox{\hyperlink{classcaffe_1_1_sigmoid_layer}{Sigmoid\+Layer}}. \end{DoxyCompactList}\item 
\mbox{\Hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_aeca896f5095bdd6e862645e0dba99352}\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_aeca896f5095bdd6e862645e0dba99352}} 
vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$ $>$ \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_aeca896f5095bdd6e862645e0dba99352}{sigmoid\+\_\+bottom\+\_\+vec\+\_\+}}
\begin{DoxyCompactList}\small\item\em bottom vector holder to call the underlying \mbox{\hyperlink{classcaffe_1_1_layer_ab57d272dabe8c709d2a785eebe72ca57}{Sigmoid\+Layer\+::\+Forward}} \end{DoxyCompactList}\item 
\mbox{\Hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_af484b314eb460e0a732c9dc7fccec8ae}\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_af484b314eb460e0a732c9dc7fccec8ae}} 
vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$ $>$ \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_af484b314eb460e0a732c9dc7fccec8ae}{sigmoid\+\_\+top\+\_\+vec\+\_\+}}
\begin{DoxyCompactList}\small\item\em top vector holder to call the underlying \mbox{\hyperlink{classcaffe_1_1_layer_ab57d272dabe8c709d2a785eebe72ca57}{Sigmoid\+Layer\+::\+Forward}} \end{DoxyCompactList}\item 
\mbox{\Hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a7e2ebf45542532439096caaec42a2a85}\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a7e2ebf45542532439096caaec42a2a85}} 
bool \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a7e2ebf45542532439096caaec42a2a85}{has\+\_\+ignore\+\_\+label\+\_\+}}
\begin{DoxyCompactList}\small\item\em Whether to ignore instances with a certain label. \end{DoxyCompactList}\item 
\mbox{\Hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_ab99d98ce823df6c90cd4c3cf8b0a793f}\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_ab99d98ce823df6c90cd4c3cf8b0a793f}} 
int \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_ab99d98ce823df6c90cd4c3cf8b0a793f}{ignore\+\_\+label\+\_\+}}
\begin{DoxyCompactList}\small\item\em The label indicating that an instance should be ignored. \end{DoxyCompactList}\item 
\mbox{\Hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_ad4e3c7105f896bd7792e53ef3a0a2dd8}\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_ad4e3c7105f896bd7792e53ef3a0a2dd8}} 
Loss\+Parameter\+\_\+\+Normalization\+Mode \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_ad4e3c7105f896bd7792e53ef3a0a2dd8}{normalization\+\_\+}}
\begin{DoxyCompactList}\small\item\em How to normalize the loss. \end{DoxyCompactList}\item 
\mbox{\Hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a10169a710108bac27a6548cc66934cb5}\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a10169a710108bac27a6548cc66934cb5}} 
Dtype {\bfseries normalizer\+\_\+}
\item 
\mbox{\Hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a3be5cd5a4489be1439bd796c55c033cf}\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a3be5cd5a4489be1439bd796c55c033cf}} 
int {\bfseries outer\+\_\+num\+\_\+}
\item 
\mbox{\Hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_ad1e5fb747a0272b5fe90d6264615334b}\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_ad1e5fb747a0272b5fe90d6264615334b}} 
int {\bfseries inner\+\_\+num\+\_\+}
\end{DoxyCompactItemize}


\subsection{Detailed Description}
\subsubsection*{template$<$typename Dtype$>$\newline
class caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer$<$ Dtype $>$}

Computes the cross-\/entropy (logistic) loss $ E = \frac{-1}{n} \sum\limits_{n=1}^N \left[ p_n \log \hat{p}_n + (1 - p_n) \log(1 - \hat{p}_n) \right] $, often used for predicting targets interpreted as probabilities. 

This layer is implemented rather than separate \mbox{\hyperlink{classcaffe_1_1_sigmoid_layer}{Sigmoid\+Layer}} + Cross\+Entropy\+Layer as its gradient computation is more numerically stable. At test time, this layer can be replaced simply by a \mbox{\hyperlink{classcaffe_1_1_sigmoid_layer}{Sigmoid\+Layer}}.


\begin{DoxyParams}{Parameters}
{\em bottom} & input \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 2)
\begin{DoxyEnumerate}
\item $ (N \times C \times H \times W) $ the scores $ x \in [-\infty, +\infty]$, which this layer maps to probability predictions $ \hat{p}_n = \sigma(x_n) \in [0, 1] $ using the sigmoid function $ \sigma(.) $ (see \mbox{\hyperlink{classcaffe_1_1_sigmoid_layer}{Sigmoid\+Layer}}).
\item $ (N \times C \times H \times W) $ the targets $ y \in [0, 1] $ 
\end{DoxyEnumerate}\\
\hline
{\em top} & output \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 1)
\begin{DoxyEnumerate}
\item $ (1 \times 1 \times 1 \times 1) $ the computed cross-\/entropy loss\+: $ E = \frac{-1}{n} \sum\limits_{n=1}^N \left[ p_n \log \hat{p}_n + (1 - p_n) \log(1 - \hat{p}_n) \right] $ 
\end{DoxyEnumerate}\\
\hline
\end{DoxyParams}


\subsection{Member Function Documentation}
\mbox{\Hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a025360b1de1fefbc4684e43603394a22}\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a025360b1de1fefbc4684e43603394a22}} 
\index{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer@{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}!Backward\+\_\+cpu@{Backward\+\_\+cpu}}
\index{Backward\+\_\+cpu@{Backward\+\_\+cpu}!caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer@{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Backward\+\_\+cpu()}{Backward\_cpu()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily template$<$typename Dtype $>$ \\
void \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer}{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}}$<$ Dtype $>$\+::Backward\+\_\+cpu (\begin{DoxyParamCaption}\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{top,  }\item[{const vector$<$ bool $>$ \&}]{propagate\+\_\+down,  }\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{bottom }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}, {\ttfamily [virtual]}}



Computes the sigmoid cross-\/entropy loss error gradient w.\+r.\+t. the predictions. 

Gradients cannot be computed with respect to the target inputs (bottom\mbox{[}1\mbox{]}), so this method ignores bottom\mbox{[}1\mbox{]} and requires !propagate\+\_\+down\mbox{[}1\mbox{]}, crashing if propagate\+\_\+down\mbox{[}1\mbox{]} is set.


\begin{DoxyParams}{Parameters}
{\em top} & output \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 1), providing the error gradient with respect to the outputs
\begin{DoxyEnumerate}
\item $ (1 \times 1 \times 1 \times 1) $ This \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}\textquotesingle{}s diff will simply contain the loss\+\_\+weight$\ast$ $ \lambda $, as $ \lambda $ is the coefficient of this layer\textquotesingle{}s output $\ell_i$ in the overall \mbox{\hyperlink{classcaffe_1_1_net}{Net}} loss $ E = \lambda_i \ell_i + \mbox{other loss terms}$; hence $ \frac{\partial E}{\partial \ell_i} = \lambda_i $. ($\ast$\+Assuming that this top \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} is not used as a bottom (input) by any other layer of the \mbox{\hyperlink{classcaffe_1_1_net}{Net}}.) 
\end{DoxyEnumerate}\\
\hline
{\em propagate\+\_\+down} & see \mbox{\hyperlink{classcaffe_1_1_layer_a183d343f5183a4762307f2c5e6ed1e12}{Layer\+::\+Backward}}. propagate\+\_\+down\mbox{[}1\mbox{]} must be false as gradient computation with respect to the targets is not implemented. \\
\hline
{\em bottom} & input \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 2)
\begin{DoxyEnumerate}
\item $ (N \times C \times H \times W) $ the predictions $x$; Backward computes diff $ \frac{\partial E}{\partial x} = \frac{1}{n} \sum\limits_{n=1}^N (\hat{p}_n - p_n) $
\item $ (N \times 1 \times 1 \times 1) $ the labels -- ignored as we can\textquotesingle{}t compute their error gradients 
\end{DoxyEnumerate}\\
\hline
\end{DoxyParams}


Implements \mbox{\hyperlink{classcaffe_1_1_layer_a75c9b2a321dc713e0eaef530d02dc37f}{caffe\+::\+Layer$<$ Dtype $>$}}.

\mbox{\Hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_adf71e6f3bc5952c721c3bdd965612aed}\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_adf71e6f3bc5952c721c3bdd965612aed}} 
\index{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer@{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}!Backward\+\_\+cpu@{Backward\+\_\+cpu}}
\index{Backward\+\_\+cpu@{Backward\+\_\+cpu}!caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer@{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Backward\+\_\+cpu()}{Backward\_cpu()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily template$<$typename Dtype$>$ \\
virtual void \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer}{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}}$<$ Dtype $>$\+::Backward\+\_\+cpu (\begin{DoxyParamCaption}\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{top,  }\item[{const vector$<$ bool $>$ \&}]{propagate\+\_\+down,  }\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{bottom }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}, {\ttfamily [virtual]}}



Computes the sigmoid cross-\/entropy loss error gradient w.\+r.\+t. the predictions. 

Gradients cannot be computed with respect to the target inputs (bottom\mbox{[}1\mbox{]}), so this method ignores bottom\mbox{[}1\mbox{]} and requires !propagate\+\_\+down\mbox{[}1\mbox{]}, crashing if propagate\+\_\+down\mbox{[}1\mbox{]} is set.


\begin{DoxyParams}{Parameters}
{\em top} & output \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 1), providing the error gradient with respect to the outputs
\begin{DoxyEnumerate}
\item $ (1 \times 1 \times 1 \times 1) $ This \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}\textquotesingle{}s diff will simply contain the loss\+\_\+weight$\ast$ $ \lambda $, as $ \lambda $ is the coefficient of this layer\textquotesingle{}s output $\ell_i$ in the overall \mbox{\hyperlink{classcaffe_1_1_net}{Net}} loss $ E = \lambda_i \ell_i + \mbox{other loss terms}$; hence $ \frac{\partial E}{\partial \ell_i} = \lambda_i $. ($\ast$\+Assuming that this top \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} is not used as a bottom (input) by any other layer of the \mbox{\hyperlink{classcaffe_1_1_net}{Net}}.) 
\end{DoxyEnumerate}\\
\hline
{\em propagate\+\_\+down} & see \mbox{\hyperlink{classcaffe_1_1_layer_a183d343f5183a4762307f2c5e6ed1e12}{Layer\+::\+Backward}}. propagate\+\_\+down\mbox{[}1\mbox{]} must be false as gradient computation with respect to the targets is not implemented. \\
\hline
{\em bottom} & input \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 2)
\begin{DoxyEnumerate}
\item $ (N \times C \times H \times W) $ the predictions $x$; Backward computes diff $ \frac{\partial E}{\partial x} = \frac{1}{n} \sum\limits_{n=1}^N (\hat{p}_n - p_n) $
\item $ (N \times 1 \times 1 \times 1) $ the labels -- ignored as we can\textquotesingle{}t compute their error gradients 
\end{DoxyEnumerate}\\
\hline
\end{DoxyParams}


Implements \mbox{\hyperlink{classcaffe_1_1_layer_a75c9b2a321dc713e0eaef530d02dc37f}{caffe\+::\+Layer$<$ Dtype $>$}}.

\mbox{\Hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_ac4530f8bfb0349021c10cea856d2348b}\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_ac4530f8bfb0349021c10cea856d2348b}} 
\index{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer@{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}!Forward\+\_\+cpu@{Forward\+\_\+cpu}}
\index{Forward\+\_\+cpu@{Forward\+\_\+cpu}!caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer@{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Forward\+\_\+cpu()}{Forward\_cpu()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily template$<$typename Dtype$>$ \\
virtual void \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer}{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}}$<$ Dtype $>$\+::Forward\+\_\+cpu (\begin{DoxyParamCaption}\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{bottom,  }\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{top }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}, {\ttfamily [virtual]}}



Computes the cross-\/entropy (logistic) loss $ E = \frac{-1}{n} \sum\limits_{n=1}^N \left[ p_n \log \hat{p}_n + (1 - p_n) \log(1 - \hat{p}_n) \right] $, often used for predicting targets interpreted as probabilities. 

This layer is implemented rather than separate \mbox{\hyperlink{classcaffe_1_1_sigmoid_layer}{Sigmoid\+Layer}} + Cross\+Entropy\+Layer as its gradient computation is more numerically stable. At test time, this layer can be replaced simply by a \mbox{\hyperlink{classcaffe_1_1_sigmoid_layer}{Sigmoid\+Layer}}.


\begin{DoxyParams}{Parameters}
{\em bottom} & input \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 2)
\begin{DoxyEnumerate}
\item $ (N \times C \times H \times W) $ the scores $ x \in [-\infty, +\infty]$, which this layer maps to probability predictions $ \hat{p}_n = \sigma(x_n) \in [0, 1] $ using the sigmoid function $ \sigma(.) $ (see \mbox{\hyperlink{classcaffe_1_1_sigmoid_layer}{Sigmoid\+Layer}}).
\item $ (N \times C \times H \times W) $ the targets $ y \in [0, 1] $ 
\end{DoxyEnumerate}\\
\hline
{\em top} & output \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 1)
\begin{DoxyEnumerate}
\item $ (1 \times 1 \times 1 \times 1) $ the computed cross-\/entropy loss\+: $ E = \frac{-1}{n} \sum\limits_{n=1}^N \left[ p_n \log \hat{p}_n + (1 - p_n) \log(1 - \hat{p}_n) \right] $ 
\end{DoxyEnumerate}\\
\hline
\end{DoxyParams}


Implements \mbox{\hyperlink{classcaffe_1_1_layer_a576ac6a60b1e99fe383831f52a6cea77}{caffe\+::\+Layer$<$ Dtype $>$}}.

\mbox{\Hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_aa3e7f285742d862435d5e49d13e05064}\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_aa3e7f285742d862435d5e49d13e05064}} 
\index{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer@{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}!Forward\+\_\+cpu@{Forward\+\_\+cpu}}
\index{Forward\+\_\+cpu@{Forward\+\_\+cpu}!caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer@{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Forward\+\_\+cpu()}{Forward\_cpu()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily template$<$typename Dtype $>$ \\
void \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer}{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}}$<$ Dtype $>$\+::Forward\+\_\+cpu (\begin{DoxyParamCaption}\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{bottom,  }\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{top }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}, {\ttfamily [virtual]}}



Computes the cross-\/entropy (logistic) loss $ E = \frac{-1}{n} \sum\limits_{n=1}^N \left[ p_n \log \hat{p}_n + (1 - p_n) \log(1 - \hat{p}_n) \right] $, often used for predicting targets interpreted as probabilities. 

This layer is implemented rather than separate \mbox{\hyperlink{classcaffe_1_1_sigmoid_layer}{Sigmoid\+Layer}} + Cross\+Entropy\+Layer as its gradient computation is more numerically stable. At test time, this layer can be replaced simply by a \mbox{\hyperlink{classcaffe_1_1_sigmoid_layer}{Sigmoid\+Layer}}.


\begin{DoxyParams}{Parameters}
{\em bottom} & input \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 2)
\begin{DoxyEnumerate}
\item $ (N \times C \times H \times W) $ the scores $ x \in [-\infty, +\infty]$, which this layer maps to probability predictions $ \hat{p}_n = \sigma(x_n) \in [0, 1] $ using the sigmoid function $ \sigma(.) $ (see \mbox{\hyperlink{classcaffe_1_1_sigmoid_layer}{Sigmoid\+Layer}}).
\item $ (N \times C \times H \times W) $ the targets $ y \in [0, 1] $ 
\end{DoxyEnumerate}\\
\hline
{\em top} & output \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 1)
\begin{DoxyEnumerate}
\item $ (1 \times 1 \times 1 \times 1) $ the computed cross-\/entropy loss\+: $ E = \frac{-1}{n} \sum\limits_{n=1}^N \left[ p_n \log \hat{p}_n + (1 - p_n) \log(1 - \hat{p}_n) \right] $ 
\end{DoxyEnumerate}\\
\hline
\end{DoxyParams}


Implements \mbox{\hyperlink{classcaffe_1_1_layer_a576ac6a60b1e99fe383831f52a6cea77}{caffe\+::\+Layer$<$ Dtype $>$}}.

\mbox{\Hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_af8ce9b84227c0be01d4a1cc248a7aa52}\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_af8ce9b84227c0be01d4a1cc248a7aa52}} 
\index{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer@{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}!get\+\_\+normalizer@{get\+\_\+normalizer}}
\index{get\+\_\+normalizer@{get\+\_\+normalizer}!caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer@{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}}
\subsubsection{\texorpdfstring{get\+\_\+normalizer()}{get\_normalizer()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily template$<$typename Dtype $>$ \\
Dtype \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer}{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}}$<$ Dtype $>$\+::get\+\_\+normalizer (\begin{DoxyParamCaption}\item[{Loss\+Parameter\+\_\+\+Normalization\+Mode}]{normalization\+\_\+mode,  }\item[{int}]{valid\+\_\+count }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}, {\ttfamily [virtual]}}

Read the normalization mode parameter and compute the normalizer based on the blob size. If normalization\+\_\+mode is V\+A\+L\+ID, the count of valid outputs will be read from valid\+\_\+count, unless it is -\/1 in which case all outputs are assumed to be valid. \mbox{\Hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_accab5e6e98e83dec65af0ecb49deb36b}\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_accab5e6e98e83dec65af0ecb49deb36b}} 
\index{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer@{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}!get\+\_\+normalizer@{get\+\_\+normalizer}}
\index{get\+\_\+normalizer@{get\+\_\+normalizer}!caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer@{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}}
\subsubsection{\texorpdfstring{get\+\_\+normalizer()}{get\_normalizer()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily template$<$typename Dtype$>$ \\
virtual Dtype \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer}{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}}$<$ Dtype $>$\+::get\+\_\+normalizer (\begin{DoxyParamCaption}\item[{Loss\+Parameter\+\_\+\+Normalization\+Mode}]{normalization\+\_\+mode,  }\item[{int}]{valid\+\_\+count }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}, {\ttfamily [virtual]}}

Read the normalization mode parameter and compute the normalizer based on the blob size. If normalization\+\_\+mode is V\+A\+L\+ID, the count of valid outputs will be read from valid\+\_\+count, unless it is -\/1 in which case all outputs are assumed to be valid. \mbox{\Hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_ac67af0cc1033db08d47a3f56aff5d600}\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_ac67af0cc1033db08d47a3f56aff5d600}} 
\index{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer@{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}!Layer\+Set\+Up@{Layer\+Set\+Up}}
\index{Layer\+Set\+Up@{Layer\+Set\+Up}!caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer@{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Layer\+Set\+Up()}{LayerSetUp()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily template$<$typename Dtype$>$ \\
virtual void \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer}{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}}$<$ Dtype $>$\+::Layer\+Set\+Up (\begin{DoxyParamCaption}\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{bottom,  }\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{top }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [virtual]}}



Does layer-\/specific setup\+: your layer should implement this function as well as Reshape. 


\begin{DoxyParams}{Parameters}
{\em bottom} & the preshaped input blobs, whose data fields store the input data for this layer \\
\hline
{\em top} & the allocated but unshaped output blobs\\
\hline
\end{DoxyParams}
This method should do one-\/time layer specific setup. This includes reading and processing relevent parameters from the {\ttfamily layer\+\_\+param\+\_\+}. Setting up the shapes of top blobs and internal buffers should be done in {\ttfamily Reshape}, which will be called before the forward pass to adjust the top blob sizes. 

Reimplemented from \mbox{\hyperlink{classcaffe_1_1_loss_layer_aa6fc7c2e90be66f1c1f0683637c949da}{caffe\+::\+Loss\+Layer$<$ Dtype $>$}}.

\mbox{\Hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_aa1535140dd4eb94557c3afc89076d56d}\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_aa1535140dd4eb94557c3afc89076d56d}} 
\index{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer@{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}!Layer\+Set\+Up@{Layer\+Set\+Up}}
\index{Layer\+Set\+Up@{Layer\+Set\+Up}!caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer@{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Layer\+Set\+Up()}{LayerSetUp()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily template$<$typename Dtype $>$ \\
void \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer}{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}}$<$ Dtype $>$\+::Layer\+Set\+Up (\begin{DoxyParamCaption}\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{bottom,  }\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{top }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [virtual]}}



Does layer-\/specific setup\+: your layer should implement this function as well as Reshape. 


\begin{DoxyParams}{Parameters}
{\em bottom} & the preshaped input blobs, whose data fields store the input data for this layer \\
\hline
{\em top} & the allocated but unshaped output blobs\\
\hline
\end{DoxyParams}
This method should do one-\/time layer specific setup. This includes reading and processing relevent parameters from the {\ttfamily layer\+\_\+param\+\_\+}. Setting up the shapes of top blobs and internal buffers should be done in {\ttfamily Reshape}, which will be called before the forward pass to adjust the top blob sizes. 

Reimplemented from \mbox{\hyperlink{classcaffe_1_1_loss_layer_aa6fc7c2e90be66f1c1f0683637c949da}{caffe\+::\+Loss\+Layer$<$ Dtype $>$}}.

Here is the call graph for this function\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=350pt]{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_aa1535140dd4eb94557c3afc89076d56d_cgraph}
\end{center}
\end{figure}
\mbox{\Hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a305423abeea4bd1652ff7e696aaba808}\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a305423abeea4bd1652ff7e696aaba808}} 
\index{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer@{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}!Reshape@{Reshape}}
\index{Reshape@{Reshape}!caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer@{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Reshape()}{Reshape()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily template$<$typename Dtype $>$ \\
void \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer}{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}}$<$ Dtype $>$\+::Reshape (\begin{DoxyParamCaption}\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{bottom,  }\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{top }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [virtual]}}



Adjust the shapes of top blobs and internal buffers to accommodate the shapes of the bottom blobs. 


\begin{DoxyParams}{Parameters}
{\em bottom} & the input blobs, with the requested input shapes \\
\hline
{\em top} & the top blobs, which should be reshaped as needed\\
\hline
\end{DoxyParams}
This method should reshape top blobs as needed according to the shapes of the bottom (input) blobs, as well as reshaping any internal buffers and making any other necessary adjustments so that the layer can accommodate the bottom blobs. 

Reimplemented from \mbox{\hyperlink{classcaffe_1_1_loss_layer_abf00412194f5413ea9468ee44b0d986f}{caffe\+::\+Loss\+Layer$<$ Dtype $>$}}.

Here is the call graph for this function\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=350pt]{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a305423abeea4bd1652ff7e696aaba808_cgraph}
\end{center}
\end{figure}
\mbox{\Hypertarget{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a4199eb0668451022f8da20ebca129eb3}\label{classcaffe_1_1_sigmoid_cross_entropy_loss_layer_a4199eb0668451022f8da20ebca129eb3}} 
\index{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer@{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}!Reshape@{Reshape}}
\index{Reshape@{Reshape}!caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer@{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Reshape()}{Reshape()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily template$<$typename Dtype$>$ \\
virtual void \mbox{\hyperlink{classcaffe_1_1_sigmoid_cross_entropy_loss_layer}{caffe\+::\+Sigmoid\+Cross\+Entropy\+Loss\+Layer}}$<$ Dtype $>$\+::Reshape (\begin{DoxyParamCaption}\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{bottom,  }\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{top }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [virtual]}}



Adjust the shapes of top blobs and internal buffers to accommodate the shapes of the bottom blobs. 


\begin{DoxyParams}{Parameters}
{\em bottom} & the input blobs, with the requested input shapes \\
\hline
{\em top} & the top blobs, which should be reshaped as needed\\
\hline
\end{DoxyParams}
This method should reshape top blobs as needed according to the shapes of the bottom (input) blobs, as well as reshaping any internal buffers and making any other necessary adjustments so that the layer can accommodate the bottom blobs. 

Reimplemented from \mbox{\hyperlink{classcaffe_1_1_loss_layer_abf00412194f5413ea9468ee44b0d986f}{caffe\+::\+Loss\+Layer$<$ Dtype $>$}}.



The documentation for this class was generated from the following files\+:\begin{DoxyCompactItemize}
\item 
build/install/include/caffe/layers/sigmoid\+\_\+cross\+\_\+entropy\+\_\+loss\+\_\+layer.\+hpp\item 
src/caffe/layers/sigmoid\+\_\+cross\+\_\+entropy\+\_\+loss\+\_\+layer.\+cpp\end{DoxyCompactItemize}
