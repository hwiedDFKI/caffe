\hypertarget{classcaffe_1_1_infogain_loss_layer}{}\section{caffe\+:\+:Infogain\+Loss\+Layer$<$ Dtype $>$ Class Template Reference}
\label{classcaffe_1_1_infogain_loss_layer}\index{caffe\+::\+Infogain\+Loss\+Layer$<$ Dtype $>$@{caffe\+::\+Infogain\+Loss\+Layer$<$ Dtype $>$}}


A generalization of \mbox{\hyperlink{classcaffe_1_1_softmax_with_loss_layer}{Softmax\+With\+Loss\+Layer}} that takes an \char`\"{}information gain\char`\"{} (infogain) matrix specifying the \char`\"{}value\char`\"{} of all label pairs.  




{\ttfamily \#include $<$infogain\+\_\+loss\+\_\+layer.\+hpp$>$}



Inheritance diagram for caffe\+:\+:Infogain\+Loss\+Layer$<$ Dtype $>$\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=214pt]{classcaffe_1_1_infogain_loss_layer__inherit__graph}
\end{center}
\end{figure}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_ac2269ba8dc7d18fa8fab90ea9f295784}\label{classcaffe_1_1_infogain_loss_layer_ac2269ba8dc7d18fa8fab90ea9f295784}} 
{\bfseries Infogain\+Loss\+Layer} (const \mbox{\hyperlink{classcaffe_1_1_layer_parameter}{Layer\+Parameter}} \&param)
\item 
virtual void \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_a772be3f4074c72b3cf9214bda3422402}{Layer\+Set\+Up}} (const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&bottom, const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&top)
\begin{DoxyCompactList}\small\item\em Does layer-\/specific setup\+: your layer should implement this function as well as Reshape. \end{DoxyCompactList}\item 
virtual void \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_a83ed478450bc7f629499fed37f654c5c}{Reshape}} (const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&bottom, const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&top)
\begin{DoxyCompactList}\small\item\em Adjust the shapes of top blobs and internal buffers to accommodate the shapes of the bottom blobs. \end{DoxyCompactList}\item 
virtual int \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_aa03732f381764180748479c83b289869}{Exact\+Num\+Bottom\+Blobs}} () const
\begin{DoxyCompactList}\small\item\em Returns the exact number of bottom blobs required by the layer, or -\/1 if no exact number is required. \end{DoxyCompactList}\item 
virtual int \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_ad8a1ef702a695e379e5d0450369b4a0c}{Min\+Bottom\+Blobs}} () const
\begin{DoxyCompactList}\small\item\em Returns the minimum number of bottom blobs required by the layer, or -\/1 if no minimum number is required. \end{DoxyCompactList}\item 
virtual int \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_a9b2372959a16da1e80ae7a98b7689a4c}{Max\+Bottom\+Blobs}} () const
\begin{DoxyCompactList}\small\item\em Returns the maximum number of bottom blobs required by the layer, or -\/1 if no maximum number is required. \end{DoxyCompactList}\item 
virtual int \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_aaf55e75f2296586b1fee0175e2d72fbb}{Exact\+Num\+Top\+Blobs}} () const
\begin{DoxyCompactList}\small\item\em Returns the exact number of top blobs required by the layer, or -\/1 if no exact number is required. \end{DoxyCompactList}\item 
virtual int \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_a15c4916e5de27151eb745491d8d14d41}{Min\+Top\+Blobs}} () const
\begin{DoxyCompactList}\small\item\em Returns the minimum number of top blobs required by the layer, or -\/1 if no minimum number is required. \end{DoxyCompactList}\item 
virtual int \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_a93019601c6256354fd4758da91d9311f}{Max\+Top\+Blobs}} () const
\begin{DoxyCompactList}\small\item\em Returns the maximum number of top blobs required by the layer, or -\/1 if no maximum number is required. \end{DoxyCompactList}\item 
\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_aada26ffd60207582fe2af602004e271b}\label{classcaffe_1_1_infogain_loss_layer_aada26ffd60207582fe2af602004e271b}} 
virtual const char $\ast$ \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_aada26ffd60207582fe2af602004e271b}{type}} () const
\begin{DoxyCompactList}\small\item\em Returns the layer type. \end{DoxyCompactList}\item 
\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_ac2269ba8dc7d18fa8fab90ea9f295784}\label{classcaffe_1_1_infogain_loss_layer_ac2269ba8dc7d18fa8fab90ea9f295784}} 
{\bfseries Infogain\+Loss\+Layer} (const \mbox{\hyperlink{classcaffe_1_1_layer_parameter}{Layer\+Parameter}} \&param)
\item 
virtual void \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_ae59c01de80f22c87c1dd2ef87c6e6a2f}{Layer\+Set\+Up}} (const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&bottom, const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&top)
\begin{DoxyCompactList}\small\item\em Does layer-\/specific setup\+: your layer should implement this function as well as Reshape. \end{DoxyCompactList}\item 
virtual void \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_aa2903026b3886816270deb038a463759}{Reshape}} (const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&bottom, const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&top)
\begin{DoxyCompactList}\small\item\em Adjust the shapes of top blobs and internal buffers to accommodate the shapes of the bottom blobs. \end{DoxyCompactList}\item 
virtual int \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_aa03732f381764180748479c83b289869}{Exact\+Num\+Bottom\+Blobs}} () const
\begin{DoxyCompactList}\small\item\em Returns the exact number of bottom blobs required by the layer, or -\/1 if no exact number is required. \end{DoxyCompactList}\item 
virtual int \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_ad8a1ef702a695e379e5d0450369b4a0c}{Min\+Bottom\+Blobs}} () const
\begin{DoxyCompactList}\small\item\em Returns the minimum number of bottom blobs required by the layer, or -\/1 if no minimum number is required. \end{DoxyCompactList}\item 
virtual int \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_a9b2372959a16da1e80ae7a98b7689a4c}{Max\+Bottom\+Blobs}} () const
\begin{DoxyCompactList}\small\item\em Returns the maximum number of bottom blobs required by the layer, or -\/1 if no maximum number is required. \end{DoxyCompactList}\item 
virtual int \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_aaf55e75f2296586b1fee0175e2d72fbb}{Exact\+Num\+Top\+Blobs}} () const
\begin{DoxyCompactList}\small\item\em Returns the exact number of top blobs required by the layer, or -\/1 if no exact number is required. \end{DoxyCompactList}\item 
virtual int \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_a15c4916e5de27151eb745491d8d14d41}{Min\+Top\+Blobs}} () const
\begin{DoxyCompactList}\small\item\em Returns the minimum number of top blobs required by the layer, or -\/1 if no minimum number is required. \end{DoxyCompactList}\item 
virtual int \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_a93019601c6256354fd4758da91d9311f}{Max\+Top\+Blobs}} () const
\begin{DoxyCompactList}\small\item\em Returns the maximum number of top blobs required by the layer, or -\/1 if no maximum number is required. \end{DoxyCompactList}\item 
\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_aada26ffd60207582fe2af602004e271b}\label{classcaffe_1_1_infogain_loss_layer_aada26ffd60207582fe2af602004e271b}} 
virtual const char $\ast$ \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_aada26ffd60207582fe2af602004e271b}{type}} () const
\begin{DoxyCompactList}\small\item\em Returns the layer type. \end{DoxyCompactList}\end{DoxyCompactItemize}
\subsection*{Protected Member Functions}
\begin{DoxyCompactItemize}
\item 
virtual void \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_a134b51c126eb4b62fac804965f8d8327}{Forward\+\_\+cpu}} (const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&bottom, const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&top)
\begin{DoxyCompactList}\small\item\em A generalization of \mbox{\hyperlink{classcaffe_1_1_softmax_with_loss_layer}{Softmax\+With\+Loss\+Layer}} that takes an \char`\"{}information gain\char`\"{} (infogain) matrix specifying the \char`\"{}value\char`\"{} of all label pairs. \end{DoxyCompactList}\item 
virtual void \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_a6d8fc17daa7233fb96629b641fbc46ac}{Backward\+\_\+cpu}} (const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&top, const vector$<$ bool $>$ \&propagate\+\_\+down, const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&bottom)
\begin{DoxyCompactList}\small\item\em Computes the infogain loss error gradient w.\+r.\+t. the predictions. \end{DoxyCompactList}\item 
virtual Dtype \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_a0e5e9667b19fb88ece7298e3e83d2fdb}{get\+\_\+normalizer}} (Loss\+Parameter\+\_\+\+Normalization\+Mode normalization\+\_\+mode, int valid\+\_\+count)
\item 
\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_a030296e6af30acd17a3cfe4463456147}\label{classcaffe_1_1_infogain_loss_layer_a030296e6af30acd17a3cfe4463456147}} 
virtual void \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_a030296e6af30acd17a3cfe4463456147}{sum\+\_\+rows\+\_\+of\+\_\+H}} (const \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$H)
\begin{DoxyCompactList}\small\item\em fill sum\+\_\+rows\+\_\+\+H\+\_\+ according to matrix H \end{DoxyCompactList}\item 
virtual void \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_a12c67529a9dbc6732db60708a4d9a9f6}{Forward\+\_\+cpu}} (const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&bottom, const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&top)
\begin{DoxyCompactList}\small\item\em A generalization of \mbox{\hyperlink{classcaffe_1_1_softmax_with_loss_layer}{Softmax\+With\+Loss\+Layer}} that takes an \char`\"{}information gain\char`\"{} (infogain) matrix specifying the \char`\"{}value\char`\"{} of all label pairs. \end{DoxyCompactList}\item 
virtual void \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_a2d3651cf83b24ee0508adeeed32d2fc2}{Backward\+\_\+cpu}} (const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&top, const vector$<$ bool $>$ \&propagate\+\_\+down, const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&bottom)
\begin{DoxyCompactList}\small\item\em Computes the infogain loss error gradient w.\+r.\+t. the predictions. \end{DoxyCompactList}\item 
virtual Dtype \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_a9c223a4b6dc5a48fb56bf653111abea1}{get\+\_\+normalizer}} (Loss\+Parameter\+\_\+\+Normalization\+Mode normalization\+\_\+mode, int valid\+\_\+count)
\item 
\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_a30c3135fc7c5d44b159e59861cd4b232}\label{classcaffe_1_1_infogain_loss_layer_a30c3135fc7c5d44b159e59861cd4b232}} 
virtual void \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_a30c3135fc7c5d44b159e59861cd4b232}{sum\+\_\+rows\+\_\+of\+\_\+H}} (const \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$H)
\begin{DoxyCompactList}\small\item\em fill sum\+\_\+rows\+\_\+\+H\+\_\+ according to matrix H \end{DoxyCompactList}\end{DoxyCompactItemize}
\subsection*{Protected Attributes}
\begin{DoxyCompactItemize}
\item 
\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_a4d15d74b6f3e109ad1fef2fdc8b2746d}\label{classcaffe_1_1_infogain_loss_layer_a4d15d74b6f3e109ad1fef2fdc8b2746d}} 
shared\+\_\+ptr$<$ \mbox{\hyperlink{classcaffe_1_1_layer}{Layer}}$<$ Dtype $>$ $>$ \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_a4d15d74b6f3e109ad1fef2fdc8b2746d}{softmax\+\_\+layer\+\_\+}}
\begin{DoxyCompactList}\small\item\em The internal \mbox{\hyperlink{classcaffe_1_1_softmax_layer}{Softmax\+Layer}} used to map predictions to a distribution. \end{DoxyCompactList}\item 
\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_aca1a18ac9d1bfa669d366d7ded927503}\label{classcaffe_1_1_infogain_loss_layer_aca1a18ac9d1bfa669d366d7ded927503}} 
\mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_aca1a18ac9d1bfa669d366d7ded927503}{prob\+\_\+}}
\begin{DoxyCompactList}\small\item\em prob stores the output probability predictions from the \mbox{\hyperlink{classcaffe_1_1_softmax_layer}{Softmax\+Layer}}. \end{DoxyCompactList}\item 
\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_a7122d05d7ea094afece0bbbf0addec33}\label{classcaffe_1_1_infogain_loss_layer_a7122d05d7ea094afece0bbbf0addec33}} 
vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$ $>$ \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_a7122d05d7ea094afece0bbbf0addec33}{softmax\+\_\+bottom\+\_\+vec\+\_\+}}
\begin{DoxyCompactList}\small\item\em bottom vector holder used in call to the underlying \mbox{\hyperlink{classcaffe_1_1_layer_ab57d272dabe8c709d2a785eebe72ca57}{Softmax\+Layer\+::\+Forward}} \end{DoxyCompactList}\item 
\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_aaad8260753f21cdd54631870128297ad}\label{classcaffe_1_1_infogain_loss_layer_aaad8260753f21cdd54631870128297ad}} 
vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$ $>$ \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_aaad8260753f21cdd54631870128297ad}{softmax\+\_\+top\+\_\+vec\+\_\+}}
\begin{DoxyCompactList}\small\item\em top vector holder used in call to the underlying \mbox{\hyperlink{classcaffe_1_1_layer_ab57d272dabe8c709d2a785eebe72ca57}{Softmax\+Layer\+::\+Forward}} \end{DoxyCompactList}\item 
\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_a7ad59c638cc23fbf53c2014375ae2b88}\label{classcaffe_1_1_infogain_loss_layer_a7ad59c638cc23fbf53c2014375ae2b88}} 
\mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ {\bfseries infogain\+\_\+}
\item 
\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_a27ea069c3bfd42e4b476696314285964}\label{classcaffe_1_1_infogain_loss_layer_a27ea069c3bfd42e4b476696314285964}} 
\mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ {\bfseries sum\+\_\+rows\+\_\+\+H\+\_\+}
\item 
\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_a421720fc0f85daf8b6b7808719b1f9e8}\label{classcaffe_1_1_infogain_loss_layer_a421720fc0f85daf8b6b7808719b1f9e8}} 
bool \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_a421720fc0f85daf8b6b7808719b1f9e8}{has\+\_\+ignore\+\_\+label\+\_\+}}
\begin{DoxyCompactList}\small\item\em Whether to ignore instances with a certain label. \end{DoxyCompactList}\item 
\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_ad3f7c2efdf32f99510186495ba7c5cff}\label{classcaffe_1_1_infogain_loss_layer_ad3f7c2efdf32f99510186495ba7c5cff}} 
int \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_ad3f7c2efdf32f99510186495ba7c5cff}{ignore\+\_\+label\+\_\+}}
\begin{DoxyCompactList}\small\item\em The label indicating that an instance should be ignored. \end{DoxyCompactList}\item 
\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_ab7fe88c996d31d67f5e13fc0ffc803c2}\label{classcaffe_1_1_infogain_loss_layer_ab7fe88c996d31d67f5e13fc0ffc803c2}} 
Loss\+Parameter\+\_\+\+Normalization\+Mode \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer_ab7fe88c996d31d67f5e13fc0ffc803c2}{normalization\+\_\+}}
\begin{DoxyCompactList}\small\item\em How to normalize the output loss. \end{DoxyCompactList}\item 
\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_a0f94b595bd8be01b31994236af8cecbd}\label{classcaffe_1_1_infogain_loss_layer_a0f94b595bd8be01b31994236af8cecbd}} 
int {\bfseries infogain\+\_\+axis\+\_\+}
\item 
\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_aaf878e99cf00ac309ed533adf43bcbc3}\label{classcaffe_1_1_infogain_loss_layer_aaf878e99cf00ac309ed533adf43bcbc3}} 
int {\bfseries outer\+\_\+num\+\_\+}
\item 
\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_a6707b103411c0acc3fddd065e91fe6e0}\label{classcaffe_1_1_infogain_loss_layer_a6707b103411c0acc3fddd065e91fe6e0}} 
int {\bfseries inner\+\_\+num\+\_\+}
\item 
\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_a8b58f39c561263cfa44c3831e6a15355}\label{classcaffe_1_1_infogain_loss_layer_a8b58f39c561263cfa44c3831e6a15355}} 
int {\bfseries num\+\_\+labels\+\_\+}
\end{DoxyCompactItemize}


\subsection{Detailed Description}
\subsubsection*{template$<$typename Dtype$>$\newline
class caffe\+::\+Infogain\+Loss\+Layer$<$ Dtype $>$}

A generalization of \mbox{\hyperlink{classcaffe_1_1_softmax_with_loss_layer}{Softmax\+With\+Loss\+Layer}} that takes an \char`\"{}information gain\char`\"{} (infogain) matrix specifying the \char`\"{}value\char`\"{} of all label pairs. 

Equivalent to the \mbox{\hyperlink{classcaffe_1_1_softmax_with_loss_layer}{Softmax\+With\+Loss\+Layer}} if the infogain matrix is the identity.


\begin{DoxyParams}{Parameters}
{\em bottom} & input \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 2-\/3)
\begin{DoxyEnumerate}
\item $ (N \times C \times H \times W) $ the predictions $ x $, a \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} with values in $ [-\infty, +\infty] $ indicating the predicted score for each of the $ K = CHW $ classes. This layer maps these scores to a probability distribution over classes using the softmax function $ \hat{p}_{nk} = \exp(x_{nk}) / \left[\sum_{k'} \exp(x_{nk'})\right] $ (see \mbox{\hyperlink{classcaffe_1_1_softmax_layer}{Softmax\+Layer}}).
\item $ (N \times 1 \times 1 \times 1) $ the labels $ l $, an integer-\/valued \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} with values $ l_n \in [0, 1, 2, ..., K - 1] $ indicating the correct class label among the $ K $ classes
\item $ (1 \times 1 \times K \times K) $ ({\bfseries optional}) the infogain matrix $ H $. This must be provided as the third bottom blob input if not provided as the infogain\+\_\+mat in the \mbox{\hyperlink{classcaffe_1_1_infogain_loss_parameter}{Infogain\+Loss\+Parameter}}. If $ H = I $, this layer is equivalent to the \mbox{\hyperlink{classcaffe_1_1_softmax_with_loss_layer}{Softmax\+With\+Loss\+Layer}}. 
\end{DoxyEnumerate}\\
\hline
{\em top} & output \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 1)
\begin{DoxyEnumerate}
\item $ (1 \times 1 \times 1 \times 1) $ the computed infogain multinomial logistic loss\+: $ E = \frac{-1}{N} \sum\limits_{n=1}^N H_{l_n} \log(\hat{p}_n) = \frac{-1}{N} \sum\limits_{n=1}^N \sum\limits_{k=1}^{K} H_{l_n,k} \log(\hat{p}_{n,k}) $, where $ H_{l_n} $ denotes row $l_n$ of $H$. 
\end{DoxyEnumerate}\\
\hline
\end{DoxyParams}


\subsection{Member Function Documentation}
\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_a6d8fc17daa7233fb96629b641fbc46ac}\label{classcaffe_1_1_infogain_loss_layer_a6d8fc17daa7233fb96629b641fbc46ac}} 
\index{caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}!Backward\+\_\+cpu@{Backward\+\_\+cpu}}
\index{Backward\+\_\+cpu@{Backward\+\_\+cpu}!caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Backward\+\_\+cpu()}{Backward\_cpu()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily template$<$typename Dtype $>$ \\
void \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer}{caffe\+::\+Infogain\+Loss\+Layer}}$<$ Dtype $>$\+::Backward\+\_\+cpu (\begin{DoxyParamCaption}\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{top,  }\item[{const vector$<$ bool $>$ \&}]{propagate\+\_\+down,  }\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{bottom }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}, {\ttfamily [virtual]}}



Computes the infogain loss error gradient w.\+r.\+t. the predictions. 

Gradients cannot be computed with respect to the label inputs (bottom\mbox{[}1\mbox{]}), so this method ignores bottom\mbox{[}1\mbox{]} and requires !propagate\+\_\+down\mbox{[}1\mbox{]}, crashing if propagate\+\_\+down\mbox{[}1\mbox{]} is set. (The same applies to the infogain matrix, if provided as bottom\mbox{[}2\mbox{]} rather than in the layer\+\_\+param.)


\begin{DoxyParams}{Parameters}
{\em top} & output \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 1), providing the error gradient with respect to the outputs
\begin{DoxyEnumerate}
\item $ (1 \times 1 \times 1 \times 1) $ This \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}\textquotesingle{}s diff will simply contain the loss\+\_\+weight$\ast$ $ \lambda $, as $ \lambda $ is the coefficient of this layer\textquotesingle{}s output $\ell_i$ in the overall \mbox{\hyperlink{classcaffe_1_1_net}{Net}} loss $ E = \lambda_i \ell_i + \mbox{other loss terms}$; hence $ \frac{\partial E}{\partial \ell_i} = \lambda_i $. ($\ast$\+Assuming that this top \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} is not used as a bottom (input) by any other layer of the \mbox{\hyperlink{classcaffe_1_1_net}{Net}}.) 
\end{DoxyEnumerate}\\
\hline
{\em propagate\+\_\+down} & see \mbox{\hyperlink{classcaffe_1_1_layer_a183d343f5183a4762307f2c5e6ed1e12}{Layer\+::\+Backward}}. propagate\+\_\+down\mbox{[}1\mbox{]} must be false as we can\textquotesingle{}t compute gradients with respect to the labels (similarly for propagate\+\_\+down\mbox{[}2\mbox{]} and the infogain matrix, if provided as bottom\mbox{[}2\mbox{]}) \\
\hline
{\em bottom} & input \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 2-\/3)
\begin{DoxyEnumerate}
\item $ (N \times C \times H \times W) $ the predictions $ x $; Backward computes diff $ \frac{\partial E}{\partial x} $
\item $ (N \times 1 \times 1 \times 1) $ the labels -- ignored as we can\textquotesingle{}t compute their error gradients
\item $ (1 \times 1 \times K \times K) $ ({\bfseries optional}) the information gain matrix -- ignored as its error gradient computation is not implemented. 
\end{DoxyEnumerate}\\
\hline
\end{DoxyParams}


Implements \mbox{\hyperlink{classcaffe_1_1_layer_a75c9b2a321dc713e0eaef530d02dc37f}{caffe\+::\+Layer$<$ Dtype $>$}}.

\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_a2d3651cf83b24ee0508adeeed32d2fc2}\label{classcaffe_1_1_infogain_loss_layer_a2d3651cf83b24ee0508adeeed32d2fc2}} 
\index{caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}!Backward\+\_\+cpu@{Backward\+\_\+cpu}}
\index{Backward\+\_\+cpu@{Backward\+\_\+cpu}!caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Backward\+\_\+cpu()}{Backward\_cpu()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily template$<$typename Dtype$>$ \\
virtual void \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer}{caffe\+::\+Infogain\+Loss\+Layer}}$<$ Dtype $>$\+::Backward\+\_\+cpu (\begin{DoxyParamCaption}\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{top,  }\item[{const vector$<$ bool $>$ \&}]{propagate\+\_\+down,  }\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{bottom }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}, {\ttfamily [virtual]}}



Computes the infogain loss error gradient w.\+r.\+t. the predictions. 

Gradients cannot be computed with respect to the label inputs (bottom\mbox{[}1\mbox{]}), so this method ignores bottom\mbox{[}1\mbox{]} and requires !propagate\+\_\+down\mbox{[}1\mbox{]}, crashing if propagate\+\_\+down\mbox{[}1\mbox{]} is set. (The same applies to the infogain matrix, if provided as bottom\mbox{[}2\mbox{]} rather than in the layer\+\_\+param.)


\begin{DoxyParams}{Parameters}
{\em top} & output \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 1), providing the error gradient with respect to the outputs
\begin{DoxyEnumerate}
\item $ (1 \times 1 \times 1 \times 1) $ This \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}\textquotesingle{}s diff will simply contain the loss\+\_\+weight$\ast$ $ \lambda $, as $ \lambda $ is the coefficient of this layer\textquotesingle{}s output $\ell_i$ in the overall \mbox{\hyperlink{classcaffe_1_1_net}{Net}} loss $ E = \lambda_i \ell_i + \mbox{other loss terms}$; hence $ \frac{\partial E}{\partial \ell_i} = \lambda_i $. ($\ast$\+Assuming that this top \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} is not used as a bottom (input) by any other layer of the \mbox{\hyperlink{classcaffe_1_1_net}{Net}}.) 
\end{DoxyEnumerate}\\
\hline
{\em propagate\+\_\+down} & see \mbox{\hyperlink{classcaffe_1_1_layer_a183d343f5183a4762307f2c5e6ed1e12}{Layer\+::\+Backward}}. propagate\+\_\+down\mbox{[}1\mbox{]} must be false as we can\textquotesingle{}t compute gradients with respect to the labels (similarly for propagate\+\_\+down\mbox{[}2\mbox{]} and the infogain matrix, if provided as bottom\mbox{[}2\mbox{]}) \\
\hline
{\em bottom} & input \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 2-\/3)
\begin{DoxyEnumerate}
\item $ (N \times C \times H \times W) $ the predictions $ x $; Backward computes diff $ \frac{\partial E}{\partial x} $
\item $ (N \times 1 \times 1 \times 1) $ the labels -- ignored as we can\textquotesingle{}t compute their error gradients
\item $ (1 \times 1 \times K \times K) $ ({\bfseries optional}) the information gain matrix -- ignored as its error gradient computation is not implemented. 
\end{DoxyEnumerate}\\
\hline
\end{DoxyParams}


Implements \mbox{\hyperlink{classcaffe_1_1_layer_a75c9b2a321dc713e0eaef530d02dc37f}{caffe\+::\+Layer$<$ Dtype $>$}}.

\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_aa03732f381764180748479c83b289869}\label{classcaffe_1_1_infogain_loss_layer_aa03732f381764180748479c83b289869}} 
\index{caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}!Exact\+Num\+Bottom\+Blobs@{Exact\+Num\+Bottom\+Blobs}}
\index{Exact\+Num\+Bottom\+Blobs@{Exact\+Num\+Bottom\+Blobs}!caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Exact\+Num\+Bottom\+Blobs()}{ExactNumBottomBlobs()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily template$<$typename Dtype$>$ \\
virtual int \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer}{caffe\+::\+Infogain\+Loss\+Layer}}$<$ Dtype $>$\+::Exact\+Num\+Bottom\+Blobs (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}, {\ttfamily [virtual]}}



Returns the exact number of bottom blobs required by the layer, or -\/1 if no exact number is required. 

This method should be overridden to return a non-\/negative value if your layer expects some exact number of bottom blobs. 

Reimplemented from \mbox{\hyperlink{classcaffe_1_1_loss_layer_af1620064baefb711e2c767bdc92b6fb1}{caffe\+::\+Loss\+Layer$<$ Dtype $>$}}.

\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_aa03732f381764180748479c83b289869}\label{classcaffe_1_1_infogain_loss_layer_aa03732f381764180748479c83b289869}} 
\index{caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}!Exact\+Num\+Bottom\+Blobs@{Exact\+Num\+Bottom\+Blobs}}
\index{Exact\+Num\+Bottom\+Blobs@{Exact\+Num\+Bottom\+Blobs}!caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Exact\+Num\+Bottom\+Blobs()}{ExactNumBottomBlobs()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily template$<$typename Dtype$>$ \\
virtual int \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer}{caffe\+::\+Infogain\+Loss\+Layer}}$<$ Dtype $>$\+::Exact\+Num\+Bottom\+Blobs (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}, {\ttfamily [virtual]}}



Returns the exact number of bottom blobs required by the layer, or -\/1 if no exact number is required. 

This method should be overridden to return a non-\/negative value if your layer expects some exact number of bottom blobs. 

Reimplemented from \mbox{\hyperlink{classcaffe_1_1_loss_layer_af1620064baefb711e2c767bdc92b6fb1}{caffe\+::\+Loss\+Layer$<$ Dtype $>$}}.

\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_aaf55e75f2296586b1fee0175e2d72fbb}\label{classcaffe_1_1_infogain_loss_layer_aaf55e75f2296586b1fee0175e2d72fbb}} 
\index{caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}!Exact\+Num\+Top\+Blobs@{Exact\+Num\+Top\+Blobs}}
\index{Exact\+Num\+Top\+Blobs@{Exact\+Num\+Top\+Blobs}!caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Exact\+Num\+Top\+Blobs()}{ExactNumTopBlobs()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily template$<$typename Dtype$>$ \\
virtual int \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer}{caffe\+::\+Infogain\+Loss\+Layer}}$<$ Dtype $>$\+::Exact\+Num\+Top\+Blobs (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}, {\ttfamily [virtual]}}



Returns the exact number of top blobs required by the layer, or -\/1 if no exact number is required. 

This method should be overridden to return a non-\/negative value if your layer expects some exact number of top blobs. 

Reimplemented from \mbox{\hyperlink{classcaffe_1_1_loss_layer_aa5d5ab714a14082f5343dc9c49025b23}{caffe\+::\+Loss\+Layer$<$ Dtype $>$}}.

\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_aaf55e75f2296586b1fee0175e2d72fbb}\label{classcaffe_1_1_infogain_loss_layer_aaf55e75f2296586b1fee0175e2d72fbb}} 
\index{caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}!Exact\+Num\+Top\+Blobs@{Exact\+Num\+Top\+Blobs}}
\index{Exact\+Num\+Top\+Blobs@{Exact\+Num\+Top\+Blobs}!caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Exact\+Num\+Top\+Blobs()}{ExactNumTopBlobs()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily template$<$typename Dtype$>$ \\
virtual int \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer}{caffe\+::\+Infogain\+Loss\+Layer}}$<$ Dtype $>$\+::Exact\+Num\+Top\+Blobs (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}, {\ttfamily [virtual]}}



Returns the exact number of top blobs required by the layer, or -\/1 if no exact number is required. 

This method should be overridden to return a non-\/negative value if your layer expects some exact number of top blobs. 

Reimplemented from \mbox{\hyperlink{classcaffe_1_1_loss_layer_aa5d5ab714a14082f5343dc9c49025b23}{caffe\+::\+Loss\+Layer$<$ Dtype $>$}}.

\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_a134b51c126eb4b62fac804965f8d8327}\label{classcaffe_1_1_infogain_loss_layer_a134b51c126eb4b62fac804965f8d8327}} 
\index{caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}!Forward\+\_\+cpu@{Forward\+\_\+cpu}}
\index{Forward\+\_\+cpu@{Forward\+\_\+cpu}!caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Forward\+\_\+cpu()}{Forward\_cpu()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily template$<$typename Dtype $>$ \\
void \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer}{caffe\+::\+Infogain\+Loss\+Layer}}$<$ Dtype $>$\+::Forward\+\_\+cpu (\begin{DoxyParamCaption}\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{bottom,  }\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{top }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}, {\ttfamily [virtual]}}



A generalization of \mbox{\hyperlink{classcaffe_1_1_softmax_with_loss_layer}{Softmax\+With\+Loss\+Layer}} that takes an \char`\"{}information gain\char`\"{} (infogain) matrix specifying the \char`\"{}value\char`\"{} of all label pairs. 

Equivalent to the \mbox{\hyperlink{classcaffe_1_1_softmax_with_loss_layer}{Softmax\+With\+Loss\+Layer}} if the infogain matrix is the identity.


\begin{DoxyParams}{Parameters}
{\em bottom} & input \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 2-\/3)
\begin{DoxyEnumerate}
\item $ (N \times C \times H \times W) $ the predictions $ x $, a \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} with values in $ [-\infty, +\infty] $ indicating the predicted score for each of the $ K = CHW $ classes. This layer maps these scores to a probability distribution over classes using the softmax function $ \hat{p}_{nk} = \exp(x_{nk}) / \left[\sum_{k'} \exp(x_{nk'})\right] $ (see \mbox{\hyperlink{classcaffe_1_1_softmax_layer}{Softmax\+Layer}}).
\item $ (N \times 1 \times 1 \times 1) $ the labels $ l $, an integer-\/valued \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} with values $ l_n \in [0, 1, 2, ..., K - 1] $ indicating the correct class label among the $ K $ classes
\item $ (1 \times 1 \times K \times K) $ ({\bfseries optional}) the infogain matrix $ H $. This must be provided as the third bottom blob input if not provided as the infogain\+\_\+mat in the \mbox{\hyperlink{classcaffe_1_1_infogain_loss_parameter}{Infogain\+Loss\+Parameter}}. If $ H = I $, this layer is equivalent to the \mbox{\hyperlink{classcaffe_1_1_softmax_with_loss_layer}{Softmax\+With\+Loss\+Layer}}. 
\end{DoxyEnumerate}\\
\hline
{\em top} & output \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 1)
\begin{DoxyEnumerate}
\item $ (1 \times 1 \times 1 \times 1) $ the computed infogain multinomial logistic loss\+: $ E = \frac{-1}{N} \sum\limits_{n=1}^N H_{l_n} \log(\hat{p}_n) = \frac{-1}{N} \sum\limits_{n=1}^N \sum\limits_{k=1}^{K} H_{l_n,k} \log(\hat{p}_{n,k}) $, where $ H_{l_n} $ denotes row $l_n$ of $H$. 
\end{DoxyEnumerate}\\
\hline
\end{DoxyParams}


Implements \mbox{\hyperlink{classcaffe_1_1_layer_a576ac6a60b1e99fe383831f52a6cea77}{caffe\+::\+Layer$<$ Dtype $>$}}.

\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_a12c67529a9dbc6732db60708a4d9a9f6}\label{classcaffe_1_1_infogain_loss_layer_a12c67529a9dbc6732db60708a4d9a9f6}} 
\index{caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}!Forward\+\_\+cpu@{Forward\+\_\+cpu}}
\index{Forward\+\_\+cpu@{Forward\+\_\+cpu}!caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Forward\+\_\+cpu()}{Forward\_cpu()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily template$<$typename Dtype$>$ \\
virtual void \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer}{caffe\+::\+Infogain\+Loss\+Layer}}$<$ Dtype $>$\+::Forward\+\_\+cpu (\begin{DoxyParamCaption}\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{bottom,  }\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{top }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}, {\ttfamily [virtual]}}



A generalization of \mbox{\hyperlink{classcaffe_1_1_softmax_with_loss_layer}{Softmax\+With\+Loss\+Layer}} that takes an \char`\"{}information gain\char`\"{} (infogain) matrix specifying the \char`\"{}value\char`\"{} of all label pairs. 

Equivalent to the \mbox{\hyperlink{classcaffe_1_1_softmax_with_loss_layer}{Softmax\+With\+Loss\+Layer}} if the infogain matrix is the identity.


\begin{DoxyParams}{Parameters}
{\em bottom} & input \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 2-\/3)
\begin{DoxyEnumerate}
\item $ (N \times C \times H \times W) $ the predictions $ x $, a \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} with values in $ [-\infty, +\infty] $ indicating the predicted score for each of the $ K = CHW $ classes. This layer maps these scores to a probability distribution over classes using the softmax function $ \hat{p}_{nk} = \exp(x_{nk}) / \left[\sum_{k'} \exp(x_{nk'})\right] $ (see \mbox{\hyperlink{classcaffe_1_1_softmax_layer}{Softmax\+Layer}}).
\item $ (N \times 1 \times 1 \times 1) $ the labels $ l $, an integer-\/valued \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} with values $ l_n \in [0, 1, 2, ..., K - 1] $ indicating the correct class label among the $ K $ classes
\item $ (1 \times 1 \times K \times K) $ ({\bfseries optional}) the infogain matrix $ H $. This must be provided as the third bottom blob input if not provided as the infogain\+\_\+mat in the \mbox{\hyperlink{classcaffe_1_1_infogain_loss_parameter}{Infogain\+Loss\+Parameter}}. If $ H = I $, this layer is equivalent to the \mbox{\hyperlink{classcaffe_1_1_softmax_with_loss_layer}{Softmax\+With\+Loss\+Layer}}. 
\end{DoxyEnumerate}\\
\hline
{\em top} & output \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}} vector (length 1)
\begin{DoxyEnumerate}
\item $ (1 \times 1 \times 1 \times 1) $ the computed infogain multinomial logistic loss\+: $ E = \frac{-1}{N} \sum\limits_{n=1}^N H_{l_n} \log(\hat{p}_n) = \frac{-1}{N} \sum\limits_{n=1}^N \sum\limits_{k=1}^{K} H_{l_n,k} \log(\hat{p}_{n,k}) $, where $ H_{l_n} $ denotes row $l_n$ of $H$. 
\end{DoxyEnumerate}\\
\hline
\end{DoxyParams}


Implements \mbox{\hyperlink{classcaffe_1_1_layer_a576ac6a60b1e99fe383831f52a6cea77}{caffe\+::\+Layer$<$ Dtype $>$}}.

\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_a0e5e9667b19fb88ece7298e3e83d2fdb}\label{classcaffe_1_1_infogain_loss_layer_a0e5e9667b19fb88ece7298e3e83d2fdb}} 
\index{caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}!get\+\_\+normalizer@{get\+\_\+normalizer}}
\index{get\+\_\+normalizer@{get\+\_\+normalizer}!caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}}
\subsubsection{\texorpdfstring{get\+\_\+normalizer()}{get\_normalizer()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily template$<$typename Dtype $>$ \\
Dtype \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer}{caffe\+::\+Infogain\+Loss\+Layer}}$<$ Dtype $>$\+::get\+\_\+normalizer (\begin{DoxyParamCaption}\item[{Loss\+Parameter\+\_\+\+Normalization\+Mode}]{normalization\+\_\+mode,  }\item[{int}]{valid\+\_\+count }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}, {\ttfamily [virtual]}}

Read the normalization mode parameter and compute the normalizer based on the blob size. If normalization\+\_\+mode is V\+A\+L\+ID, the count of valid outputs will be read from valid\+\_\+count, unless it is -\/1 in which case all outputs are assumed to be valid. \mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_a9c223a4b6dc5a48fb56bf653111abea1}\label{classcaffe_1_1_infogain_loss_layer_a9c223a4b6dc5a48fb56bf653111abea1}} 
\index{caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}!get\+\_\+normalizer@{get\+\_\+normalizer}}
\index{get\+\_\+normalizer@{get\+\_\+normalizer}!caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}}
\subsubsection{\texorpdfstring{get\+\_\+normalizer()}{get\_normalizer()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily template$<$typename Dtype$>$ \\
virtual Dtype \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer}{caffe\+::\+Infogain\+Loss\+Layer}}$<$ Dtype $>$\+::get\+\_\+normalizer (\begin{DoxyParamCaption}\item[{Loss\+Parameter\+\_\+\+Normalization\+Mode}]{normalization\+\_\+mode,  }\item[{int}]{valid\+\_\+count }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [protected]}, {\ttfamily [virtual]}}

Read the normalization mode parameter and compute the normalizer based on the blob size. If normalization\+\_\+mode is V\+A\+L\+ID, the count of valid outputs will be read from valid\+\_\+count, unless it is -\/1 in which case all outputs are assumed to be valid. \mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_a772be3f4074c72b3cf9214bda3422402}\label{classcaffe_1_1_infogain_loss_layer_a772be3f4074c72b3cf9214bda3422402}} 
\index{caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}!Layer\+Set\+Up@{Layer\+Set\+Up}}
\index{Layer\+Set\+Up@{Layer\+Set\+Up}!caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Layer\+Set\+Up()}{LayerSetUp()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily template$<$typename Dtype $>$ \\
void \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer}{caffe\+::\+Infogain\+Loss\+Layer}}$<$ Dtype $>$\+::Layer\+Set\+Up (\begin{DoxyParamCaption}\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{bottom,  }\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{top }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [virtual]}}



Does layer-\/specific setup\+: your layer should implement this function as well as Reshape. 


\begin{DoxyParams}{Parameters}
{\em bottom} & the preshaped input blobs, whose data fields store the input data for this layer \\
\hline
{\em top} & the allocated but unshaped output blobs\\
\hline
\end{DoxyParams}
This method should do one-\/time layer specific setup. This includes reading and processing relevent parameters from the {\ttfamily layer\+\_\+param\+\_\+}. Setting up the shapes of top blobs and internal buffers should be done in {\ttfamily Reshape}, which will be called before the forward pass to adjust the top blob sizes. 

Reimplemented from \mbox{\hyperlink{classcaffe_1_1_loss_layer_aa6fc7c2e90be66f1c1f0683637c949da}{caffe\+::\+Loss\+Layer$<$ Dtype $>$}}.

Here is the call graph for this function\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=350pt]{classcaffe_1_1_infogain_loss_layer_a772be3f4074c72b3cf9214bda3422402_cgraph}
\end{center}
\end{figure}
\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_ae59c01de80f22c87c1dd2ef87c6e6a2f}\label{classcaffe_1_1_infogain_loss_layer_ae59c01de80f22c87c1dd2ef87c6e6a2f}} 
\index{caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}!Layer\+Set\+Up@{Layer\+Set\+Up}}
\index{Layer\+Set\+Up@{Layer\+Set\+Up}!caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Layer\+Set\+Up()}{LayerSetUp()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily template$<$typename Dtype$>$ \\
virtual void \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer}{caffe\+::\+Infogain\+Loss\+Layer}}$<$ Dtype $>$\+::Layer\+Set\+Up (\begin{DoxyParamCaption}\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{bottom,  }\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{top }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [virtual]}}



Does layer-\/specific setup\+: your layer should implement this function as well as Reshape. 


\begin{DoxyParams}{Parameters}
{\em bottom} & the preshaped input blobs, whose data fields store the input data for this layer \\
\hline
{\em top} & the allocated but unshaped output blobs\\
\hline
\end{DoxyParams}
This method should do one-\/time layer specific setup. This includes reading and processing relevent parameters from the {\ttfamily layer\+\_\+param\+\_\+}. Setting up the shapes of top blobs and internal buffers should be done in {\ttfamily Reshape}, which will be called before the forward pass to adjust the top blob sizes. 

Reimplemented from \mbox{\hyperlink{classcaffe_1_1_loss_layer_aa6fc7c2e90be66f1c1f0683637c949da}{caffe\+::\+Loss\+Layer$<$ Dtype $>$}}.

\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_a9b2372959a16da1e80ae7a98b7689a4c}\label{classcaffe_1_1_infogain_loss_layer_a9b2372959a16da1e80ae7a98b7689a4c}} 
\index{caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}!Max\+Bottom\+Blobs@{Max\+Bottom\+Blobs}}
\index{Max\+Bottom\+Blobs@{Max\+Bottom\+Blobs}!caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Max\+Bottom\+Blobs()}{MaxBottomBlobs()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily template$<$typename Dtype$>$ \\
virtual int \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer}{caffe\+::\+Infogain\+Loss\+Layer}}$<$ Dtype $>$\+::Max\+Bottom\+Blobs (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}, {\ttfamily [virtual]}}



Returns the maximum number of bottom blobs required by the layer, or -\/1 if no maximum number is required. 

This method should be overridden to return a non-\/negative value if your layer expects some maximum number of bottom blobs. 

Reimplemented from \mbox{\hyperlink{classcaffe_1_1_layer_af8bdc989053e0363ab032026b46de7c3}{caffe\+::\+Layer$<$ Dtype $>$}}.

\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_a9b2372959a16da1e80ae7a98b7689a4c}\label{classcaffe_1_1_infogain_loss_layer_a9b2372959a16da1e80ae7a98b7689a4c}} 
\index{caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}!Max\+Bottom\+Blobs@{Max\+Bottom\+Blobs}}
\index{Max\+Bottom\+Blobs@{Max\+Bottom\+Blobs}!caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Max\+Bottom\+Blobs()}{MaxBottomBlobs()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily template$<$typename Dtype$>$ \\
virtual int \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer}{caffe\+::\+Infogain\+Loss\+Layer}}$<$ Dtype $>$\+::Max\+Bottom\+Blobs (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}, {\ttfamily [virtual]}}



Returns the maximum number of bottom blobs required by the layer, or -\/1 if no maximum number is required. 

This method should be overridden to return a non-\/negative value if your layer expects some maximum number of bottom blobs. 

Reimplemented from \mbox{\hyperlink{classcaffe_1_1_layer_af8bdc989053e0363ab032026b46de7c3}{caffe\+::\+Layer$<$ Dtype $>$}}.

\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_a93019601c6256354fd4758da91d9311f}\label{classcaffe_1_1_infogain_loss_layer_a93019601c6256354fd4758da91d9311f}} 
\index{caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}!Max\+Top\+Blobs@{Max\+Top\+Blobs}}
\index{Max\+Top\+Blobs@{Max\+Top\+Blobs}!caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Max\+Top\+Blobs()}{MaxTopBlobs()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily template$<$typename Dtype$>$ \\
virtual int \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer}{caffe\+::\+Infogain\+Loss\+Layer}}$<$ Dtype $>$\+::Max\+Top\+Blobs (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}, {\ttfamily [virtual]}}



Returns the maximum number of top blobs required by the layer, or -\/1 if no maximum number is required. 

This method should be overridden to return a non-\/negative value if your layer expects some maximum number of top blobs. 

Reimplemented from \mbox{\hyperlink{classcaffe_1_1_layer_ac6c03df0b6e40e776c94001e19994a2e}{caffe\+::\+Layer$<$ Dtype $>$}}.

\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_a93019601c6256354fd4758da91d9311f}\label{classcaffe_1_1_infogain_loss_layer_a93019601c6256354fd4758da91d9311f}} 
\index{caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}!Max\+Top\+Blobs@{Max\+Top\+Blobs}}
\index{Max\+Top\+Blobs@{Max\+Top\+Blobs}!caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Max\+Top\+Blobs()}{MaxTopBlobs()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily template$<$typename Dtype$>$ \\
virtual int \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer}{caffe\+::\+Infogain\+Loss\+Layer}}$<$ Dtype $>$\+::Max\+Top\+Blobs (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}, {\ttfamily [virtual]}}



Returns the maximum number of top blobs required by the layer, or -\/1 if no maximum number is required. 

This method should be overridden to return a non-\/negative value if your layer expects some maximum number of top blobs. 

Reimplemented from \mbox{\hyperlink{classcaffe_1_1_layer_ac6c03df0b6e40e776c94001e19994a2e}{caffe\+::\+Layer$<$ Dtype $>$}}.

\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_ad8a1ef702a695e379e5d0450369b4a0c}\label{classcaffe_1_1_infogain_loss_layer_ad8a1ef702a695e379e5d0450369b4a0c}} 
\index{caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}!Min\+Bottom\+Blobs@{Min\+Bottom\+Blobs}}
\index{Min\+Bottom\+Blobs@{Min\+Bottom\+Blobs}!caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Min\+Bottom\+Blobs()}{MinBottomBlobs()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily template$<$typename Dtype$>$ \\
virtual int \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer}{caffe\+::\+Infogain\+Loss\+Layer}}$<$ Dtype $>$\+::Min\+Bottom\+Blobs (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}, {\ttfamily [virtual]}}



Returns the minimum number of bottom blobs required by the layer, or -\/1 if no minimum number is required. 

This method should be overridden to return a non-\/negative value if your layer expects some minimum number of bottom blobs. 

Reimplemented from \mbox{\hyperlink{classcaffe_1_1_layer_aca3cb2bafaefda5d4760aaebd0b72def}{caffe\+::\+Layer$<$ Dtype $>$}}.

\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_ad8a1ef702a695e379e5d0450369b4a0c}\label{classcaffe_1_1_infogain_loss_layer_ad8a1ef702a695e379e5d0450369b4a0c}} 
\index{caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}!Min\+Bottom\+Blobs@{Min\+Bottom\+Blobs}}
\index{Min\+Bottom\+Blobs@{Min\+Bottom\+Blobs}!caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Min\+Bottom\+Blobs()}{MinBottomBlobs()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily template$<$typename Dtype$>$ \\
virtual int \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer}{caffe\+::\+Infogain\+Loss\+Layer}}$<$ Dtype $>$\+::Min\+Bottom\+Blobs (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}, {\ttfamily [virtual]}}



Returns the minimum number of bottom blobs required by the layer, or -\/1 if no minimum number is required. 

This method should be overridden to return a non-\/negative value if your layer expects some minimum number of bottom blobs. 

Reimplemented from \mbox{\hyperlink{classcaffe_1_1_layer_aca3cb2bafaefda5d4760aaebd0b72def}{caffe\+::\+Layer$<$ Dtype $>$}}.

\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_a15c4916e5de27151eb745491d8d14d41}\label{classcaffe_1_1_infogain_loss_layer_a15c4916e5de27151eb745491d8d14d41}} 
\index{caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}!Min\+Top\+Blobs@{Min\+Top\+Blobs}}
\index{Min\+Top\+Blobs@{Min\+Top\+Blobs}!caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Min\+Top\+Blobs()}{MinTopBlobs()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily template$<$typename Dtype$>$ \\
virtual int \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer}{caffe\+::\+Infogain\+Loss\+Layer}}$<$ Dtype $>$\+::Min\+Top\+Blobs (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}, {\ttfamily [virtual]}}



Returns the minimum number of top blobs required by the layer, or -\/1 if no minimum number is required. 

This method should be overridden to return a non-\/negative value if your layer expects some minimum number of top blobs. 

Reimplemented from \mbox{\hyperlink{classcaffe_1_1_layer_ab9e4c8d642e413948b131d851a8462a4}{caffe\+::\+Layer$<$ Dtype $>$}}.

\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_a15c4916e5de27151eb745491d8d14d41}\label{classcaffe_1_1_infogain_loss_layer_a15c4916e5de27151eb745491d8d14d41}} 
\index{caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}!Min\+Top\+Blobs@{Min\+Top\+Blobs}}
\index{Min\+Top\+Blobs@{Min\+Top\+Blobs}!caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Min\+Top\+Blobs()}{MinTopBlobs()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily template$<$typename Dtype$>$ \\
virtual int \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer}{caffe\+::\+Infogain\+Loss\+Layer}}$<$ Dtype $>$\+::Min\+Top\+Blobs (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}, {\ttfamily [virtual]}}



Returns the minimum number of top blobs required by the layer, or -\/1 if no minimum number is required. 

This method should be overridden to return a non-\/negative value if your layer expects some minimum number of top blobs. 

Reimplemented from \mbox{\hyperlink{classcaffe_1_1_layer_ab9e4c8d642e413948b131d851a8462a4}{caffe\+::\+Layer$<$ Dtype $>$}}.

\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_aa2903026b3886816270deb038a463759}\label{classcaffe_1_1_infogain_loss_layer_aa2903026b3886816270deb038a463759}} 
\index{caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}!Reshape@{Reshape}}
\index{Reshape@{Reshape}!caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Reshape()}{Reshape()}\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily template$<$typename Dtype$>$ \\
virtual void \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer}{caffe\+::\+Infogain\+Loss\+Layer}}$<$ Dtype $>$\+::Reshape (\begin{DoxyParamCaption}\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{bottom,  }\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{top }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [virtual]}}



Adjust the shapes of top blobs and internal buffers to accommodate the shapes of the bottom blobs. 


\begin{DoxyParams}{Parameters}
{\em bottom} & the input blobs, with the requested input shapes \\
\hline
{\em top} & the top blobs, which should be reshaped as needed\\
\hline
\end{DoxyParams}
This method should reshape top blobs as needed according to the shapes of the bottom (input) blobs, as well as reshaping any internal buffers and making any other necessary adjustments so that the layer can accommodate the bottom blobs. 

Reimplemented from \mbox{\hyperlink{classcaffe_1_1_loss_layer_abf00412194f5413ea9468ee44b0d986f}{caffe\+::\+Loss\+Layer$<$ Dtype $>$}}.

\mbox{\Hypertarget{classcaffe_1_1_infogain_loss_layer_a83ed478450bc7f629499fed37f654c5c}\label{classcaffe_1_1_infogain_loss_layer_a83ed478450bc7f629499fed37f654c5c}} 
\index{caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}!Reshape@{Reshape}}
\index{Reshape@{Reshape}!caffe\+::\+Infogain\+Loss\+Layer@{caffe\+::\+Infogain\+Loss\+Layer}}
\subsubsection{\texorpdfstring{Reshape()}{Reshape()}\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily template$<$typename Dtype $>$ \\
void \mbox{\hyperlink{classcaffe_1_1_infogain_loss_layer}{caffe\+::\+Infogain\+Loss\+Layer}}$<$ Dtype $>$\+::Reshape (\begin{DoxyParamCaption}\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{bottom,  }\item[{const vector$<$ \mbox{\hyperlink{classcaffe_1_1_blob}{Blob}}$<$ Dtype $>$ $\ast$$>$ \&}]{top }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [virtual]}}



Adjust the shapes of top blobs and internal buffers to accommodate the shapes of the bottom blobs. 


\begin{DoxyParams}{Parameters}
{\em bottom} & the input blobs, with the requested input shapes \\
\hline
{\em top} & the top blobs, which should be reshaped as needed\\
\hline
\end{DoxyParams}
This method should reshape top blobs as needed according to the shapes of the bottom (input) blobs, as well as reshaping any internal buffers and making any other necessary adjustments so that the layer can accommodate the bottom blobs. 

Reimplemented from \mbox{\hyperlink{classcaffe_1_1_loss_layer_abf00412194f5413ea9468ee44b0d986f}{caffe\+::\+Loss\+Layer$<$ Dtype $>$}}.

Here is the call graph for this function\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=350pt]{classcaffe_1_1_infogain_loss_layer_a83ed478450bc7f629499fed37f654c5c_cgraph}
\end{center}
\end{figure}


The documentation for this class was generated from the following files\+:\begin{DoxyCompactItemize}
\item 
build/install/include/caffe/layers/infogain\+\_\+loss\+\_\+layer.\+hpp\item 
src/caffe/layers/infogain\+\_\+loss\+\_\+layer.\+cpp\end{DoxyCompactItemize}
