

 \subsection*{title\+: Loss }

\section*{Loss}

In Caffe, as in most of machine learning, learning is driven by a {\bfseries loss} function (also known as an {\bfseries error}, {\bfseries cost}, or {\bfseries objective} function). A loss function specifies the goal of learning by mapping parameter settings (i.\+e., the current network weights) to a scalar value specifying the \char`\"{}badness\char`\"{} of these parameter settings. Hence, the goal of learning is to find a setting of the weights that {\itshape minimizes} the loss function.

The loss in Caffe is computed by the Forward pass of the network. Each layer takes a set of input ({\ttfamily bottom}) blobs and produces a set of output ({\ttfamily top}) blobs. Some of these layers\textquotesingle{} outputs may be used in the loss function. A typical choice of loss function for one-\/versus-\/all classification tasks is the {\ttfamily Softmax\+With\+Loss} function, used in a network definition as follows, for example\+: \begin{DoxyVerb}layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "pred"
  bottom: "label"
  top: "loss"
}
\end{DoxyVerb}


In a {\ttfamily Softmax\+With\+Loss} function, the {\ttfamily top} blob is a scalar (empty shape) which averages the loss (computed from predicted labels {\ttfamily pred} and actuals labels {\ttfamily label}) over the entire mini-\/batch.

\subsubsection*{Loss weights}

For nets with multiple layers producing a loss (e.\+g., a network that both classifies the input using a {\ttfamily Softmax\+With\+Loss} layer and reconstructs it using a {\ttfamily Euclidean\+Loss} layer), {\itshape loss weights} can be used to specify their relative importance.

By convention, Caffe layer types with the suffix {\ttfamily Loss} contribute to the loss function, but other layers are assumed to be purely used for intermediate computations. However, any layer can be used as a loss by adding a field {\ttfamily loss\+\_\+weight\+: $<$float$>$} to a layer definition for each {\ttfamily top} blob produced by the layer. Layers with the suffix {\ttfamily Loss} have an implicit {\ttfamily loss\+\_\+weight\+: 1} for the first {\ttfamily top} blob (and {\ttfamily loss\+\_\+weight\+: 0} for any additional {\ttfamily top}s); other layers have an implicit {\ttfamily loss\+\_\+weight\+: 0} for all {\ttfamily top}s. So, the above {\ttfamily Softmax\+With\+Loss} layer could be equivalently written as\+: \begin{DoxyVerb}layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "pred"
  bottom: "label"
  top: "loss"
  loss_weight: 1
}
\end{DoxyVerb}


However, {\itshape any} layer able to backpropagate may be given a non-\/zero {\ttfamily loss\+\_\+weight}, allowing one to, for example, regularize the activations produced by some intermediate layer(s) of the network if desired. For non-\/singleton outputs with an associated non-\/zero loss, the loss is computed simply by summing over all entries of the blob.

The final loss in Caffe, then, is computed by summing the total weighted loss over the network, as in the following pseudo-\/code\+: \begin{DoxyVerb}loss := 0
for layer in layers:
  for top, loss_weight in layer.tops, layer.loss_weights:
    loss += loss_weight * sum(top)\end{DoxyVerb}
 