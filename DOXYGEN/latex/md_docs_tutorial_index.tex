

 \subsection*{title\+: Caffe Tutorial }

\section*{Caffe Tutorial}

Caffe is a deep learning framework and this tutorial explains its philosophy, architecture, and usage. This is a practical guide and framework introduction, so the full frontier, context, and history of deep learning cannot be covered here. While explanations will be given where possible, a background in machine learning and neural networks is helpful.

\subsection*{Philosophy}

In one sip, Caffe is brewed for


\begin{DoxyItemize}
\item Expression\+: models and optimizations are defined as plaintext schemas instead of code.
\item Speed\+: for research and industry alike speed is crucial for state-\/of-\/the-\/art models and massive data.
\item Modularity\+: new tasks and settings require flexibility and extension.
\item Openness\+: scientific and applied progress call for common code, reference models, and reproducibility.
\item Community\+: academic research, startup prototypes, and industrial applications all share strength by joint discussion and development in a B\+S\+D-\/2 project.
\end{DoxyItemize}

and these principles direct the project.

\subsection*{Tour}


\begin{DoxyItemize}
\item \href{net_layer_blob.html}{\tt Nets, Layers, and Blobs}\+: the anatomy of a Caffe model.
\item \href{forward_backward.html}{\tt Forward / Backward}\+: the essential computations of layered compositional models.
\item \href{loss.html}{\tt Loss}\+: the task to be learned is defined by the loss.
\item \href{solver.html}{\tt Solver}\+: the solver coordinates model optimization.
\item \href{layers.html}{\tt Layer Catalogue}\+: the layer is the fundamental unit of modeling and computation -- Caffe\textquotesingle{}s catalogue includes layers for state-\/of-\/the-\/art models.
\item \href{interfaces.html}{\tt Interfaces}\+: command line, Python, and M\+A\+T\+L\+AB Caffe.
\item \href{data.html}{\tt Data}\+: how to caffeinate data for model input.
\end{DoxyItemize}

For a closer look at a few details\+:


\begin{DoxyItemize}
\item \href{convolution.html}{\tt Caffeinated Convolution}\+: how Caffe computes convolutions.
\end{DoxyItemize}

\subsection*{Deeper Learning}

There are helpful references freely online for deep learning that complement our hands-\/on tutorial. These cover introductory and advanced material, background and history, and the latest advances.

The \href{https://sites.google.com/site/deeplearningcvpr2014/}{\tt Tutorial on Deep Learning for Vision} from C\+V\+PR \textquotesingle{}14 is a good companion tutorial for researchers. Once you have the framework and practice foundations from the Caffe tutorial, explore the fundamental ideas and advanced research directions in the C\+V\+PR \textquotesingle{}14 tutorial.

A broad introduction is given in the free online draft of \href{http://neuralnetworksanddeeplearning.com/index.html}{\tt Neural Networks and Deep Learning} by Michael Nielsen. In particular the chapters on using neural nets and how backpropagation works are helpful if you are new to the subject.

These recent academic tutorials cover deep learning for researchers in machine learning and vision\+:


\begin{DoxyItemize}
\item \href{http://www.cs.nyu.edu/~yann/talks/lecun-ranzato-icml2013.pdf}{\tt Deep Learning Tutorial} by Yann Le\+Cun (N\+YU, Facebook) and Marc\textquotesingle{}Aurelio Ranzato (Facebook). I\+C\+ML 2013 tutorial.
\item \href{http://deeplearning.net/tutorial/deeplearning.pdf}{\tt L\+I\+SA Deep Learning Tutorial} by the L\+I\+SA Lab directed by Yoshua Bengio (U. Montr√©al).
\end{DoxyItemize}

For an exposition of neural networks in circuits and code, check out \href{http://karpathy.github.io/neuralnets/}{\tt Understanding Neural Networks from a Programmer\textquotesingle{}s Perspective} by Andrej Karpathy (Stanford). 